"""
ìœ ì‚¬ ì±„ìš©ê³µê³  ì¶”ì²œ ì‹œìŠ¤í…œ v2 (Two-Stage Pipeline)

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1ë‹¨ê³„: First Stage Filter                 â”‚
â”‚  (Embedding + Community + Jaccardë¡œ í›„ë³´êµ° ì¶”ì¶œ)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
            Top-N í›„ë³´êµ° (ì˜ˆ: 100ê°œ)
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2ë‹¨ê³„: PageRank ReRanker                        â”‚
â”‚  (PPR ìˆœìœ„ ê¸°ë°˜ ìµœì¢… ì •ë ¬, 0~1 ì •ê·œí™”)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                ìµœì¢… Top-K ê²°ê³¼

ìˆ˜í•™ì  ê·¼ê±°:
1. First Stage (Hybrid Filtering):
   - Embedding: cos(Î¸) = (AÂ·B) / (||A|| Ã— ||B||)
   - Community: Modularity Q = 1/(2m) Î£[A_ij - k_iÂ·k_j/(2m)]Â·Î´(c_i, c_j)
   - Jaccard: J(A,B) = |Aâˆ©B| / |AâˆªB|
   - Combined: score = Î±Â·emb + Î²Â·comm + Î³Â·jacc

2. Second Stage (PageRank ReRanking):
   - PPR: Ï€ = (1-Î±)M^T Ï€ + Î±Â·v
   - Rank Normalization: rank_score = (max_rank - rank) / (max_rank - 1)
   - ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ 1.0ì— ê°€ê¹Œì›€, ë‚®ì„ìˆ˜ë¡ 0.0ì— ê°€ê¹Œì›€
"""

import json
import pickle
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Set, Optional
from dataclasses import dataclass, field

import numpy as np
import networkx as nx
from node2vec import Node2Vec
from community import community_louvain


# ============================================================================
# Data Classes (ê¸°ì¡´ ìœ ì§€)
# ============================================================================

@dataclass
class JobPosting:
    """ì±„ìš©ê³µê³  ë°ì´í„° í´ë˜ìŠ¤"""
    posting_id: str
    company: str
    title: str
    url: str
    skills: List[str]
    job_category: str = ""

    def __hash__(self):
        return hash(self.posting_id)


@dataclass
class SimilarityScore:
    """ìœ ì‚¬ë„ ì ìˆ˜ ìƒì„¸ (2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ë²„ì „)"""
    posting_id: str
    final_score: float  # ìµœì¢… ì ìˆ˜ (PPR ìˆœìœ„ ê¸°ë°˜)

    # 1ë‹¨ê³„ ì ìˆ˜ë“¤
    first_stage_score: float = 0.0
    embedding_score: float = 0.0
    community_score: float = 0.0
    jaccard_score: float = 0.0

    # 2ë‹¨ê³„ ì ìˆ˜
    pagerank_rank_score: float = 0.0  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ (0~1)
    pagerank_raw_score: float = 0.0   # ì›ë³¸ PPR í™•ë¥ ê°’ (ì°¸ê³ ìš©)

    reason: str = ""


# ============================================================================
# Graph Infrastructure (ê¸°ì¡´ ìœ ì§€)
# ============================================================================

class JobPostingGraph:
    """ì±„ìš©ê³µê³  ê·¸ë˜í”„ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.G = nx.Graph()
        self.postings: Dict[str, JobPosting] = {}
        self.communities: Dict[str, int] = {}
        self.community_profiles: Dict[int, Dict] = {}

    def add_posting(self, posting: JobPosting):
        """ì±„ìš©ê³µê³ ë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€"""
        posting_node = f"posting:{posting.posting_id}"
        self.postings[posting_node] = posting
        self.G.add_node(posting_node, type='posting')

        # íšŒì‚¬ ë…¸ë“œ ì—°ê²°
        if posting.company:
            company_node = f"company:{posting.company}"
            self.G.add_edge(posting_node, company_node, weight=1.0)

        # ì§ë¬´ ì¹´í…Œê³ ë¦¬ ì—°ê²°
        if posting.job_category:
            category_node = f"category:{posting.job_category}"
            self.G.add_edge(posting_node, category_node, weight=1.0)

        # ìŠ¤í‚¬ ë…¸ë“œ ì—°ê²°
        for skill in posting.skills:
            skill_normalized = self._normalize_skill(skill)
            if skill_normalized:
                skill_node = f"skill:{skill_normalized}"
                self.G.add_edge(posting_node, skill_node, weight=1.0)

    def build_skill_cooccurrence(self, min_cooccur: int = 2):
        """ìŠ¤í‚¬ ê°„ ë™ì‹œ ì¶œí˜„ ì—£ì§€ ìƒì„±"""
        skill_pairs = Counter()

        for posting in self.postings.values():
            skills = [f"skill:{self._normalize_skill(s)}" for s in posting.skills]
            for i, skill1 in enumerate(skills):
                for skill2 in skills[i+1:]:
                    pair = tuple(sorted([skill1, skill2]))
                    skill_pairs[pair] += 1

        for (skill1, skill2), count in skill_pairs.items():
            if count >= min_cooccur:
                self.G.add_edge(skill1, skill2, weight=count)

    def detect_communities(self):
        """Louvain ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° íƒì§€"""
        posting_nodes = [n for n in self.G.nodes() if n.startswith('posting:')]
        posting_graph = nx.Graph()
        posting_graph.add_nodes_from(posting_nodes)

        # posting ê°„ ê°„ì ‘ ì—°ê²°ì„ ì§ì ‘ ì—°ê²°ë¡œ ë³€í™˜
        for i, posting1 in enumerate(posting_nodes):
            posting1_skills = set(n for n in self.G.neighbors(posting1) if n.startswith('skill:'))

            for posting2 in posting_nodes[i+1:]:
                posting2_skills = set(n for n in self.G.neighbors(posting2) if n.startswith('skill:'))
                common_skills = len(posting1_skills & posting2_skills)

                if common_skills > 0:
                    posting_graph.add_edge(posting1, posting2, weight=common_skills)

        if posting_graph.number_of_edges() == 0:
            print("  ! ê²½ê³ : posting ê°„ ì—°ê²° ì—†ìŒ, ê° ê³µê³ ë¥¼ ë³„ë„ ì»¤ë®¤ë‹ˆí‹°ë¡œ ì²˜ë¦¬")
            self.communities = {node: i for i, node in enumerate(posting_nodes)}
            self._build_community_profiles()
            return 0.0

        partition = community_louvain.best_partition(posting_graph, weight='weight')
        self.communities = partition
        self._build_community_profiles()

        modularity = community_louvain.modularity(partition, posting_graph, weight='weight')
        return modularity

    def _build_community_profiles(self):
        """ê° ì»¤ë®¤ë‹ˆí‹°ì˜ ëŒ€í‘œ íŠ¹ì§• ì¶”ì¶œ"""
        community_skills = defaultdict(Counter)
        community_companies = defaultdict(Counter)

        for posting_node, comm_id in self.communities.items():
            posting = self.postings[posting_node]

            for skill in posting.skills:
                skill_norm = self._normalize_skill(skill)
                if skill_norm:
                    community_skills[comm_id][skill_norm] += 1

            if posting.company:
                community_companies[comm_id][posting.company] += 1

        for comm_id in set(self.communities.values()):
            self.community_profiles[comm_id] = {
                'top_skills': community_skills[comm_id].most_common(10),
                'top_companies': community_companies[comm_id].most_common(5),
                'size': list(self.communities.values()).count(comm_id)
            }

        # ì»¤ë®¤ë‹ˆí‹° ë¶„í¬ ì¶œë ¥
        comm_sizes = Counter(self.communities.values())
        print(f"\n  ğŸ“Š ì»¤ë®¤ë‹ˆí‹° í¬ê¸° ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        for comm_id, size in sorted(comm_sizes.items(), key=lambda x: x[1], reverse=True)[:10]:
            profile = self.community_profiles.get(comm_id, {})
            top_skills = [s for s, _ in profile.get('top_skills', [])[:3]]
            top_companies = [c for c, _ in profile.get('top_companies', [])[:2]]
            print(f"    ì»¤ë®¤ë‹ˆí‹° {comm_id}: {size:2}ê°œ ê³µê³  | ìŠ¤í‚¬: {', '.join(top_skills)} | íšŒì‚¬: {', '.join(top_companies)}")

    @staticmethod
    def _normalize_skill(skill: str) -> str:
        """ìŠ¤í‚¬ ì •ê·œí™”"""
        return skill.lower().replace('-', '').replace('_', '').replace(' ', '').strip()


# ============================================================================
# Node2Vec Embedder (ê¸°ì¡´ ìœ ì§€)
# ============================================================================

class Node2VecEmbedder:
    """Node2Vec ê¸°ë°˜ ë…¸ë“œ ì„ë² ë”© í•™ìŠµ"""

    def __init__(self, graph: nx.Graph, dimensions: int = 64,
                 walk_length: int = 30, num_walks: int = 200,
                 p: float = 1.0, q: float = 1.0):
        self.graph = graph
        self.dimensions = dimensions
        self.model = None
        self.embeddings = {}

        print(f"[Node2Vec] Random walk ìƒì„± ì¤‘... (walks={num_walks}, length={walk_length})")
        node2vec = Node2Vec(
            graph,
            dimensions=dimensions,
            walk_length=walk_length,
            num_walks=num_walks,
            p=p,
            q=q,
            workers=4,
            quiet=True
        )

        print("[Node2Vec] Skip-gram ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.model = node2vec.fit(window=10, min_count=1, batch_words=4)

        for node in graph.nodes():
            try:
                self.embeddings[node] = self.model.wv[node]
            except KeyError:
                self.embeddings[node] = np.zeros(dimensions)

    def get_embedding(self, node: str) -> np.ndarray:
        """ë…¸ë“œ ì„ë² ë”© ë°˜í™˜"""
        return self.embeddings.get(node, np.zeros(self.dimensions))

    def cosine_similarity(self, node1: str, node2: str) -> float:
        """ë‘ ë…¸ë“œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
        emb1 = self.get_embedding(node1)
        emb2 = self.get_embedding(node2)

        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return np.dot(emb1, emb2) / (norm1 * norm2)


# ============================================================================
# Stage 1: First Stage Filter (ìƒˆë¡œ ì¶”ê°€)
# ============================================================================

class FirstStageFilter:
    """
    1ì°¨ í•„í„°ë§: Embedding + Community + Jaccardë¡œ í›„ë³´êµ° ì¶”ì¶œ

    ëª©ì :
    - ì „ì²´ ê³µê³  ì¤‘ ìƒìœ„ Nê°œ í›„ë³´êµ°ë§Œ ì¶”ì¶œí•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ
    - 3ê°€ì§€ ì§€í‘œì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ìœ ì‚¬ë„ í‰ê°€

    ê°€ì¤‘ì¹˜:
    - alpha: Embedding ê°€ì¤‘ì¹˜ (ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„)
    - beta: Community ê°€ì¤‘ì¹˜ (í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„)
    - gamma: Jaccard ê°€ì¤‘ì¹˜ (ì§ì ‘ ìŠ¤í‚¬ ë§¤ì¹­)
    """

    def __init__(self, graph: JobPostingGraph, embedder: Node2VecEmbedder,
                 alpha: float = 0.33, beta: float = 0.33, gamma: float = 0.34):
        self.graph = graph
        self.embedder = embedder
        self.alpha = alpha  # Embedding ê°€ì¤‘ì¹˜
        self.beta = beta    # Community ê°€ì¤‘ì¹˜
        self.gamma = gamma  # Jaccard ê°€ì¤‘ì¹˜

    def filter_candidates(self, query_posting: JobPosting, top_n: int = 100) -> List[Tuple[str, float, Dict]]:
        """
        1ì°¨ í›„ë³´êµ° ì¶”ì¶œ

        Returns:
            List of (posting_id, first_stage_score, score_details)
            score_details = {
                'embedding': float,
                'community': float,
                'jaccard': float
            }
        """
        candidates = []

        for posting_node, target_posting in self.graph.postings.items():
            # ìê¸° ìì‹  ì œì™¸
            if query_posting.posting_id == target_posting.posting_id:
                continue

            # 3ê°€ì§€ ìœ ì‚¬ë„ ê³„ì‚°
            embedding_similarity = self._calculate_embedding_similarity(query_posting, target_posting)
            community_similarity = self._calculate_community_similarity(query_posting, target_posting)
            jaccard_similarity = self._calculate_jaccard_similarity(query_posting.skills, target_posting.skills)

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
            first_stage_combined_score = (
                self.alpha * embedding_similarity +
                self.beta * community_similarity +
                self.gamma * jaccard_similarity
            )

            score_details = {
                'embedding': embedding_similarity,
                'community': community_similarity,
                'jaccard': jaccard_similarity
            }

            candidates.append((target_posting.posting_id, first_stage_combined_score, score_details))

        # Top-N í›„ë³´êµ°ë§Œ ì„ íƒ
        candidates.sort(key=lambda x: x[1], reverse=True)
        top_n_candidates = candidates[:top_n]

        print(f"\n[1ì°¨ í•„í„°ë§] ì „ì²´ {len(candidates)}ê°œ ì¤‘ ìƒìœ„ {len(top_n_candidates)}ê°œ í›„ë³´ ì¶”ì¶œ")
        if top_n_candidates:
            print(f"  - ìµœê³  ì ìˆ˜: {top_n_candidates[0][1]:.4f}")
            print(f"  - ìµœì € ì ìˆ˜: {top_n_candidates[-1][1]:.4f}")

        return top_n_candidates

    def _calculate_embedding_similarity(self, query: JobPosting, target: JobPosting) -> float:
        """Node2Vec ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„"""
        if not self.embedder:
            return 0.0

        query_skill_embeddings = [
            self.embedder.get_embedding(f"skill:{self.graph._normalize_skill(s)}")
            for s in query.skills
        ]
        target_skill_embeddings = [
            self.embedder.get_embedding(f"skill:{self.graph._normalize_skill(s)}")
            for s in target.skills
        ]

        if not query_skill_embeddings or not target_skill_embeddings:
            return 0.0

        query_avg_embedding = np.mean(query_skill_embeddings, axis=0)
        target_avg_embedding = np.mean(target_skill_embeddings, axis=0)

        norm_query = np.linalg.norm(query_avg_embedding)
        norm_target = np.linalg.norm(target_avg_embedding)

        if norm_query == 0 or norm_target == 0:
            return 0.0

        return np.dot(query_avg_embedding, target_avg_embedding) / (norm_query * norm_target)

    def _calculate_community_similarity(self, query: JobPosting, target: JobPosting) -> float:
        """ì»¤ë®¤ë‹ˆí‹° ìœ ì‚¬ë„"""
        query_skills_normalized = set(self.graph._normalize_skill(s) for s in query.skills)
        target_skills_normalized = set(self.graph._normalize_skill(s) for s in target.skills)

        # ê° ì»¤ë®¤ë‹ˆí‹°ì™€ ì¿¼ë¦¬ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        community_match_scores = {}
        for comm_id, profile in self.graph.community_profiles.items():
            comm_top_skills = set(skill for skill, _ in profile['top_skills'])
            if comm_top_skills:
                similarity = len(query_skills_normalized & comm_top_skills) / len(query_skills_normalized | comm_top_skills)
                community_match_scores[comm_id] = similarity

        target_node = f"posting:{target.posting_id}"
        target_community_id = self.graph.communities.get(target_node)

        if target_community_id is None:
            return 0.0

        community_level_score = community_match_scores.get(target_community_id, 0.0)

        # íƒ€ê²Ÿê³¼ ì¿¼ë¦¬ì˜ ì§ì ‘ ìŠ¤í‚¬ ìœ ì‚¬ë„
        if query_skills_normalized and target_skills_normalized:
            direct_skill_similarity = len(query_skills_normalized & target_skills_normalized) / len(query_skills_normalized | target_skills_normalized)
        else:
            direct_skill_similarity = 0.0

        # í•˜ì´ë¸Œë¦¬ë“œ: 50% ì»¤ë®¤ë‹ˆí‹° ë ˆë²¨ + 50% ì§ì ‘ ìœ ì‚¬ë„
        return 0.5 * community_level_score + 0.5 * direct_skill_similarity

    def _calculate_jaccard_similarity(self, skills1: List[str], skills2: List[str]) -> float:
        """Jaccard ìœ ì‚¬ë„"""
        set1 = set(self.graph._normalize_skill(s) for s in skills1)
        set2 = set(self.graph._normalize_skill(s) for s in skills2)

        if not set1 or not set2:
            return 0.0

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0


# ============================================================================
# Stage 2: PageRank ReRanker (ìƒˆë¡œ ì¶”ê°€)
# ============================================================================

class PageRankReRanker:
    """
    2ì°¨ ë¦¬ë­í‚¹: Personalized PageRank ê¸°ë°˜ ìµœì¢… ìˆœìœ„ ê²°ì •

    í•µì‹¬ ë³€ê²½ì‚¬í•­:
    - ê¸°ì¡´: PPR í™•ë¥ ê°’ ì§ì ‘ ì‚¬ìš©
    - ë³€ê²½: PPR ì ìˆ˜ â†’ ìˆœìœ„ ë³€í™˜ â†’ 0~1 ì •ê·œí™”

    ì •ê·œí™” ê³µì‹:
    - rank_score = (max_rank - current_rank) / (max_rank - 1)
    - 1ë“±: (N-1) / (N-1) = 1.0
    - 2ë“±: (N-2) / (N-1) = 0.9x
    - ê¼´ë“±: (N-N) / (N-1) = 0.0

    íš¨ê³¼:
    - PPR í™•ë¥ ê°’ì˜ í¸ì°¨ê°€ ì‘ì•„ë„ ìˆœìœ„ëŠ” ëª…í™•íˆ êµ¬ë¶„
    - ìµœì¢… ì ìˆ˜ê°€ ì§ê´€ì  (1ë“±=1.0, ê¼´ë“±=0.0)
    """

    def __init__(self, graph: JobPostingGraph):
        self.graph = graph

    def rerank_with_pagerank(self, query_posting: JobPosting,
                            first_stage_candidates: List[Tuple[str, float, Dict]]) -> List[SimilarityScore]:
        """
        PPR ê¸°ë°˜ ë¦¬ë­í‚¹

        Args:
            query_posting: ì¿¼ë¦¬ ê³µê³ 
            first_stage_candidates: 1ì°¨ í•„í„°ë§ ê²°ê³¼ [(posting_id, score, details), ...]

        Returns:
            List[SimilarityScore]: ìµœì¢… ìˆœìœ„ ê²°ê³¼
        """
        if not first_stage_candidates:
            return []

        # 1. ëª¨ë“  í›„ë³´ì— ëŒ€í•´ PPR ì ìˆ˜ ê³„ì‚°
        candidate_ids = [posting_id for posting_id, _, _ in first_stage_candidates]
        ppr_scores = self._calculate_ppr_scores(query_posting, candidate_ids)

        # 2. PPR ì ìˆ˜ ê¸°ì¤€ ì •ë ¬í•˜ì—¬ ìˆœìœ„ ë¶€ì—¬
        sorted_ppr = sorted(ppr_scores.items(), key=lambda x: x[1], reverse=True)

        # 3. ìˆœìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”
        rank_scores = self._normalize_ranks(sorted_ppr)

        # 4. ìµœì¢… SimilarityScore ê°ì²´ ìƒì„±
        final_results = []
        first_stage_dict = {posting_id: (score, details) for posting_id, score, details in first_stage_candidates}

        for posting_id, ppr_raw_score in sorted_ppr:
            first_stage_score, score_details = first_stage_dict[posting_id]
            rank_score = rank_scores[posting_id]

            similarity_score = SimilarityScore(
                posting_id=posting_id,
                final_score=rank_score,  # ìµœì¢… ì ìˆ˜ëŠ” ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
                first_stage_score=first_stage_score,
                embedding_score=score_details['embedding'],
                community_score=score_details['community'],
                jaccard_score=score_details['jaccard'],
                pagerank_rank_score=rank_score,
                pagerank_raw_score=ppr_raw_score,
                reason=self._generate_reason(query_posting, posting_id)
            )

            final_results.append(similarity_score)

        print(f"\n[2ì°¨ ë¦¬ë­í‚¹] PPR ìˆœìœ„ ê¸°ë°˜ ì •ë ¬ ì™„ë£Œ")
        if final_results:
            print(f"  - 1ë“± PPR ìˆœìœ„ ì ìˆ˜: {final_results[0].pagerank_rank_score:.4f} (ì›ë³¸: {final_results[0].pagerank_raw_score:.6f})")
            print(f"  - ê¼´ë“± PPR ìˆœìœ„ ì ìˆ˜: {final_results[-1].pagerank_rank_score:.4f} (ì›ë³¸: {final_results[-1].pagerank_raw_score:.6f})")

        return final_results

    def _calculate_ppr_scores(self, query: JobPosting, candidate_ids: List[str]) -> Dict[str, float]:
        """
        Personalized PageRank ì ìˆ˜ ê³„ì‚°

        Returns:
            Dict[posting_id, ppr_score]
        """
        # Personalization vector: ì¿¼ë¦¬ ìŠ¤í‚¬ì—ë§Œ í™•ë¥  ë¶€ì—¬
        personalization = {}
        query_skill_nodes = [f"skill:{self.graph._normalize_skill(s)}" for s in query.skills]

        for node in self.graph.G.nodes():
            if node in query_skill_nodes:
                personalization[node] = 1.0 / len(query_skill_nodes)
            else:
                personalization[node] = 0.0

        try:
            ppr_results = nx.pagerank(self.graph.G, personalization=personalization,
                                     alpha=0.85, max_iter=100, weight='weight')

            # í›„ë³´êµ°ì˜ PPR ì ìˆ˜ë§Œ ì¶”ì¶œ
            ppr_scores = {}
            for posting_id in candidate_ids:
                posting_node = f"posting:{posting_id}"
                ppr_scores[posting_id] = ppr_results.get(posting_node, 0.0)

            return ppr_scores

        except Exception as e:
            print(f"  ! PPR ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {posting_id: 0.0 for posting_id in candidate_ids}

    def _normalize_ranks(self, sorted_ppr: List[Tuple[str, float]]) -> Dict[str, float]:
        """
        ìˆœìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”

        Args:
            sorted_ppr: PPR ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ëœ [(posting_id, ppr_score), ...]

        Returns:
            Dict[posting_id, normalized_rank_score]

        ì •ê·œí™” ê³µì‹:
            rank_score = (max_rank - current_rank) / (max_rank - 1)
            - 1ë“± (rank=0): (N-1-0) / (N-1) = 1.0
            - 2ë“± (rank=1): (N-1-1) / (N-1) = (N-2)/(N-1)
            - ê¼´ë“± (rank=N-1): (N-1-(N-1)) / (N-1) = 0.0
        """
        num_candidates = len(sorted_ppr)

        if num_candidates <= 1:
            return {sorted_ppr[0][0]: 1.0} if sorted_ppr else {}

        rank_scores = {}
        for rank, (posting_id, _) in enumerate(sorted_ppr):
            # ìˆœìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™” (1ë“±=1.0, ê¼´ë“±=0.0)
            normalized_score = (num_candidates - 1 - rank) / (num_candidates - 1)
            rank_scores[posting_id] = normalized_score

        return rank_scores

    def _generate_reason(self, query: JobPosting, target_posting_id: str) -> str:
        """ì¶”ì²œ ì´ìœ  ìƒì„±"""
        target_node = f"posting:{target_posting_id}"
        target_posting = self.graph.postings.get(target_node)

        if not target_posting:
            return "ê·¸ë˜í”„ êµ¬ì¡°ì  ìœ ì‚¬ì„±"

        query_skills_normalized = set(self.graph._normalize_skill(s) for s in query.skills)
        target_skills_normalized = set(self.graph._normalize_skill(s) for s in target_posting.skills)

        common_skills = query_skills_normalized & target_skills_normalized

        if len(common_skills) >= 3:
            return f"ê³µí†µ ìŠ¤í‚¬ {len(common_skills)}ê°œ: {', '.join(list(common_skills)[:3])}"
        elif len(common_skills) > 0:
            return f"ê³µí†µ ìŠ¤í‚¬: {', '.join(common_skills)}"
        else:
            return "ê·¸ë˜í”„ êµ¬ì¡°ì  ìœ ì‚¬ì„±"


# ============================================================================
# Main Recommender: Two-Stage Pipeline (ìƒˆë¡œ ì¶”ê°€)
# ============================================================================

class TwoStageJobRecommender:
    """
    2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì¶”ì²œ ì‹œìŠ¤í…œ

    Pipeline:
    1. FirstStageFilter: Embedding + Community + Jaccardë¡œ ìƒìœ„ Nê°œ í›„ë³´ ì¶”ì¶œ
    2. PageRankReRanker: PPR ìˆœìœ„ ê¸°ë°˜ ìµœì¢… ì •ë ¬

    ì¥ì :
    - ê³„ì‚° íš¨ìœ¨ì„±: ì „ì²´ê°€ ì•„ë‹Œ ìƒìœ„ í›„ë³´êµ°ë§Œ PPR ê³„ì‚°
    - ì •í™•ë„ í–¥ìƒ: ë‹¤ì–‘í•œ ì§€í‘œë¡œ 1ì°¨ í•„í„°ë§ í›„ êµ¬ì¡°ì  ìœ ì‚¬ë„ë¡œ ìµœì¢… ê²°ì •
    - ìˆœìœ„ ëª…í™•í™”: PPR í™•ë¥ ê°’ì´ ì•„ë‹Œ ìˆœìœ„ë¡œ ì§ê´€ì  ë¹„êµ
    """

    def __init__(self):
        self.graph = JobPostingGraph()
        self.embedder: Optional[Node2VecEmbedder] = None
        self.first_stage_filter: Optional[FirstStageFilter] = None
        self.pagerank_reranker: Optional[PageRankReRanker] = None

    def _extract_posting_id(self, url: str, title: str, filepath: str, idx: int) -> str:
        """ê³ ìœ  ID ì¶”ì¶œ"""
        import hashlib
        from urllib.parse import urlparse, parse_qs

        company_prefix = Path(filepath).stem.split('_')[0]

        # URLì—ì„œ ID íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        if url:
            try:
                parsed = urlparse(url)
                params = parse_qs(parsed.query)

                for param_name in ['rtSeq', 'job_id', 'id', 'recruitNo', 'jobNo', 'no']:
                    if param_name in params:
                        return f"{company_prefix}_{params[param_name][0]}"
            except:
                pass

        # URL + title í•´ì‹œ
        if url and title:
            hash_input = f"{url}:{title}".encode('utf-8')
            hash_digest = hashlib.md5(hash_input).hexdigest()[:8]
            return f"{company_prefix}_{hash_digest}"

        # Fallback
        return f"{company_prefix}_jobs_{idx}"

    def load_data(self, job_files: List[str]):
        """ì±„ìš©ê³µê³  ë°ì´í„° ë¡œë“œ"""
        for filepath in job_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                for idx, job in enumerate(data):
                    skills = []
                    skill_info = job.get('skill_set_info', {})
                    if isinstance(skill_info, dict):
                        skill_set = skill_info.get('skill_set', [])
                        if isinstance(skill_set, list):
                            skills = skill_set

                    url = job.get('url', '')
                    title = job.get('title', '')
                    posting_id = self._extract_posting_id(url, title, filepath, idx)

                    posting = JobPosting(
                        posting_id=posting_id,
                        company=job.get('company', 'Unknown'),
                        title=title,
                        url=url,
                        skills=skills,
                        job_category=job.get('job_category', '')
                    )

                    self.graph.add_posting(posting)

                print(f"âœ“ {Path(filepath).name}: {len(data)}ê°œ ë¡œë“œ")
            except Exception as e:
                print(f"âœ— {filepath} ë¡œë“œ ì‹¤íŒ¨: {e}")

    def build_graph(self, use_skill_cooccurrence: bool = True):
        """ê·¸ë˜í”„ êµ¬ì¶• ë° ì»¤ë®¤ë‹ˆí‹° íƒì§€"""
        print("\n[ê·¸ë˜í”„ êµ¬ì¶•]")
        print(f"  ë…¸ë“œ: {self.graph.G.number_of_nodes()}ê°œ")
        print(f"  ì—£ì§€: {self.graph.G.number_of_edges()}ê°œ")

        if use_skill_cooccurrence:
            print("\n[ìŠ¤í‚¬ ë™ì‹œ ì¶œí˜„ ì—£ì§€ ì¶”ê°€]")
            self.graph.build_skill_cooccurrence(min_cooccur=2)
            print(f"  ì—£ì§€: {self.graph.G.number_of_edges()}ê°œ (ì—…ë°ì´íŠ¸)")

        print("\n[ì»¤ë®¤ë‹ˆí‹° íƒì§€]")
        modularity = self.graph.detect_communities()
        num_communities = len(set(self.graph.communities.values()))
        print(f"  ì»¤ë®¤ë‹ˆí‹° ìˆ˜: {num_communities}ê°œ")
        print(f"  Modularity: {modularity:.4f}")

    def train_embeddings(self, dimensions: int = 64):
        """Node2Vec ì„ë² ë”© í•™ìŠµ"""
        print("\n[Node2Vec ì„ë² ë”© í•™ìŠµ]")
        self.embedder = Node2VecEmbedder(
            self.graph.G,
            dimensions=dimensions,
            walk_length=30,
            num_walks=200,
            p=1.0,
            q=1.0
        )
        print(f"âœ“ ì„ë² ë”© í•™ìŠµ ì™„ë£Œ (dim={dimensions})")

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.first_stage_filter = FirstStageFilter(self.graph, self.embedder)
        self.pagerank_reranker = PageRankReRanker(self.graph)

    def recommend_similar_jobs(self, query_posting: JobPosting,
                               first_stage_top_n: int = 100,
                               final_top_k: int = 10) -> List[SimilarityScore]:
        """
        2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì¶”ì²œ

        Args:
            query_posting: ì¿¼ë¦¬ ì±„ìš©ê³µê³ 
            first_stage_top_n: 1ì°¨ í•„í„°ë§ í›„ë³´êµ° í¬ê¸° (ê¸°ë³¸ 100ê°œ)
            final_top_k: ìµœì¢… ì¶”ì²œ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)

        Returns:
            List[SimilarityScore]: ìµœì¢… ì¶”ì²œ ê²°ê³¼ (ìˆœìœ„ìˆœ ì •ë ¬)
        """
        if not self.first_stage_filter or not self.pagerank_reranker:
            raise ValueError("train_embeddings()ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

        print("\n" + "="*60)
        print("2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì¶”ì²œ ì‹œì‘")
        print("="*60)

        # Stage 1: First Stage Filtering
        print("\n[Stage 1] Embedding + Community + Jaccard í•„í„°ë§")
        first_stage_candidates = self.first_stage_filter.filter_candidates(
            query_posting,
            top_n=first_stage_top_n
        )

        # Stage 2: PageRank ReRanking
        print("\n[Stage 2] PageRank ìˆœìœ„ ê¸°ë°˜ ë¦¬ë­í‚¹")
        final_ranked_results = self.pagerank_reranker.rerank_with_pagerank(
            query_posting,
            first_stage_candidates
        )

        # ìµœì¢… Top-Kë§Œ ë°˜í™˜
        return final_ranked_results[:final_top_k]

    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'graph': self.graph,
                'embedder': self.embedder
            }, f)
        print(f"âœ“ ëª¨ë¸ ì €ì¥: {filepath}")

    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.graph = data['graph']
            self.embedder = data['embedder']

            # ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”
            self.first_stage_filter = FirstStageFilter(self.graph, self.embedder)
            self.pagerank_reranker = PageRankReRanker(self.graph)
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ: {filepath}")


# ============================================================================
# Main Execution
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ìœ ì‚¬ ì±„ìš©ê³µê³  ì¶”ì²œ ì‹œìŠ¤í…œ v2 (Two-Stage Pipeline)")
    print("="*60)

    # 1. ë°ì´í„° ë¡œë“œ
    recommender = TwoStageJobRecommender()

    script_dir = Path(__file__).parent
    job_files = [
        str(script_dir / 'data/hanwha_jobs.json'),
        str(script_dir / 'data/kakao_jobs.json'),
        str(script_dir / 'data/line_jobs.json'),
        str(script_dir / 'data/naver_jobs.json')
    ]

    print("\n[1/4] ë°ì´í„° ë¡œë“œ")
    recommender.load_data(job_files)

    # 2. ê·¸ë˜í”„ êµ¬ì¶•
    print("\n[2/4] ê·¸ë˜í”„ êµ¬ì¶•")
    recommender.build_graph(use_skill_cooccurrence=True)

    # 3. ì„ë² ë”© í•™ìŠµ
    print("\n[3/4] ì„ë² ë”© í•™ìŠµ")
    recommender.train_embeddings(dimensions=64)

    # 4. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    print("\n[4/4] ìœ ì‚¬ ê³µê³  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    test_query = JobPosting(
        posting_id="test_query",
        company="í…ŒìŠ¤íŠ¸ íšŒì‚¬",
        title="AI ì—”ì§€ë‹ˆì–´",
        url="",
        skills=["Python", "TensorFlow", "PyTorch", "Machine Learning"],
        job_category="AI"
    )

    results = recommender.recommend_similar_jobs(
        test_query,
        first_stage_top_n=100,  # 1ì°¨ í•„í„°ë§: ìƒìœ„ 100ê°œ í›„ë³´
        final_top_k=10          # ìµœì¢… ê²°ê³¼: Top 10
    )

    print(f"\nì¿¼ë¦¬: {test_query.title} ({', '.join(test_query.skills)})")
    print("\nìµœì¢… ì¶”ì²œ ê²°ê³¼ (PPR ìˆœìœ„ ê¸°ë°˜):")
    print("-" * 80)

    for i, score in enumerate(results, 1):
        posting = recommender.graph.postings[f"posting:{score.posting_id}"]
        print(f"{i}. {posting.company} - {posting.title}")
        print(f"   [ìµœì¢… ì ìˆ˜] {score.final_score:.4f} (PPR ìˆœìœ„ ê¸°ë°˜)")
        print(f"   [1ì°¨ ì ìˆ˜] {score.first_stage_score:.4f} "
              f"(Emb: {score.embedding_score:.4f}, "
              f"Comm: {score.community_score:.4f}, "
              f"Jacc: {score.jaccard_score:.4f})")
        print(f"   [2ì°¨ ì ìˆ˜] PPR ìˆœìœ„: {score.pagerank_rank_score:.4f} "
              f"(ì›ë³¸ í™•ë¥ : {score.pagerank_raw_score:.6f})")
        print(f"   [ìŠ¤í‚¬] {', '.join(posting.skills[:5])}")
        print(f"   [ì´ìœ ] {score.reason}")
        print()

    # ëª¨ë¸ ì €ì¥
    print("\n[ëª¨ë¸ ì €ì¥]")
    output_dir = script_dir / 'output'
    output_dir.mkdir(exist_ok=True)
    recommender.save_model(str(output_dir / 'two_stage_job_recommender.pkl'))
    print("\nì™„ë£Œ!")


if __name__ == '__main__':
    main()
