# SQL ì¿¼ë¦¬ ë¡œê¹… ì‹œìŠ¤í…œ

RAG ê²€ìƒ‰ì—ì„œ "statistics_with_stats" ë¼ìš°íŒ… ì‹œ LLMì´ ìƒì„±í•˜ëŠ” SQL ì¿¼ë¦¬ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë¡œê¹… ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

ì´ ì‹œìŠ¤í…œì€ ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ë¡œê¹…í•©ë‹ˆë‹¤:
1. **ë¼ìš°íŒ… ê²°ì •**: ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ ë° ë¼ìš°íŒ… ì „ëµ
2. **SQL ì¿¼ë¦¬ ìƒì„±**: LLMì´ ìƒì„±í•œ SQL ì¿¼ë¦¬
3. **ì‹¤í–‰ ê²°ê³¼**: ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„, ê²°ê³¼ ê°œìˆ˜, ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€

## ğŸ“‚ ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜

```
logs/sql_queries/
â”œâ”€â”€ sql_queries_2024-12-12.jsonl          # SQL ì¿¼ë¦¬ ë¡œê·¸ (ë‚ ì§œë³„)
â”œâ”€â”€ sql_queries_2024-12-13.jsonl
â”œâ”€â”€ routing_decisions_2024-12-12.jsonl    # ë¼ìš°íŒ… ê²°ì • ë¡œê·¸ (ë‚ ì§œë³„)
â””â”€â”€ routing_decisions_2024-12-13.jsonl
```

## ğŸ“Š ë¡œê·¸ êµ¬ì¡°

### SQL ì¿¼ë¦¬ ë¡œê·¸ (sql_queries_*.jsonl)

```json
{
  "timestamp": "2024-12-12T10:30:45.123456",
  "question": "2025ë…„ í•˜ë°˜ê¸° í† ìŠ¤ ì±„ìš©ê³µê³  ì´ ëª‡ê°œì•¼?",
  "route_decision": "statistics_with_stats",
  "extracted_entities": {
    "company_name": "í† ìŠ¤",
    "year": 2025,
    "period": "í•˜ë°˜ê¸°"
  },
  "query_info": {
    "query_type": "ì±„ìš©ê³µê³  ê°œìˆ˜ ì§‘ê³„",
    "generated_sql": "SELECT COUNT(*) as count FROM posts WHERE ...",
    "llm_response": "ì›ë³¸ LLM ì‘ë‹µ..."
  },
  "execution": {
    "success": true,
    "execution_time_ms": 45.23,
    "result_count": 25,
    "error": null
  }
}
```

### ë¼ìš°íŒ… ê²°ì • ë¡œê·¸ (routing_decisions_*.jsonl)

```json
{
  "timestamp": "2024-12-12T10:30:44.000000",
  "type": "routing_decision",
  "question": "2025ë…„ í•˜ë°˜ê¸° í† ìŠ¤ ì±„ìš©ê³µê³  ì´ ëª‡ê°œì•¼?",
  "route_decision": "statistics_with_stats",
  "extracted_entities": {
    "company_name": "í† ìŠ¤",
    "year": 2025,
    "period": "í•˜ë°˜ê¸°"
  },
  "params": {
    "needs_stats": true,
    "top_k": 5,
    "reason": "ì§ˆë¬¸ì— 'ì´ ëª‡ê°œ'ë¼ëŠ” ì§‘ê³„ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ í†µê³„ ì¿¼ë¦¬ê°€ í•„ìš”í•¨"
  },
  "llm_response": "ì›ë³¸ LLM ì‘ë‹µ..."
}
```

## ğŸ” ë¡œê·¸ ì¡°íšŒ ë°©ë²•

### 1. ëª…ë ¹ì¤„ ë„êµ¬ ì‚¬ìš©

```bash
# ì˜¤ëŠ˜ì˜ SQL ì¿¼ë¦¬ ë¡œê·¸ ë³´ê¸°
python -m app.utils.view_query_logs view

# íŠ¹ì • ë‚ ì§œì˜ ë¡œê·¸ ë³´ê¸°
python -m app.utils.view_query_logs view --date 2024-12-12

# ë¼ìš°íŒ… ê²°ì • ë¡œê·¸ ë³´ê¸°
python -m app.utils.view_query_logs view --type routing_decisions

# ìµœê·¼ 10ê°œë§Œ ë³´ê¸°
python -m app.utils.view_query_logs view --limit 10

# í†µê³„ ë³´ê¸°
python -m app.utils.view_query_logs stats

# íŠ¹ì • ë‚ ì§œì˜ í†µê³„
python -m app.utils.view_query_logs stats --date 2024-12-12

# í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
python -m app.utils.view_query_logs search --keyword "í† ìŠ¤"
```

### 2. Python ì½”ë“œì—ì„œ ì‚¬ìš©

```python
from app.utils.query_logger import get_query_logger

# QueryLogger ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
logger = get_query_logger()

# ì˜¤ëŠ˜ì˜ ë¡œê·¸ ì½ê¸°
logs = logger.read_logs()

# íŠ¹ì • ë‚ ì§œì˜ ë¡œê·¸ ì½ê¸°
logs = logger.read_logs(date="2024-12-12", log_type="sql_queries")

# í†µê³„ ê°€ì ¸ì˜¤ê¸°
stats = logger.get_statistics()
print(f"Total queries: {stats['total_queries']}")
print(f"Success rate: {stats['success_rate']}")
```

### 3. ì§ì ‘ íŒŒì¼ ì½ê¸°

JSONL í˜•ì‹ì´ë¯€ë¡œ ê° ì¤„ì´ ë…ë¦½ì ì¸ JSON ê°ì²´ì…ë‹ˆë‹¤:

```python
import json

with open('logs/sql_queries/sql_queries_2024-12-12.jsonl', 'r', encoding='utf-8') as f:
    for line in f:
        log_entry = json.loads(line)
        print(log_entry['question'])
        print(log_entry['query_info']['generated_sql'])
```

## ğŸ“ˆ í†µê³„ ë° ëª¨ë‹ˆí„°ë§

### ì¼ì¼ í†µê³„ ì˜ˆì‹œ

```
ğŸ“Š SQL QUERY STATISTICS - 2024-12-12
================================================================================

ğŸ“ˆ Overall Statistics:
  â€¢ Total Queries: 42
  â€¢ Successful: 40
  â€¢ Failed: 2
  â€¢ Success Rate: 95.2%
  â€¢ Avg Execution Time: 78.45ms

ğŸ“Š Query Types Distribution:
  â€¢ ì±„ìš©ê³µê³  ê°œìˆ˜ ì§‘ê³„: 15 (35.7%)
  â€¢ ê¸°ìˆ ìŠ¤íƒ í†µê³„: 12 (28.6%)
  â€¢ íšŒì‚¬ë³„ ì±„ìš© íŠ¸ë Œë“œ: 10 (23.8%)
  â€¢ ê¸°ê°„ë³„ ì±„ìš© ì¶”ì´: 5 (11.9%)

ğŸ¯ Route Decisions Distribution:
  â€¢ statistics_with_stats: 42 (100.0%)
```

## ğŸ¯ í™œìš© ë°©ì•ˆ

### 1. ì¿¼ë¦¬ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- LLMì´ ìƒì„±í•œ SQL ì¿¼ë¦¬ì˜ ì •í™•ì„± í™•ì¸
- ì—ëŸ¬ê°€ ë°œìƒí•œ ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„
- ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ìµœì í™”

### 2. ë¼ìš°íŒ… ì •í™•ë„ ë¶„ì„
- ì§ˆë¬¸ ìœ í˜•ë³„ ë¼ìš°íŒ… ê²°ì • íŒ¨í„´ íŒŒì•…
- ì—”í‹°í‹° ì¶”ì¶œ ì •í™•ë„ í™•ì¸
- ë¼ìš°íŒ… ë¡œì§ ê°œì„  ë°©í–¥ ë„ì¶œ

### 3. ì‚¬ìš©ì íŒ¨í„´ ë¶„ì„
- ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ìœ í˜• íŒŒì•…
- ì‹œê°„ëŒ€ë³„ ì¿¼ë¦¬ ë¶„í¬ ë¶„ì„
- ì¸ê¸° ìˆëŠ” íšŒì‚¬/ì§ë¬´ í‚¤ì›Œë“œ ì¶”ì¶œ

### 4. ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„
- ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¶”ì´ ëª¨ë‹ˆí„°ë§
- ì‹¤íŒ¨ìœ¨ ì¶”ì  ë° ì›ì¸ ë¶„ì„
- ë³‘ëª© ì§€ì  ì‹ë³„

## ğŸ”§ ì„¤ì •

### ë¡œê·¸ ë””ë ‰í† ë¦¬ ë³€ê²½

```python
from app.utils.query_logger import QueryLogger

# ì»¤ìŠ¤í…€ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
logger = QueryLogger(log_dir="custom_logs/queries")
```

### ë¡œê·¸ ë ˆë²¨ ì¡°ì •

ë¡œê±°ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒì„ ê¸°ë¡í•©ë‹ˆë‹¤:
- âœ… ëª¨ë“  ì¿¼ë¦¬ ìƒì„± ì‹œë„
- âœ… ì‹¤í–‰ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€
- âœ… ì—ëŸ¬ ë©”ì‹œì§€ ë° ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤

ì½˜ì†” ì¶œë ¥ì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´ `query_logger.py`ì˜ `print()` ë¬¸ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.

## ğŸ“ ì˜ˆì œ

### ì˜ˆì œ 1: ì‹¤íŒ¨í•œ ì¿¼ë¦¬ ì°¾ê¸°

```python
from app.utils.query_logger import get_query_logger

logger = get_query_logger()
logs = logger.read_logs()

failed_queries = [log for log in logs if not log['execution']['success']]

for log in failed_queries:
    print(f"Question: {log['question']}")
    print(f"SQL: {log['query_info']['generated_sql']}")
    print(f"Error: {log['execution']['error']}")
    print("-" * 80)
```

### ì˜ˆì œ 2: ëŠë¦° ì¿¼ë¦¬ ë¶„ì„

```python
from app.utils.query_logger import get_query_logger

logger = get_query_logger()
logs = logger.read_logs()

# 100ms ì´ìƒ ì†Œìš”ëœ ì¿¼ë¦¬ ì°¾ê¸°
slow_queries = [
    log for log in logs 
    if log['execution'].get('execution_time_ms', 0) > 100
]

# ì‹¤í–‰ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
slow_queries.sort(
    key=lambda x: x['execution'].get('execution_time_ms', 0),
    reverse=True
)

for log in slow_queries[:10]:  # Top 10
    exec_time = log['execution']['execution_time_ms']
    print(f"{exec_time:.2f}ms - {log['question']}")
```

### ì˜ˆì œ 3: íšŒì‚¬ë³„ ì¿¼ë¦¬ í†µê³„

```python
from app.utils.query_logger import get_query_logger
from collections import Counter

logger = get_query_logger()
logs = logger.read_logs()

# íšŒì‚¬ëª… ì¶”ì¶œ
companies = [
    log['extracted_entities'].get('company_name')
    for log in logs
    if log['extracted_entities'].get('company_name')
]

# ë¹ˆë„ìˆ˜ ê³„ì‚°
company_counts = Counter(companies)

print("íšŒì‚¬ë³„ ì¿¼ë¦¬ ìˆ˜:")
for company, count in company_counts.most_common(10):
    print(f"  {company}: {count}")
```

## ğŸš€ ìë™í™”

### Cron Jobìœ¼ë¡œ ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±

```bash
# ë§¤ì¼ ìì •ì— ì „ë‚  í†µê³„ ì´ë©”ì¼ ë°œì†¡
0 0 * * * python -m app.utils.view_query_logs stats --date $(date -d "yesterday" +\%Y-\%m-\%d) | mail -s "Daily Query Stats" admin@example.com
```

### ë¡œê·¸ íŒŒì¼ ì •ë¦¬

ì˜¤ë˜ëœ ë¡œê·¸ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ì •ë¦¬:

```bash
# 30ì¼ ì´ì „ ë¡œê·¸ ì‚­ì œ
find logs/sql_queries/ -name "*.jsonl" -mtime +30 -delete
```

## ğŸ“ ë¬¸ì˜

ë¡œê¹… ì‹œìŠ¤í…œ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ê°œì„  ì œì•ˆì´ ìˆìœ¼ì‹œë©´ ê°œë°œíŒ€ì— ì—°ë½í•´ì£¼ì„¸ìš”.

