"""
LG ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (ì‹¤ì œ API ì‚¬ìš©)

API: https://api.careers.lg.com/rmk/job/retrieveJobNoticesList
ê° ê³µê³ ì˜ jobNoticeIdë¥¼ ì¶”ì¶œí•˜ì—¬ ìƒì„¸ í˜ì´ì§€ URL ìƒì„±
"""

import requests
import json
import time
from typing import List, Dict, Optional
from datetime import datetime


class LGCareerCrawler:
    def __init__(self):
        self.base_url = "https://api.careers.lg.com"
        self.list_endpoint = "/rmk/job/retrieveJobNoticesList"
        self.detail_url_template = "https://careers.lg.com/apply/detail?id={}"
        
        self.headers = {
            'authority': 'api.careers.lg.com',
            'accept': 'application/json, text/plain, */*',
            'accept-encoding': 'gzip, deflate, br, zstd',
            'accept-language': 'ko,en-US;q=0.9,en;q=0.8,ko-KR;q=0.7',
            'content-type': 'application/json',
            'cookie': 'SCOUTER=x5uv8k0el4hj7o; rmkonba=YjQ3ODMwZWUtY2I0Ny00NmNiLWE0OTMtYTI1N2I0OWIxNTg4',
            'origin': 'https://careers.lg.com',
            'referer': 'https://careers.lg.com/',
            'sec-ch-ua': '"Google Chrome";v="141", "Not?A_Brand";v="8", "Chromium";v="141"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-site',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36'
        }
    
    def get_job_list(self, page: int = 1, page_size: int = 20) -> Dict:
        """
        ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            page_size: í˜ì´ì§€ë‹¹ ê³µê³  ìˆ˜
            
        Returns:
            API ì‘ë‹µ ë°ì´í„°
        """
        url = f"{self.base_url}{self.list_endpoint}"
        
        # ì‹¤ì œ API ìš”ì²­ ë°”ë”” (ê°œë°œì ë„êµ¬ì—ì„œ í™•ì¸í•œ ì‹¤ì œ payload)
        payload = {
            "lnbSearch": "",
            "hashTagText": "",
            "recDate": "CREATION_DATE",
            "order": "DESC",
            "careerList": [],
            "companyCodeList": [],
            "desireLocList": [],
            "jobGroupList": []
        }
        
        try:
            print(f"ğŸ“¡ API í˜¸ì¶œ ì¤‘... (í˜ì´ì§€: {page})")
            response = requests.post(
                url,
                headers=self.headers,
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                return data
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                print(f"ì‘ë‹µ: {response.text}")
                return {}
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return {}
    
    def parse_job_list(self, api_response: Dict) -> List[Dict]:
        """
        API ì‘ë‹µì—ì„œ ì±„ìš©ê³µê³  ì •ë³´ íŒŒì‹±
        
        Args:
            api_response: API ì‘ë‹µ ë°ì´í„°
            
        Returns:
            íŒŒì‹±ëœ ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸
        """
        jobs = []
        
        try:
            # data.jobNoticeListì—ì„œ ê³µê³  ëª©ë¡ ì¶”ì¶œ
            job_notice_list = api_response.get('data', {}).get('jobNoticeList', [])
            
            for job in job_notice_list:
                job_info = {
                    'id': job.get('jobNoticeId'),
                    'title': job.get('jobNoticeName'),
                    'company': job.get('companyName'),
                    'career_type': job.get('careerTypeName'),
                    'job_group': job.get('jobGroupName'),
                    'status': job.get('noticeStatusName'),
                    'deadline': job.get('recEndDateTime'),
                    'url': self.detail_url_template.format(job.get('jobNoticeId'))
                }
                
                jobs.append(job_info)
            
            return jobs
            
        except Exception as e:
            print(f"âŒ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def crawl_page(self, page: int = 1, page_size: int = 20) -> List[Dict]:
        """
        íŠ¹ì • í˜ì´ì§€ì˜ ì±„ìš©ê³µê³  í¬ë¡¤ë§
        
        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸
            page_size: í˜ì´ì§€ë‹¹ ê³µê³  ìˆ˜
            
        Returns:
            ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸
        """
        # API í˜¸ì¶œ
        api_response = self.get_job_list(page, page_size)
        
        if not api_response:
            return []
        
        # ë°ì´í„° íŒŒì‹±
        jobs = self.parse_job_list(api_response)
        
        return jobs
    
    def crawl_all_pages(self, max_pages: int = None) -> List[Dict]:
        """
        ëª¨ë“  í˜ì´ì§€ì˜ ì±„ìš©ê³µê³  í¬ë¡¤ë§
        
        Args:
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ì „ì²´ ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸
        """
        all_jobs = []
        page = 1
        
        print("ğŸš€ LG ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì‹œì‘...\n")
        
        while True:
            # í˜ì´ì§€ í¬ë¡¤ë§
            jobs = self.crawl_page(page)
            
            if not jobs:
                print(f"âœ… ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {page-1}í˜ì´ì§€)")
                break
            
            all_jobs.extend(jobs)
            print(f"ğŸ“„ {page}í˜ì´ì§€: {len(jobs)}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(all_jobs)}ê°œ)")
            
            # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì²´í¬
            if max_pages and page >= max_pages:
                print(f"âœ… ì„¤ì •í•œ ìµœëŒ€ í˜ì´ì§€({max_pages})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break
            
            page += 1
            
            # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸° ìœ„í•œ ë”œë ˆì´
            time.sleep(1)
        
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_jobs)}ê°œ ê³µê³  ìˆ˜ì§‘")
        return all_jobs
    
    def print_job_info(self, jobs: List[Dict]):
        """ì±„ìš©ê³µê³  ì •ë³´ë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“‹ LG ì±„ìš©ê³µê³  ëª©ë¡")
        print("="*80 + "\n")
        
        for idx, job in enumerate(jobs, 1):
            print(f"{idx}. {job['title']}")
            print(f"   íšŒì‚¬: {job['company']}")
            print(f"   êµ¬ë¶„: {job['career_type']}")
            print(f"   ì§êµ°: {job['job_group']}")
            print(f"   ìƒíƒœ: {job['status']}")
            print(f"   ë§ˆê°: {job['deadline']}")
            print(f"   ğŸ”— {job['url']}")
            print()
    
    def save_to_json(self, jobs: List[Dict], filename: str = "lg_jobs.json"):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(jobs, f, ensure_ascii=False, indent=2)
            print(f"ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_to_csv(self, jobs: List[Dict], filename: str = "lg_jobs.csv"):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        try:
            import csv
            
            if not jobs:
                print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=jobs[0].keys())
                writer.writeheader()
                writer.writerows(jobs)
            
            print(f"ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_job_detail(self, job_id: str) -> Optional[str]:
        """
        ê°œë³„ ì±„ìš©ê³µê³  ìƒì„¸ í˜ì´ì§€ URL ë°˜í™˜
        
        Args:
            job_id: ì±„ìš©ê³µê³  ID (jobNoticeId)
            
        Returns:
            ìƒì„¸ í˜ì´ì§€ URL
        """
        return self.detail_url_template.format(job_id)
    
    def filter_jobs(self, jobs: List[Dict], **filters) -> List[Dict]:
        """
        ì±„ìš©ê³µê³  í•„í„°ë§
        
        Args:
            jobs: ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸
            **filters: í•„í„° ì¡°ê±´ (company, career_type, status ë“±)
            
        Returns:
            í•„í„°ë§ëœ ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸
        """
        filtered = jobs
        
        for key, value in filters.items():
            if value:
                filtered = [job for job in filtered if value.lower() in str(job.get(key, '')).lower()]
        
        return filtered


def main():
    """ì‹¤í–‰ ì˜ˆì œ"""
    
    print("="*80)
    print("LG ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬")
    print("="*80)
    print()
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = LGCareerCrawler()
    
    # ë°©ë²• 1: ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ê¸°
    print("ğŸ“‹ ì²« í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...\n")
    jobs = crawler.crawl_page(page=1, page_size=20)
    
    if jobs:
        crawler.print_job_info(jobs)
        
        # íŠ¹ì • íšŒì‚¬ë§Œ í•„í„°ë§
        # lg_electronics = crawler.filter_jobs(jobs, company="LGì „ì")
        # crawler.print_job_info(lg_electronics)
    
    # ë°©ë²• 2: ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 3í˜ì´ì§€)
    # all_jobs = crawler.crawl_all_pages(max_pages=3)
    # crawler.print_job_info(all_jobs)
    
    # ë°©ë²• 3: ì €ì¥í•˜ê¸°
    if jobs:
        crawler.save_to_json(jobs, "lg_jobs.json")
        crawler.save_to_csv(jobs, "lg_jobs.csv")
    
    # ë°©ë²• 4: ê°œë³„ ê³µê³  URL ìƒì„±
    if jobs:
        print("\n" + "="*80)
        print("ğŸ”— ìƒì„¸ í˜ì´ì§€ URL ì˜ˆì‹œ")
        print("="*80 + "\n")
        
        for i, job in enumerate(jobs[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ
            detail_url = crawler.get_job_detail(job['id'])
            print(f"{i}. {job['title']}")
            print(f"   {detail_url}\n")


if __name__ == "__main__":
    main()