"""
í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸

ì „ì²´ í”„ë¡œì„¸ìŠ¤:
1. í¬ë¡¤ëŸ¬ ì‹¤í–‰ (call_all_crawler.py) â†’ data/output/*_jobs.json ìƒì„±
2. ìŠ¤í‚¬ì…‹ ì¶”ì¶œ (extract_skillsets_async.py) â†’ skill_set_info ì¶”ê°€ (ë¹„ë™ê¸°)
3. ì§ë¬´ ë§¤ì¹­ (job_matching_system.py) â†’ ë§¤ì¹­ ê²°ê³¼ ìƒì„±

ê° ë‹¨ê³„ëŠ” ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ì´ì „ ë‹¨ê³„ì˜ ì¶œë ¥ì´ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ë©ë‹ˆë‹¤.
"""

import sys
import logging
import time
import subprocess
import os
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
backend_root = Path(__file__).resolve().parents[3]
if str(backend_root) not in sys.path:
    sys.path.insert(0, str(backend_root))

# ê° ë‹¨ê³„ì˜ ëª¨ë“ˆ import
from app.services.crawler.call_all_crawler import run_all_crawlers_sequentially
from app.utils.parser.extract_skillsets import main as extract_skillsets_main_async
from app.core.job_matching.job_matching_system import JobMatchingSystem
from app.scripts.db.insert_post_to_db import main as insert_post_main


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)


class DataPipeline:
    """í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        skip_crawler: bool = False,
        skip_skillset: bool = False,
        skip_post_insert: bool = False
    ):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            skip_crawler: Trueë©´ í¬ë¡¤ë§ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ìš©)
            skip_skillset: Trueë©´ ìŠ¤í‚¬ì…‹ ì¶”ì¶œ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ìš©)
            skip_post_insert: Trueë©´ ì±„ìš©ê³µê³  DB ì‚½ì… ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
        """
        self.skip_crawler = skip_crawler
        self.skip_skillset = skip_skillset
        self.skip_post_insert = skip_post_insert
        self.backend_root = Path(__file__).resolve().parents[3]
        self.data_output_dir = self.backend_root / 'data' / 'output'
        self.data_dir = self.backend_root / 'data'
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì  ê¸°ë¡)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.pipeline_results: Dict[str, Any] = {
            'timestamp': self.timestamp,
            'crawler': None,
            'skillset_extraction': None,
            'job_matching': None,
            'post_insert': None,
        }
    
    def _check_prerequisites(self) -> bool:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì „ ì‚¬ì „ ì²´í¬
        
        Returns:
            True: ëª¨ë“  ì‚¬ì „ ì¡°ê±´ ì¶©ì¡±
            False: ì‚¬ì „ ì¡°ê±´ ë¯¸ì¶©ì¡±
        """
        logger.info("\n" + "="*80)
        logger.info("ğŸ” ì‚¬ì „ ì¡°ê±´ ì²´í¬")
        logger.info("="*80)
        
        all_passed = True
        
        # 1. OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì²´í¬ (ìŠ¤í‚¬ì…‹ ì¶”ì¶œì— í•„ìš”)
        if not self.skip_skillset:
            if not os.getenv('OPENAI_API_KEY'):
                logger.error("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.error("   ìŠ¤í‚¬ì…‹ ì¶”ì¶œì— í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
                all_passed = False
            else:
                logger.info("âœ… OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í™•ì¸")
        
        # 2. Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì—¬ë¶€ ì²´í¬ (í¬ë¡¤ë§ì— í•„ìš”)
        if not self.skip_crawler:
            playwright_installed = self._check_playwright_browsers()
            if not playwright_installed:
                logger.error("âŒ Playwright ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.error("   ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
                logger.error("   playwright install")
                logger.error("   ë˜ëŠ”: python -m playwright install")
                all_passed = False
            else:
                logger.info("âœ… Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í™•ì¸")
        
        # 3. í•„ìˆ˜ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
        if not self.data_dir.exists():
            logger.error(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            all_passed = False
        else:
            logger.info(f"âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸: {self.data_dir}")
        
        # 4. ì§ë¬´ ì •ì˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
        job_desc_file = self.data_dir / 'new_job_description.json'
        job_desc_file_alt = self.data_dir / 'description.json'
        
        if not job_desc_file.exists() and not job_desc_file_alt.exists():
            logger.error(f"âŒ ì§ë¬´ ì •ì˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤:")
            logger.error(f"   {job_desc_file} ë˜ëŠ”")
            logger.error(f"   {job_desc_file_alt}")
            all_passed = False
        else:
            logger.info("âœ… ì§ë¬´ ì •ì˜ íŒŒì¼ í™•ì¸")
        
        # 5. output ë””ë ‰í† ë¦¬ ìƒì„±
        self.data_output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸: {self.data_output_dir}")
        
        logger.info("="*80)
        
        if not all_passed:
            logger.error("\nâŒ ì‚¬ì „ ì¡°ê±´ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        return all_passed
    
    def _check_playwright_browsers(self) -> bool:
        """
        Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
        
        Returns:
            True: ë¸Œë¼ìš°ì € ì„¤ì¹˜ë¨
            False: ë¸Œë¼ìš°ì € ë¯¸ì„¤ì¹˜
        """
        try:
            # playwright ë¸Œë¼ìš°ì € ëª©ë¡ í™•ì¸ ì‹œë„
            result = subprocess.run(
                [sys.executable, '-m', 'playwright', 'install', '--dry-run'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            # dry-runì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  "is already installed" ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì„¤ì¹˜ë¨
            # ë˜ëŠ” AppData ê²½ë¡œì— playwright ë¸Œë¼ìš°ì € í´ë”ê°€ ìˆëŠ”ì§€ ì§ì ‘ í™•ì¸
            if sys.platform == 'win32':
                playwright_dir = Path.home() / 'AppData' / 'Local' / 'ms-playwright'
            elif sys.platform == 'darwin':
                playwright_dir = Path.home() / 'Library' / 'Caches' / 'ms-playwright'
            else:
                playwright_dir = Path.home() / '.cache' / 'ms-playwright'
            
            if playwright_dir.exists():
                # chromium í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
                chromium_dirs = list(playwright_dir.glob('chromium*'))
                return len(chromium_dirs) > 0
            
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ Playwright ì„¤ì¹˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            # í™•ì¸ ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ì§„í–‰ (ì‹¤ì œ ì‹¤í–‰ ì‹œ ì—ëŸ¬ ë°œìƒí•˜ë©´ ê·¸ë•Œ ì¤‘ë‹¨)
            return True
    
    def run(self) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Returns:
            ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("="*80)
        logger.info(f"ğŸš€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘ ({self.timestamp})")
        logger.info("="*80)
        
        # ì‚¬ì „ ì¡°ê±´ ì²´í¬
        if not self._check_prerequisites():
            self.pipeline_results['status'] = 'failed'
            self.pipeline_results['error'] = 'ì‚¬ì „ ì¡°ê±´ ë¯¸ì¶©ì¡±'
            return self.pipeline_results
        
        start_time = time.time()
        
        try:
            # Step 1: í¬ë¡¤ëŸ¬ ì‹¤í–‰
            if not self.skip_crawler:
                success = self._run_crawler()
                if not success:
                    logger.error("\nâŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    self.pipeline_results['status'] = 'failed'
                    return self.pipeline_results
            else:
                logger.info("\n[Step 1] í¬ë¡¤ëŸ¬ ì‹¤í–‰ - SKIPPED")
                self.pipeline_results['crawler'] = {'status': 'skipped'}
            
            # Step 2: ìŠ¤í‚¬ì…‹ ì¶”ì¶œ (ë¹„ë™ê¸°)
            if not self.skip_skillset:
                success = self._run_skillset_extraction()
                if not success:
                    logger.error("\nâŒ ìŠ¤í‚¬ì…‹ ì¶”ì¶œ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    self.pipeline_results['status'] = 'failed'
                    return self.pipeline_results
            else:
                logger.info("\n[Step 2] ìŠ¤í‚¬ì…‹ ì¶”ì¶œ - SKIPPED")
                self.pipeline_results['skillset_extraction'] = {'status': 'skipped'}
            
            # Step 3: ì§ë¬´ ë§¤ì¹­
            success = self._run_job_matching()
            if not success:
                logger.error("\nâŒ ì§ë¬´ ë§¤ì¹­ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                self.pipeline_results['status'] = 'failed'
                return self.pipeline_results
            
            # Step 4: ì±„ìš©ê³µê³  DB ì‚½ì… (ì˜µì…˜)
            if not self.skip_post_insert:
                success = self._run_post_insert()
                if not success:
                    logger.error("\nâŒ ì±„ìš©ê³µê³  DB ì‚½ì… ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    self.pipeline_results['status'] = 'failed'
                    return self.pipeline_results
            else:
                logger.info("\n[Step 4] ì±„ìš©ê³µê³  DB ì‚½ì… - SKIPPED")
                self.pipeline_results['post_insert'] = {'status': 'skipped'}
            
            # ì´ ì‹¤í–‰ ì‹œê°„
            duration = time.time() - start_time
            self.pipeline_results['total_duration_minutes'] = round(duration / 60, 2)
            
            logger.info("\n" + "="*80)
            logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì´ {duration/60:.1f}ë¶„)")
            logger.info("="*80)
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            self._print_summary()
            
            # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
            self._save_pipeline_results()
            
            return self.pipeline_results
            
        except Exception as e:
            logger.error(f"\n{'='*80}")
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
            logger.error(f"{'='*80}")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"{'='*80}\n")
            
            import traceback
            traceback.print_exc()
            
            self.pipeline_results['error'] = str(e)
            self.pipeline_results['status'] = 'failed'
            
            return self.pipeline_results
    
    def _run_crawler(self) -> bool:
        """
        Step 1: í¬ë¡¤ëŸ¬ ì‹¤í–‰
        
        Returns:
            True: ì„±ê³µ
            False: ì‹¤íŒ¨
        """
        logger.info("\n" + "="*80)
        logger.info("[Step 1/4] í¬ë¡¤ëŸ¬ ì‹¤í–‰")
        logger.info("="*80)
        
        step_start = time.time()
        
        try:
            # call_all_crawler.pyì˜ run_all_crawlers_sequentially() ì‹¤í–‰
            run_all_crawlers_sequentially()
            
            duration = time.time() - step_start
            
            # ìƒì„±ëœ íŒŒì¼ í™•ì¸
            job_files = list(self.data_output_dir.glob('*_jobs.json'))
            
            if len(job_files) == 0:
                logger.error("âŒ í¬ë¡¤ë§ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ìƒì„±ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                self.pipeline_results['crawler'] = {
                    'status': 'failed',
                    'error': 'ìƒì„±ëœ íŒŒì¼ ì—†ìŒ',
                    'duration_minutes': round(duration / 60, 2),
                }
                return False
            
            self.pipeline_results['crawler'] = {
                'status': 'success',
                'duration_minutes': round(duration / 60, 2),
                'output_files': [f.name for f in job_files],
                'output_count': len(job_files),
            }
            
            logger.info(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ ({duration/60:.1f}ë¶„)")
            logger.info(f"   ìƒì„±ëœ íŒŒì¼: {len(job_files)}ê°œ")
            for f in job_files:
                logger.info(f"   - {f.name}")
            
            return True
            
        except Exception as e:
            duration = time.time() - step_start
            logger.error(f"\n{'='*80}")
            logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            logger.error(f"{'='*80}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"ì†Œìš” ì‹œê°„: {duration/60:.1f}ë¶„")
            logger.error(f"{'='*80}\n")
            
            import traceback
            traceback.print_exc()
            
            self.pipeline_results['crawler'] = {
                'status': 'failed',
                'error': str(e),
                'error_type': type(e).__name__,
                'duration_minutes': round(duration / 60, 2),
            }
            
            return False
    
    def _run_skillset_extraction(self) -> bool:
        """
        Step 2: ìŠ¤í‚¬ì…‹ ì¶”ì¶œ (ë¹„ë™ê¸° ë²„ì „)
        
        Returns:
            True: ì„±ê³µ
            False: ì‹¤íŒ¨
        """
        logger.info("\n" + "="*80)
        logger.info("[Step 2/4] ìŠ¤í‚¬ì…‹ ì¶”ì¶œ (LLM ê¸°ë°˜ - ë¹„ë™ê¸°)")
        logger.info("="*80)
        
        step_start = time.time()
        
        try:
            # extract_skillsets_async.pyì˜ main() ì‹¤í–‰ (ë¹„ë™ê¸°)
            # asyncio.run()ìœ¼ë¡œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
            extract_skillsets_main_async()
            
            duration = time.time() - step_start
            
            # ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸
            job_files = list(self.data_output_dir.glob('*_jobs.json'))
            
            if len(job_files) == 0:
                logger.error("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                self.pipeline_results['skillset_extraction'] = {
                    'status': 'failed',
                    'error': 'ì²˜ë¦¬í•  íŒŒì¼ ì—†ìŒ',
                    'duration_minutes': round(duration / 60, 2),
                }
                return False
            
            self.pipeline_results['skillset_extraction'] = {
                'status': 'success',
                'duration_minutes': round(duration / 60, 2),
                'processed_files': [f.name for f in job_files],
                'processed_count': len(job_files),
            }
            
            logger.info(f"\nâœ… ìŠ¤í‚¬ì…‹ ì¶”ì¶œ ì™„ë£Œ ({duration/60:.1f}ë¶„)")
            logger.info(f"   ì²˜ë¦¬ëœ íŒŒì¼: {len(job_files)}ê°œ")
            logger.info(f"   âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ìµœëŒ€ 30ê°œì”© ë™ì‹œ ì‹¤í–‰")
            
            return True
            
        except Exception as e:
            duration = time.time() - step_start
            logger.error(f"\n{'='*80}")
            logger.error(f"âŒ ìŠ¤í‚¬ì…‹ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            logger.error(f"{'='*80}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"ì†Œìš” ì‹œê°„: {duration/60:.1f}ë¶„")
            logger.error(f"{'='*80}\n")
            
            import traceback
            traceback.print_exc()
            
            self.pipeline_results['skillset_extraction'] = {
                'status': 'failed',
                'error': str(e),
                'error_type': type(e).__name__,
                'duration_minutes': round(duration / 60, 2),
            }
            
            return False
    
    def _run_job_matching(self) -> bool:
        """
        Step 3: ì§ë¬´ ë§¤ì¹­
        
        Returns:
            True: ì„±ê³µ
            False: ì‹¤íŒ¨
        """
        logger.info("\n" + "="*80)
        logger.info("[Step 3/4] ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ")
        logger.info("="*80)
        
        step_start = time.time()
        
        try:
            # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            log_file = self.data_output_dir / f"job_matching_log_{self.timestamp}.txt"
            
            # JobMatchingSystem ì´ˆê¸°í™”
            system = JobMatchingSystem(log_file=str(log_file))
            
            # ì§ë¬´ ì •ì˜ ë¡œë“œ
            logger.info("\n[3-1] ì§ë¬´ ì •ì˜ ë¡œë“œ")
            system.load_job_descriptions()
            
            # í•™ìŠµ ë°ì´í„° ë¡œë“œ
            logger.info("\n[3-2] í•™ìŠµ ë°ì´í„° ë¡œë“œ")
            job_files = list(self.data_output_dir.glob('*_jobs.json'))
            training_files = [str(f) for f in job_files]
            system.load_training_data(training_files)
            
            # ê·¸ë˜í”„ êµ¬ì¶•
            logger.info("\n[3-3] ê·¸ë˜í”„ êµ¬ì¶•")
            system.build_graph()
            
            # Matchers ì´ˆê¸°í™” (Louvain + SBERT)
            logger.info("\n[3-4] Matchers ì´ˆê¸°í™” (Louvain + SBERT)")
            system.build_matchers()
            
            logger.info("\n[3-5] ì§ë¬´ ë§¤ì¹­ ì‹¤í–‰")
            logger.info("="*80)
            
            # ê° íŒŒì¼ì— ëŒ€í•´ ë§¤ì¹­ ì‹¤í–‰
            all_matching_results = []
            
            for job_file in job_files:
                logger.info(f"\nì²˜ë¦¬ ì¤‘: {job_file.name}")
                
                try:
                    results = system.match_company_jobs(
                        str(job_file),
                        ppr_top_n=20,
                        final_top_k=2,
                    )
                    
                    all_matching_results.append({
                        'file': job_file.name,
                        'results': results,
                    })
                    
                    # ë§¤ì¹­ ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ì— ë³‘í•©
                    self._merge_matching_results_to_original(job_file, results)
                    
                    logger.info(f"   âœ… ë§¤ì¹­ ê²°ê³¼ê°€ ì›ë³¸ íŒŒì¼ì— ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤: {job_file.name}")
                    
                except Exception as e:
                    logger.error(f"   âŒ {job_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            duration = time.time() - step_start
            
            self.pipeline_results['job_matching'] = {
                'status': 'success',
                'duration_minutes': round(duration / 60, 2),
                'processed_files': len(all_matching_results),
                'log_file': log_file.name,
            }
            
            logger.info(f"\nâœ… ì§ë¬´ ë§¤ì¹­ ì™„ë£Œ ({duration/60:.1f}ë¶„)")
            logger.info(f"   ë¡œê·¸ íŒŒì¼: {log_file.name}")
            
            return True
            
        except Exception as e:
            duration = time.time() - step_start
            logger.error(f"\n{'='*80}")
            logger.error(f"âŒ ì§ë¬´ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            logger.error(f"{'='*80}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"ì†Œìš” ì‹œê°„: {duration/60:.1f}ë¶„")
            logger.error(f"{'='*80}\n")
            
            import traceback
            traceback.print_exc()
            
            self.pipeline_results['job_matching'] = {
                'status': 'failed',
                'error': str(e),
                'error_type': type(e).__name__,
                'duration_minutes': round(duration / 60, 2),
            }
            
            return False
    
    def _run_post_insert(self) -> bool:
        """
        Step 4: ì±„ìš©ê³µê³  DB ì‚½ì…
        
        Returns:
            True: ì„±ê³µ
            False: ì‹¤íŒ¨
        """
        logger.info("\n" + "="*80)
        logger.info("[Step 4/4] ì±„ìš©ê³µê³  DB ì‚½ì…")
        logger.info("="*80)
        
        step_start = time.time()
        
        try:
            # data/output/*_jobs.json íŒŒì¼ í™•ì¸
            job_files = list(self.data_output_dir.glob('*_jobs.json'))
            
            if len(job_files) == 0:
                logger.error("âŒ ì‚½ì…í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë‹¨ê³„ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                self.pipeline_results['post_insert'] = {
                    'status': 'failed',
                    'error': 'ì‚½ì…í•  íŒŒì¼ ì—†ìŒ'
                }
                return False
            
            logger.info(f"ì‚½ì…í•  íŒŒì¼: {len(job_files)}ê°œ")
            for f in job_files:
                logger.info(f"  - {f.name}")
            
            # insert_post_to_dbì˜ main() ì‹¤í–‰
            insert_post_main()
            
            duration = time.time() - step_start
            
            self.pipeline_results['post_insert'] = {
                'status': 'success',
                'duration_minutes': round(duration / 60, 2),
                'inserted_files': [f.name for f in job_files],
                'file_count': len(job_files),
            }
            
            logger.info(f"\nâœ… ì±„ìš©ê³µê³  DB ì‚½ì… ì™„ë£Œ ({duration/60:.1f}ë¶„)")
            
            return True
            
        except Exception as e:
            duration = time.time() - step_start
            logger.error(f"\n{'='*80}")
            logger.error(f"âŒ ì±„ìš©ê³µê³  DB ì‚½ì… ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            logger.error(f"{'='*80}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"ì†Œìš” ì‹œê°„: {duration/60:.1f}ë¶„")
            logger.error(f"{'='*80}\n")
            
            import traceback
            traceback.print_exc()
            
            self.pipeline_results['post_insert'] = {
                'status': 'failed',
                'error': str(e),
                'error_type': type(e).__name__,
                'duration_minutes': round(duration / 60, 2),
            }
            
            return False
    
    def _merge_matching_results_to_original(self, job_file: Path, results: List[Dict]) -> None:
        """
        ë§¤ì¹­ ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ì— ë³‘í•©
        
        Args:
            job_file: ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            results: ë§¤ì¹­ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì›ë³¸ íŒŒì¼ ë¡œë“œ
        with open(job_file, 'r', encoding='utf-8') as f:
            original_jobs = json.load(f)
        
        # ë§¤ì¹­ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (company, title, urlì„ í‚¤ë¡œ ì‚¬ìš©)
        matching_dict = {}
        for result_item in results:
            if result_item.get('db_result'):
                posting = result_item['posting']
                # í‚¤ ìƒì„±: company + title + urlì˜ ì¡°í•©
                key = self._create_job_key(posting.company, posting.title, posting.url)
                matching_dict[key] = result_item['db_result']
        
        # ì›ë³¸ ë°ì´í„°ì— ë§¤ì¹­ ê²°ê³¼ ë³‘í•©
        merged_count = 0
        for job in original_jobs:
            key = self._create_job_key(
                job.get('company', ''),
                job.get('title', ''),
                job.get('url', '')
            )
            
            if key in matching_dict:
                match_result = matching_dict[key]
                # sim_position, sim_industry, sim_score, sim_skill_matching ì¶”ê°€
                job['sim_position'] = match_result.get('position')
                job['sim_industry'] = match_result.get('industry')
                job['sim_score'] = match_result.get('sim_score')
                job['sim_skill_matching'] = match_result.get('sim_skill_matching', [])
                merged_count += 1
        
        # ë³‘í•©ëœ ë°ì´í„°ë¥¼ ì›ë³¸ íŒŒì¼ì— ì €ì¥ (ë®ì–´ì“°ê¸°)
        with open(job_file, 'w', encoding='utf-8') as f:
            json.dump(original_jobs, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   ğŸ“Š {merged_count}/{len(original_jobs)}ê°œ ì±„ìš©ê³µê³ ì— ë§¤ì¹­ ê²°ê³¼ ë³‘í•© ì™„ë£Œ")
        
        # ë³„ë„ë¡œ ë§¤ì¹­ ê²°ê³¼ë§Œ ì €ì¥ (ë°±ì—…/ì°¸ê³ ìš©)
        backup_file = self.data_output_dir / f"matched_{job_file.stem}_{self.timestamp}.json"
        db_results = []
        for result_item in results:
            if result_item.get('db_result'):
                db_entry = {
                    'company': result_item['posting'].company,
                    'title': result_item['posting'].title,
                    'url': result_item['posting'].url,
                    'skills': result_item['posting'].skills,
                    **result_item['db_result']
                }
                db_results.append(db_entry)
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(db_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   ğŸ’¾ ë§¤ì¹­ ê²°ê³¼ ë°±ì—…: {backup_file.name}")
    
    def _create_job_key(self, company: str, title: str, url: str) -> str:
        """
        ì±„ìš©ê³µê³ ë¥¼ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” í‚¤ ìƒì„±
        
        Args:
            company: íšŒì‚¬ëª…
            title: ì±„ìš©ê³µê³  ì œëª©
            url: URL
            
        Returns:
            ê³ ìœ  í‚¤ ë¬¸ìì—´
        """
        # ê³µë°± ì œê±° ë° ì†Œë¬¸ì ë³€í™˜ìœ¼ë¡œ ì •ê·œí™”
        normalized_company = company.strip().lower()
        normalized_title = title.strip().lower()
        normalized_url = url.strip().lower()
        
        return f"{normalized_company}||{normalized_title}||{normalized_url}"
    
    def _print_summary(self):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìš”ì•½")
        logger.info("="*80)
        
        for step_name, result in self.pipeline_results.items():
            if step_name in ['timestamp', 'total_duration_minutes']:
                continue
            
            if result and isinstance(result, dict):
                status = result.get('status', 'unknown')
                status_emoji = "âœ…" if status == 'success' else "â­ï¸" if status == 'skipped' else "âŒ"
                
                logger.info(f"\n{step_name.upper()}:")
                logger.info(f"  ìƒíƒœ: {status_emoji} {status}")
                
                if 'duration_minutes' in result:
                    logger.info(f"  ì†Œìš” ì‹œê°„: {result['duration_minutes']:.1f}ë¶„")
                
                if 'output_count' in result:
                    logger.info(f"  ìƒì„± íŒŒì¼: {result['output_count']}ê°œ")
                
                if 'processed_count' in result:
                    logger.info(f"  ì²˜ë¦¬ íŒŒì¼: {result['processed_count']}ê°œ")
                
                if 'error' in result:
                    logger.info(f"  ì˜¤ë¥˜: {result['error']}")
        
        if 'total_duration_minutes' in self.pipeline_results:
            logger.info(f"\nì´ ì‹¤í–‰ ì‹œê°„: {self.pipeline_results['total_duration_minutes']:.1f}ë¶„")
        
        logger.info("="*80)
    
    def _save_pipeline_results(self):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        result_file = self.data_output_dir / f"pipeline_results_{self.timestamp}.json"
        
        # NewJobPosting ë“±ì˜ ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
        serializable_results = self._make_serializable(self.pipeline_results)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥: {result_file.name}")
    
    def _make_serializable(self, obj):
        """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            return self._make_serializable(obj.__dict__)
        else:
            return obj


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    ëª…ë ¹ì¤„ ì¸ì:
        --skip-crawler: í¬ë¡¤ë§ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
        --skip-skillset: ìŠ¤í‚¬ì…‹ ì¶”ì¶œ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
        --skip-post-insert: ì±„ìš©ê³µê³  DB ì‚½ì… ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
    
    Note:
        ì§ë¬´ ì •ì˜ DB ì‚½ì…(insert_job_description_to_db)ì€ ë³„ë„ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:
        python -m app.scripts.db.insert_job_description_to_db
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰')
    parser.add_argument('--skip-crawler', action='store_true', help='í¬ë¡¤ë§ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-skillset', action='store_true', help='ìŠ¤í‚¬ì…‹ ì¶”ì¶œ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-post-insert', action='store_true', help='ì±„ìš©ê³µê³  DB ì‚½ì… ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = DataPipeline(
        skip_crawler=args.skip_crawler,
        skip_skillset=args.skip_skillset,
        skip_post_insert=args.skip_post_insert,
    )
    
    results = pipeline.run()
    
    # ì„±ê³µ ì—¬ë¶€ì— ë”°ë¼ ì¢…ë£Œ ì½”ë“œ ë°˜í™˜
    if results.get('status') == 'failed':
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == '__main__':
    main()