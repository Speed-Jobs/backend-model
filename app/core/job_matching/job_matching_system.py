"""
ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ v13 - PPR í•„í„°ë§ ì „ìš© ë²„ì „

í•µì‹¬ ì „ëµ:
1. PPRì€ "í•„í„°ë§ ì „ìš©" - specific_skillsë§Œ ì‚¬ìš©, ì ìˆ˜ ë²„ë¦¼, ìƒìœ„ 20ê°œë§Œ ì¶”ì¶œ
2. Jaccard + SBERTë¡œë§Œ ì ìˆ˜ ê³„ì‚° - ì „ì²´ ìŠ¤í‚¬(common + specific) ì‚¬ìš©
3. ê°€ì¤‘ì¹˜ ì—†ì´ ë‹¨ìˆœ í•©ì‚° (0~2 ë²”ìœ„)

ì£¼ìš” ë³€ê²½ì‚¬í•­ (v7 â†’ v13):
1. PPR ì—­í•  ë³€ê²½
   - ê¸°ì¡´: ì ìˆ˜ ê³„ì‚°ì— í¬í•¨ (15%)
   - ë³€ê²½: í›„ë³´ í•„í„°ë§ ì „ìš© (ì ìˆ˜ ë²„ë¦¼)
   - ì´ìœ : ì •ê·œí™” ì‹œ Hallucination ë°œìƒ
   - PPR ê³„ì‚°: specific_skillsë§Œ ì‚¬ìš© (ì§ë¬´ íŠ¹í™” ìŠ¤í‚¬ë¡œ í•„í„°ë§)

2. Clustering ì œê±°
   - Louvain ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì œê±°
   - ì§ë¬´/ì‚°ì—… ì •ë³´ë¡œ ì¶©ë¶„í•œ êµ¬ë¶„ ê°€ëŠ¥

3. Jaccard ê°•í™”
   - Weighted Jaccard (common 0.33 + specific 0.67)
   - í•„í„°ë§ ì œê±° (ëª¨ë“  í›„ë³´ ê³„ì‚°)
   - Jaccard ê³„ì‚°: ì „ì²´ ìŠ¤í‚¬(common + specific) ì‚¬ìš©

4. ì ìˆ˜ ê³„ì‚°
   - Final = Jaccard + SBERT (0~2 ë²”ìœ„)
   - ê°€ì¤‘ì¹˜ ì—†ìŒ (ë‹¨ìˆœ í•©ì‚°)

ì ìˆ˜ êµ¬ì„±:
- PPR: í•„í„°ë§ ì „ìš© (specific_skillsë¡œ ìƒìœ„ 20ê°œ ì¶”ì¶œ)
- Jaccard: ìŠ¤í‚¬ ì§ì ‘ ë§¤ì¹­ (common + specific ì „ì²´, 0~1)
- SBERT: ì˜ë¯¸ ìœ ì‚¬ë„ (0~1)
- Final: Jaccard + SBERT (0~2)

ì¥ì :
- PPR Hallucination ì œê±°
- Jaccardë¡œ ìŠ¤í‚¬ ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ
- SBERT ë†’ì€ ì„±ëŠ¥ í™œìš©
- ë‹¨ìˆœí•˜ê³  íˆ¬ëª…í•œ ì ìˆ˜ ì²´ê³„
"""

import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Optional, Any
from sqlalchemy.orm import Session
from dataclasses import dataclass, field
from datetime import datetime

import numpy as np
import networkx as nx
# ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ import (HTTP í†µì‹ )
try:
    from app.utils.model import ModelServiceClient as SentenceTransformer
    USE_MODEL_SERVICE = True
except ImportError:
    # Fallback: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
    from sentence_transformers import SentenceTransformer
    USE_MODEL_SERVICE = False

from app.config.job_matching.config import (
    JOB_DESCRIPTION_FILE,
    SBERT_MODEL_NAME,
    TRAINING_DATA_FILES,
)

# ============================================================================
# Output Logger (Terminal + File)
# ============================================================================

class OutputLogger:
    """Terminalê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥"""

    def __init__(self, log_file: str):
        self.terminal = sys.stdout
        self.log = open(log_file, 'w', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

    def close(self):
        self.log.close()

# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class JobDescription:
    """ì§ë¬´ ì •ì˜ (new_job_description.json)"""
    job_name: str
    job_definition: str
    industry: str
    common_skills: List[str]
    specific_skills: List[str]
    all_skills: List[str] = field(default_factory=list)
    specific_only_skills: List[str] = field(default_factory=list)  # PPR ì „ìš©
    skill_set_description: str = ""  # ì£¼ìš” ì—…ë¬´ ì„¤ëª… (SBERTì— ì‚¬ìš©)
    common_skill_set_description: str = ""  # ê³µí†µ ìŠ¤í‚¬ ì„¤ëª… (SBERTì— ì‚¬ìš©)

    def __post_init__(self):
        # v13: all_skillsëŠ” ì „ì²´ (Jaccard/ë§¤ì¹­ìš©), specific_only_skillsëŠ” PPR ì „ìš©
        self.all_skills = list(set(self.common_skills + self.specific_skills))
        self.specific_only_skills = list(set(self.specific_skills))

@dataclass
class NewJobPosting:
    """ìƒˆë¡œìš´ ì±„ìš©ê³µê³  (ë§¤ì¹­ ëŒ€ìƒ)"""
    posting_id: str
    company: str
    title: str
    skills: List[str]
    url: str = ""
    description: str = ""

@dataclass
class JobPosting:
    """ê¸°ì¡´ ì±„ìš©ê³µê³  (í•™ìŠµ ë°ì´í„°)"""
    posting_id: str
    company: str
    title: str
    url: str
    skills: List[str]

    def __hash__(self):
        return hash(self.posting_id)


@dataclass
class JobMatchResult:
    """ì§ë¬´ ë§¤ì¹­ ê²°ê³¼"""
    job_name: str
    industry: str
    final_score: float
    
    jaccard_score: float = 0.0
    pagerank_score: float = 0.0  # ë¡œê¹…ìš©, ì ìˆ˜ ê³„ì‚°ì—ëŠ” ë¯¸í¬í•¨
    sbert_score: float = 0.0
    
    matching_skills: List[str] = field(default_factory=list)
    missing_skills: List[str] = field(default_factory=list)
    job_definition: str = ""
    
    reason: str = ""

# ============================================================================
# Graph Infrastructure
# ============================================================================

class JobPostingGraph:
    """ì±„ìš©ê³µê³  ê·¸ë˜í”„"""

    def __init__(self):
        self.G = nx.Graph()
        self.postings: Dict[str, JobPosting] = {}

    def add_posting(self, posting: JobPosting):
        posting_node = f"posting:{posting.posting_id}"
        self.postings[posting_node] = posting
        self.G.add_node(posting_node, type='posting')

        if posting.company:
            company_node = f"company:{posting.company}"
            self.G.add_edge(posting_node, company_node, weight=1.0)

        for skill in posting.skills:
            skill_normalized = self._normalize_skill(skill)
            if skill_normalized:
                skill_node = f"skill:{skill_normalized}"
                self.G.add_edge(posting_node, skill_node, weight=1.0)


    @staticmethod
    def _normalize_skill(skill: str) -> str:
        return (
            skill.lower()
            .replace('-', '')
            .replace('_', '')
            .replace(' ', '')
            .replace('.', '')
            .strip()
        )


# ============================================================================
# SBERT Description Matcher
# ============================================================================

class SbertDescriptionMatcher:
    """
    Sentence-BERTë¡œ description ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
    - ì§ë¬´ ì •ì˜ í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©í•´ë‘ê³ 
    - ìƒˆ ê³µê³ ì˜ (ì œëª© + ë³¸ë¬¸ ì „ì²´) ì„ë² ë”©ê³¼ cosine similarity ê³„ì‚°
    """

    def __init__(
        self,
        job_descriptions: List[JobDescription],
        model_name: str = None,
    ):
        self.job_descriptions = job_descriptions

        # configì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        if model_name is None:
            model_name = SBERT_MODEL_NAME

        # ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™”
        if USE_MODEL_SERVICE:
            print(f"[SBERT] ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            # Kubernetes í™˜ê²½ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ URL ì§€ì •
            import os
            model_service_url = os.getenv(
                "MODEL_SERVICE_URL",
                "http://model-service.skala-practice.svc.cluster.local:8001"
            )
            self.model = SentenceTransformer(base_url=model_service_url)
        else:
            print(f"[SBERT] ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
            self.model = SentenceTransformer(model_name)

        print(f"[SBERT] ì§ë¬´ definition ì„ë² ë”© ìƒì„± ì¤‘... (ì§ë¬´ ì •ì˜ + industry + skill_set_description + ê³µí†µ_skill_set_description)")
        corpus = []
        
        for jd in job_descriptions:
            parts = []
            
            # 1. ì§ë¬´ ì •ì˜ (ê¸°ì¡´)
            if jd.job_definition:
                parts.append(jd.job_definition)
            
            # 2. Industry ì¶”ê°€ (Front-end vs Back-end êµ¬ë¶„ì— í•„ìˆ˜!)
            if jd.industry:
                parts.append(f"ì‚°ì—… ë¶„ì•¼: {jd.industry}")
            
            # 3. Skill Set Description ì¶”ê°€ (êµ¬ì²´ì ì¸ ì—…ë¬´ ì„¤ëª…)
            if jd.skill_set_description:
                parts.append(f"ì£¼ìš” ì—…ë¬´: {jd.skill_set_description}")
            
            # 4. ê³µí†µ Skill Set Description ì¶”ê°€ (í”„ë¡œê·¸ë˜ë° ì–¸ì–´, í˜‘ì—… ë„êµ¬ ë“±)
            if jd.common_skill_set_description:
                parts.append(f"ê³µí†µ ê¸°ìˆ : {jd.common_skill_set_description}")
            
            # ëª¨ë“  ì •ë³´ ê²°í•© (ì •ë³´ê°€ ì—†ìœ¼ë©´ job_nameë§Œ ì‚¬ìš©)
            combined_text = "\n\n".join(parts) if parts else jd.job_name
            corpus.append(combined_text)

        # normalize_embeddings=True â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì 
        self.job_embeddings = self.model.encode(
            corpus,
            convert_to_numpy=True,
            normalize_embeddings=True,
        )

        print(f"[OK] {len(job_descriptions)}ê°œ ì§ë¬´ ì •ì˜ ì„ë² ë”© ì™„ë£Œ (industry + skill_set_description + ê³µí†µ_skill_set_description í¬í•¨)")

    def calculate_similarity_no_normalize(self, query_text: str) -> Dict[str, float]:
        """
        ìƒˆ ê³µê³ ì˜ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì§ë¬´ ì •ì˜ì˜ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™” ì œê±°)

        Returns:
            Dict[job_name, raw_similarity_score] - ì›ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1 ë³€í™˜ë§Œ)
        """
        if not query_text or not query_text.strip():
            return {jd.job_name: 0.0 for jd in self.job_descriptions}

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_emb = self.model.encode(
            [query_text],
            convert_to_numpy=True,
            normalize_embeddings=True,
        )[0]

        # cosine similarity (normalized embeddings â†’ dot product)
        sims = np.dot(self.job_embeddings, query_emb)  # [-1, 1]

        # [-1, 1] â†’ [0, 1] ë³€í™˜ë§Œ (max ì •ê·œí™” ì œê±°!)
        sims = (sims + 1.0) / 2.0

        result = {}
        for i, jd in enumerate(self.job_descriptions):
            result[jd.job_name] = float(sims[i])

        return result


# ============================================================================
# Job Matcher (í•µì‹¬ ë¡œì§)
# ============================================================================

class JobMatcher:
    """ìƒˆ ì±„ìš©ê³µê³  â†’ ì§ë¬´ ë§¤ì¹­ (PPR í•„í„°ë§ ì „ìš©)"""

    def __init__(
        self,
        graph: JobPostingGraph,
        sbert_matcher: SbertDescriptionMatcher,
        job_descriptions: List[JobDescription],
    ):
        self.graph = graph
        self.sbert_matcher = sbert_matcher
        self.job_descriptions = job_descriptions

    def match_job(
        self,
        new_posting: NewJobPosting,
        ppr_top_n: int = 20,
        final_top_k: int = 2,
    ) -> List[JobMatchResult]:
        """
        ìƒˆ ì±„ìš©ê³µê³ ë¥¼ ì§ë¬´ì™€ ë§¤ì¹­
        
        v13: PPR í•„í„°ë§ ì „ìš© + Jaccard + SBERT
        - PPR: ìƒìœ„ 20ê°œ ì¶”ì¶œ (ì ìˆ˜ ë²„ë¦¼)
        - Jaccard + SBERT: ë‹¨ìˆœ í•©ì‚° (0~2)
        - í•„í„°ë§ ì œê±° (ëª¨ë“  í›„ë³´ ê³„ì‚°)
        """
        print(f"\n[Stage 1] PPR ê¸°ë°˜ 1ì°¨ í•„í„°ë§ (ìƒìœ„ {ppr_top_n}ê°œ ì¶”ì¶œ, ì ìˆ˜ ë²„ë¦¼)")
        
        # Stage 1: PPRë¡œ ìƒìœ„ Nê°œ ì§ë¬´ ì¶”ì¶œ (ì ìˆ˜ëŠ” ë²„ë¦¼!)
        ppr_candidates = self._get_ppr_top_jobs(new_posting, ppr_top_n)
        
        if not ppr_candidates:
            print("  ! PPR í›„ë³´ ì—†ìŒ, ì „ì²´ ì§ë¬´ ëŒ€ìƒìœ¼ë¡œ ì§„í–‰")
            ppr_candidates = [(jd, 0.0) for jd in self.job_descriptions]
        
        print(f"  [OK] {len(ppr_candidates)} jobs selected")
        
        # SBERT ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„± (title + description)
        query_text = f"{new_posting.title}\n\n{new_posting.description}".strip()
        
        # Stage 2: SBERT ê³„ì‚° (ì „ì²´ ì§ë¬´ ëŒ€ìƒ)
        print(f"\n[Stage 2] SBERT + Jaccard ì ìˆ˜ ê³„ì‚°")
        sbert_scores = self.sbert_matcher.calculate_similarity_no_normalize(query_text)
        
        if query_text:
            top_sbert = max(sbert_scores.items(), key=lambda x: x[1])
            print(f"  - SBERT 1ë“±: {top_sbert[0]} (ì ìˆ˜: {top_sbert[1]:.4f})")
        else:
            print(f"  ! Description/Title ì—†ìŒ - SBERT ì ìˆ˜ ëª¨ë‘ 0")
        
        # Stage 3: PPR í›„ë³´ë“¤ì— ëŒ€í•´ Jaccard + SBERT ê³„ì‚°
        results = []
        
        for job_desc, ppr_score in ppr_candidates:
            # Weighted Jaccard ê³„ì‚°
            jaccard = self._calculate_weighted_jaccard(new_posting.skills, job_desc)
            
            # SBERT ì ìˆ˜
            sbert = sbert_scores.get(job_desc.job_name, 0.0)
            
            # ë§¤ì¹­ ìŠ¤í‚¬ ë¶„ì„
            new_skills_norm = set(self.graph._normalize_skill(s) for s in new_posting.skills)
            job_skills_norm = set(self.graph._normalize_skill(s) for s in job_desc.all_skills)
            
            matching_skills = list(new_skills_norm & job_skills_norm)
            missing_skills = list(job_skills_norm - new_skills_norm)
            
            # v13: í•„í„°ë§ ì œê±°! ëª¨ë“  í›„ë³´ ê³„ì‚°
            
            # ìµœì¢… ì ìˆ˜: Jaccard + SBERT (0~2 ë²”ìœ„)
            final_score = jaccard + sbert
            
            result = JobMatchResult(
                job_name=job_desc.job_name,
                industry=job_desc.industry,
                final_score=final_score,
                jaccard_score=jaccard,
                pagerank_score=ppr_score,  # ë¡œê¹…ìš©
                sbert_score=sbert,
                matching_skills=matching_skills[:10],
                missing_skills=missing_skills[:5],
                job_definition=job_desc.job_definition,
                reason=self._generate_reason(matching_skills, jaccard, sbert),
            )
            
            results.append(result)
        
        # ì •ë ¬ ë° Top-K ë°˜í™˜
        results.sort(key=lambda x: x.final_score, reverse=True)
        
        print(f"  [OK] Final top {min(final_top_k, len(results))} returned")
        if results:
            # final_top_k ê°œìˆ˜ë§Œí¼ ê²°ê³¼ ì¶œë ¥
            for i in range(min(final_top_k, len(results))):
                rank = i + 1
                result = results[i]
                print(
                    f"  - {rank}ë“±: "
                    f"{result.job_name}/{result.industry}\n"
                    "         ì ìˆ˜: "
                    f"{result.final_score:.4f} "
                    f"(Jacc:{result.jaccard_score:.4f}, "
                    f"SBERT:{result.sbert_score:.4f})"
                )
        else:
            print(f"  [WARNING] ë§¤ì¹­ ê°€ëŠ¥í•œ ì§ë¬´ ì—†ìŒ")
        
        return results[:final_top_k]
    
    def _get_ppr_top_jobs(self, new_posting: NewJobPosting, top_n: int) -> List[Tuple[JobDescription, float]]:
        """
        PPRë¡œ ìƒìœ„ Nê°œ ì§ë¬´ ì¶”ì¶œ (ì ìˆ˜ëŠ” ë²„ë¦¼!)
        """
        try:
            personalization = {}
            new_skill_nodes = [
                f"skill:{self.graph._normalize_skill(s)}"
                for s in new_posting.skills
            ]
            
            for node in self.graph.G.nodes():
                if node in new_skill_nodes:
                    personalization[node] = 1.0 / len(new_skill_nodes)
                else:
                    personalization[node] = 0.0
            
            ppr = nx.pagerank(
                self.graph.G,
                personalization=personalization,
                alpha=0.85,
                max_iter=100,
                weight='weight',
            )
            
            job_ppr_scores = []
            
            for job_desc in self.job_descriptions:
                skill_nodes = [
                    f"skill:{self.graph._normalize_skill(s)}"
                    for s in job_desc.specific_only_skills  # PPRì€ specificë§Œ ì‚¬ìš©
                ]
                
                ppr_scores = [ppr.get(node, 0.0) for node in skill_nodes]
                avg_ppr = np.mean(ppr_scores) if ppr_scores else 0.0
                
                job_ppr_scores.append((job_desc, avg_ppr))
            
            # ì •ê·œí™” ì—†ì´ ê·¸ëƒ¥ ì •ë ¬ë§Œ
            job_ppr_scores.sort(key=lambda x: x[1], reverse=True)
            top_candidates = job_ppr_scores[:top_n]
            
            if top_candidates:
                print(
                    f"  - PPR 1ë“±: {top_candidates[0][0].job_name}/"
                    f"{top_candidates[0][0].industry} (ì›ë³¸ ì ìˆ˜: {top_candidates[0][1]:.6f})"
                )
                print(
                    f"  - PPR {len(top_candidates)}ë“±: {top_candidates[-1][0].job_name}/"
                    f"{top_candidates[-1][0].industry} (ì›ë³¸ ì ìˆ˜: {top_candidates[-1][1]:.6f})"
                )
            
            return top_candidates
        
        except Exception as e:
            print(f"  ! PPR ê³„ì‚° ì‹¤íŒ¨: {e}")
            return []

    def _calculate_jaccard(self, skills1: List[str], skills2: List[str]) -> float:
        """ê¸°ë³¸ Jaccard ê³„ì‚° (ë‹¨ì¼ ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸ ë¹„êµ)"""
        set1 = set(self.graph._normalize_skill(s) for s in skills1)
        set2 = set(self.graph._normalize_skill(s) for s in skills2)
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_weighted_jaccard(self, new_posting_skills: List[str], job_desc: JobDescription) -> float:
        """
        ê°€ì¤‘ì¹˜ ì ìš© Jaccard ê³„ì‚°
        
        - common_skills: 0.33 ê°€ì¤‘ì¹˜ (ëª¨ë“  ì§ë¬´ ê³µí†µ, ëœ ì¤‘ìš”)
        - specific_skills: 0.67 ê°€ì¤‘ì¹˜ (ì§ë¬´ íŠ¹í™”, ë§¤ìš° ì¤‘ìš”)
        
        Args:
            new_posting_skills: ì±„ìš©ê³µê³ ì˜ ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸
            job_desc: ì§ë¬´ ì •ì˜ ê°ì²´
            
        Returns:
            float (0~1): ê°€ì¤‘ì¹˜ ì ìš©ëœ Jaccard ì ìˆ˜
        """
        # 1. Common skills Jaccard
        jaccard_common = self._calculate_jaccard(
            new_posting_skills, 
            job_desc.common_skills
        )
        
        # 2. Specific skills Jaccard
        jaccard_specific = self._calculate_jaccard(
            new_posting_skills, 
            job_desc.specific_skills
        )
        
        # 3. ê°€ì¤‘ í‰ê·  (specificì— 2ë°° ê°€ì¤‘ì¹˜)
        weighted_jaccard = 0.33 * jaccard_common + 0.67 * jaccard_specific
        
        return weighted_jaccard

    def _generate_reason(self, matching_skills: List[str], jaccard: float, sbert: float) -> str:
        """ë§¤ì¹­ ì´ìœ  ìƒì„±"""
        num_matches = len(matching_skills)
        
        if sbert > 0.5 and jaccard > 0.3:
            return f"ì˜ë¯¸ + ìŠ¤í‚¬ ë§¤ì¹­ ê°•í•¨ (SBERT: {sbert:.3f}, Jacc: {jaccard:.3f}), ìŠ¤í‚¬ {num_matches}ê°œ"
        elif sbert > 0.5:
            return f"Description ì˜ë¯¸ ë§¤ì¹­ ê°•í•¨ (SBERT: {sbert:.3f}), ìŠ¤í‚¬ {num_matches}ê°œ"
        elif num_matches >= 5:
            return f"ë§¤ì¹­ ìŠ¤í‚¬ {num_matches}ê°œ (Jacc: {jaccard:.3f})"
        elif num_matches >= 3:
            return f"ë§¤ì¹­ ìŠ¤í‚¬ {num_matches}ê°œ: {', '.join(matching_skills[:3])} (Jacc: {jaccard:.3f})"
        elif num_matches > 0:
            return f"ë§¤ì¹­ ìŠ¤í‚¬: {', '.join(matching_skills)} (Jacc: {jaccard:.3f})"
        else:
            return f"ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ (SBERT: {sbert:.3f})"


# ============================================================================
# Main System
# ============================================================================

class JobMatchingSystem:
    """í†µí•© ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ v13 (PPR í•„í„°ë§ ì „ìš©)"""

    def __init__(self, log_file: Optional[str] = None):
        self.graph = JobPostingGraph()
        self.sbert_matcher: Optional[SbertDescriptionMatcher] = None
        self.job_descriptions: List[JobDescription] = []
        self.matcher: Optional[JobMatcher] = None

        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        if log_file:
            self.logger = OutputLogger(log_file)
            sys.stdout = self.logger

    def __del__(self):
        # ì¢…ë£Œ ì‹œ ë¡œê·¸ íŒŒì¼ ë‹«ê¸°
        if hasattr(self, 'logger'):
            sys.stdout = self.logger.terminal
            self.logger.close()

    def load_job_descriptions(self, filepath: Optional[str] = None):
        """
        ì§ë¬´ ì •ì˜ ë¡œë“œ (JSON íŒŒì¼)
        
        Args:
            filepath: ì§ë¬´ ì •ì˜ JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        """
        if filepath is None:
            filepath = str(JOB_DESCRIPTION_FILE)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            print(f"[ERROR] Job description file not found: {filepath}")
            raise
        
        for item in data:
            job_desc = JobDescription(
                job_name=item.get('ì§ë¬´', ''),
                job_definition=item.get('ì§ë¬´ ì •ì˜', ''),
                industry=item.get('industry', ''),
                common_skills=item.get('ê³µí†µ_skill_set', []),
                specific_skills=item.get('skill_set', []),
                skill_set_description=item.get('skill_set_description', ''),
                common_skill_set_description=item.get('ê³µí†µ_skill_set_description', ''),  # v13: ì¶”ê°€
            )
            self.job_descriptions.append(job_desc)
        
        print(f"[OK] Job descriptions loaded: {len(self.job_descriptions)}")

    def load_training_data(
        self, 
        db: Optional[Session] = None,
        company_groups: Optional[List[str]] = None,
        job_files: Optional[List[str]] = None
    ):
        """
        ê¸°ì¡´ ì±„ìš©ê³µê³  ë¡œë“œ (DB ë˜ëŠ” JSON íŒŒì¼)
        
        Args:
            db: Database session (DBì—ì„œ ë¡œë“œí•  ê²½ìš° í•„ìˆ˜)
            company_groups: íšŒì‚¬ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸ 
                           ì˜ˆ: ["í† ìŠ¤", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "ì¿ íŒ¡", "ë¼ì¸"]
                           Noneì´ë©´ ì „ì²´ 9ê°œ ê·¸ë£¹
            job_files: JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (filepathê°€ Noneì¼ ë•Œ ì‚¬ìš©, í•˜ìœ„ í˜¸í™˜ì„±)
            
        Note:
            DB ì„¸ì…˜ì´ ì œê³µë˜ë©´ DBì—ì„œ ë¡œë“œ, ì—†ìœ¼ë©´ JSON íŒŒì¼ì—ì„œ ë¡œë“œ (í•˜ìœ„ í˜¸í™˜)
        """
        # DBì—ì„œ ë¡œë“œ
        if db is not None:
            from app.db.crud.job_matching_post import get_posts_by_competitor_groups
            from app.models.post import Post
            from app.models.post_skill import PostSkill
            
            # ê¸°ë³¸ 9ê°œ íšŒì‚¬ ê·¸ë£¹
            if company_groups is None:
                company_groups = [
                    "í† ìŠ¤", "ì¹´ì¹´ì˜¤", "í•œí™”ì‹œìŠ¤í…œ", "í˜„ëŒ€ì˜¤í† ì—ë²„", 
                    "ìš°ì•„í•œí˜•ì œë“¤", "LG CNS", "ë„¤ì´ë²„", "ì¿ íŒ¡", "ë¼ì¸"
                ]
            
            # DBì—ì„œ Post ì¡°íšŒ
            posts = get_posts_by_competitor_groups(db, company_groups)
            
            if not posts:
                print(f"[WARNING] {len(company_groups)}ê°œ ê·¸ë£¹ì—ì„œ Postë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # Post â†’ JobPosting ë³€í™˜
            loaded_count = 0
            for post in posts:
                # PostSkillì—ì„œ ìŠ¤í‚¬ ì´ë¦„ ì¶”ì¶œ
                skills = []
                if post.post_skills:
                    skills = [ps.skill.name for ps in post.post_skills if ps.skill]
                
                if not skills:
                    continue  # ìŠ¤í‚¬ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
                company_name = post.company.name if post.company else "Unknown"
                
                posting = JobPosting(
                    posting_id=str(post.id),
                    company=company_name,
                    title=post.title,
                    url=post.source_url or "",
                    skills=skills,
                )
                
                self.graph.add_posting(posting)
                loaded_count += 1
            
            print(f"[OK] {loaded_count} posts loaded from DB (companies: {', '.join(company_groups)})")
            return
        
        # JSON íŒŒì¼ì—ì„œ ë¡œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)
        if job_files is None:
            job_files = TRAINING_DATA_FILES
        
        for filepath in job_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                for idx, job in enumerate(data):
                    skills = []
                    skill_info = job.get('skill_set_info', {})
                    if isinstance(skill_info, dict):
                        skill_set = skill_info.get('skill_set', [])
                        if isinstance(skill_set, list):
                            skills = skill_set

                    posting = JobPosting(
                        posting_id=f"{Path(filepath).stem}_{idx}",
                        company=job.get('company', 'Unknown'),
                        title=job.get('title', ''),
                        url=job.get('url', ''),
                        skills=skills,
                    )

                    self.graph.add_posting(posting)

                print(f"[OK] {Path(filepath).name}: {len(data)} loaded")
            except Exception as e:
                print(f"[FAIL] {filepath} load failed: {e}")

    def build_graph(self):
        """ê·¸ë˜í”„ êµ¬ì¶•"""
        print("\n[ê·¸ë˜í”„ êµ¬ì¶•]")
        print(f"  ë…¸ë“œ: {self.graph.G.number_of_nodes()}ê°œ")
        print(f"  ì—£ì§€: {self.graph.G.number_of_edges()}ê°œ")
        print(f"  [NOTE] v13: ìŠ¤í‚¬ ë™ì‹œ ì¶œí˜„ ì—£ì§€ ìƒì„± ìƒëµ")
        print(f"  [NOTE] v13: PPRì€ specific_skillsë§Œ, JaccardëŠ” ì „ì²´ ìŠ¤í‚¬ ì‚¬ìš©")

    def build_matchers(self):
        """SBERT ì¸ë±ì‹±"""
        print("\n[SBERT Description ì¸ë±ì‹±]")
        self.sbert_matcher = SbertDescriptionMatcher(self.job_descriptions)
        
        self.matcher = JobMatcher(
            self.graph,
            self.sbert_matcher,
            self.job_descriptions,
        )
        
        print("[NOTE] v13: PPRì€ í•„í„°ë§ ì „ìš© (ì ìˆ˜ ë²„ë¦¼)")
        print("[NOTE] v13: Jaccard + SBERT ë‹¨ìˆœ í•©ì‚° (0~2)")
        print("[NOTE] v13: í•„í„°ë§ ì œê±° (ëª¨ë“  í›„ë³´ ê³„ì‚°)")

    def match_new_job(
        self,
        new_posting: NewJobPosting,
        ppr_top_n: int = 20,
        final_top_k: int = 2,
    ) -> List[JobMatchResult]:
        """ìƒˆ ì±„ìš©ê³µê³  ë§¤ì¹­"""
        if not self.matcher:
            raise ValueError("build_matchers()ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        print("\n" + "="*60)
        print("ì§ë¬´ ë§¤ì¹­ ì‹œì‘")
        print("="*60)
        
        results = self.matcher.match_job(
            new_posting,
            ppr_top_n=ppr_top_n,
            final_top_k=final_top_k,
        )
        return results
    
    def _convert_to_db_format(self, match_result: JobMatchResult) -> Dict[str, Any]:
        """
        JobMatchResultë¥¼ DB ì €ì¥ìš© JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Returns:
            {
                "position": "Software Development",
                "industry": "Front-end Development",
                "sim_score": 0.6541,
                "sim_skill_matching": ["react", "git", "css"]
            }
        """
        return {
            "position": match_result.job_name,
            "industry": match_result.industry,
            "sim_score": round(match_result.final_score, 4),
            "sim_skill_matching": match_result.matching_skills,
        }

    def match_company_jobs(
        self,
        company_json_file: str,
        ppr_top_n: int = 20,
        final_top_k: int = 2,
    ) -> List[Dict]:
        """
        íšŒì‚¬ ì „ì²´ ì±„ìš©ê³µê³  ë§¤ì¹­
        
        Returns:
            List[Dict] - ê° í•­ëª©ì€ ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§:
                - 'posting': NewJobPosting ê°ì²´
                - 'matches': List[JobMatchResult] (ì „ì²´ ë§¤ì¹­ ê²°ê³¼)
                - 'db_result': Dict (DB ì €ì¥ìš©, 1ë“±ë§Œ í¬í•¨) ë˜ëŠ” None
        """
        with open(company_json_file, 'r', encoding='utf-8') as f:
            jobs_data = json.load(f)
        
        print(f"\n{'='*80}")
        print(f"[INFO] Company job matching: {len(jobs_data)} postings")
        print(f"íŒŒì¼: {Path(company_json_file).name}")
        print(f"{'='*80}")
        
        all_results = []
        
        for idx, job in enumerate(jobs_data):
            # ìŠ¤í‚¬ ì¶”ì¶œ
            skills = []
            skill_info = job.get('skill_set_info', {})
            if isinstance(skill_info, dict):
                skill_set = skill_info.get('skill_set', [])
                if isinstance(skill_set, list):
                    skills = skill_set
            
            if not skills:
                continue
            
            # ---------- ğŸ”§ ë³€ê²½ í¬ì¸íŠ¸ 2: descriptionì— ë³¸ë¬¸ ì „ì²´ ìš°ì„  ì‚¬ìš© ----------
            # í¬ë¡¤ëŸ¬ì—ì„œ ë³¸ë¬¸ ì „ì²´ë¥¼ job["description"]ì´ë‚˜ ìœ ì‚¬ í‚¤ë¡œ ë„£ì–´ì¤€ë‹¤ ê°€ì •
            raw_body = (
                job.get('description')
                or job.get('content')
                or job.get('ë³¸ë¬¸')
                or ""
            )
            
            new_posting = NewJobPosting(
                posting_id=f"new_{idx}",
                company=job.get('company', 'Unknown'),
                title=job.get('title', ''),
                skills=skills,
                url=job.get('url', ''),
                # ì—¬ê¸°ì—ëŠ” "ë³¸ë¬¸ ì „ì²´"ë¥¼ ë„£ì–´ë‘ê³ , SBERT ì¿¼ë¦¬ì—ì„œëŠ” titleê³¼ í•©ì³ì„œ ì‚¬ìš©
                description=raw_body,
            )
            
            try:
                matches = self.match_new_job(
                    new_posting,
                    ppr_top_n=ppr_top_n,
                    final_top_k=final_top_k,
                )
                
                print(f"\n{'>'*40}")
                print(f"[{idx+1}/{len(jobs_data)}] {new_posting.company} - {new_posting.title}")
                print(f"ìŠ¤í‚¬: {', '.join(new_posting.skills[:5])}{'...' if len(new_posting.skills) > 5 else ''}")
                print()
                
                for i, result in enumerate(matches, 1):
                    print(f"  {i}ìœ„. {result.job_name} / {result.industry}")
                    print(f"       ì ìˆ˜: {result.final_score:.4f} | ë§¤ì¹­: {', '.join(result.matching_skills[:3])}")
                
                # DB ì €ì¥ìš© ë°ì´í„° (1ë“±ë§Œ)
                db_result = None
                if matches and len(matches) > 0:
                    db_result = self._convert_to_db_format(matches[0])
                
                all_results.append({
                    'posting': new_posting,
                    'matches': matches,
                    'db_result': db_result,
                })
                
            except Exception as e:
                print(f"  [ERROR] Matching failed: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ DB ê²°ê³¼ëŠ” Noneìœ¼ë¡œ ì €ì¥
                all_results.append({
                    'posting': new_posting,
                    'matches': [],
                    'db_result': None,
                })
        
        print(f"\n{'='*80}")
        print(f"[DONE] Complete: {len(all_results)} job postings matched")
        print(f"{'='*80}")
        
        self._print_summary(all_results)
        
        return all_results
    
    def _print_summary(self, results: List[Dict]):
        """ë§¤ì¹­ ê²°ê³¼ ìš”ì•½"""
        job_counter = Counter()
        industry_counter = Counter()
        
        for item in results:
            if item['matches']:
                top_match = item['matches'][0]
                job_counter[top_match.job_name] += 1
                industry_counter[top_match.industry] += 1
        
        print(f"\n{'='*80}")
        print("[SUMMARY] Matching results")
        print(f"{'='*80}")
        
        print("\n[ì§ë¬´ë³„ ë¶„í¬ (Top 10)]")
        for job_name, count in job_counter.most_common(10):
            print(f"  {job_name}: {count}ê°œ ({count/len(results)*100:.1f}%)")
        
        print("\n[ì‚°ì—…ë³„ ë¶„í¬]")
        for industry, count in industry_counter.most_common():
            print(f"  {industry}: {count}ê°œ ({count/len(results)*100:.1f}%)")
        
        print(f"\n{'='*80}")


# ============================================================================
# Main Execution (ì£¼ì„ ì²˜ë¦¬ - FastAPIì—ì„œ ì‚¬ìš©í•  ì˜ˆì •)
# ============================================================================

# def main():
#     """ë©”ì¸ ì‹¤í–‰"""
# 
#     # ë¡œê·¸ íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     log_file = f"job_matching_v7_results_{timestamp}.txt"
# 
#     print("="*80)
#     print("ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ v7 - SBERT DESCRIPTION MATCHING")
#     print(f"ë¡œê·¸ íŒŒì¼: {log_file}")
#     print("="*80)
# 
#     # app/core/job_matchingì—ì„œ AI_Lab/dataë¡œ ì ‘ê·¼
#     base_dir = Path(__file__).parent.parent.parent.parent
#     data_dir = base_dir / "AI_Lab" / "data"
# 
#     # ë¡œê·¸ íŒŒì¼ í™œì„±í™”
#     system = JobMatchingSystem(log_file=log_file)
# 
#     print("\n[1/4] ì§ë¬´ ì •ì˜ ë¡œë“œ")
#     system.load_job_descriptions(str(data_dir / 'new_job_description.json'))
# 
#     print("\n[2/4] í•™ìŠµ ë°ì´í„° ë¡œë“œ")
#     training_files = [
#         str(data_dir / 'hanwha_jobs.json'),
#         str(data_dir / 'kakao_jobs.json'),
#         str(data_dir / 'line_jobs.json'),
#         str(data_dir / 'naver_jobs.json'),
#     ]
#     system.load_training_data(training_files)
# 
#     print("\n[3/4] ê·¸ë˜í”„ êµ¬ì¶•")
#     system.build_graph()
# 
#     print("\n[4/4] Matchers ì´ˆê¸°í™”")
#     system.build_matchers()
# 
#     print("\n" + "="*80)
#     print("[OK] System ready!")
#     print("="*80)
# 
#     # line_jobs.json ì•ˆì— 'description'(ë³¸ë¬¸ ì „ì²´) í•„ë“œê¹Œì§€ ë“¤ì–´ê°€ ìˆìœ¼ë©´
#     # SBERTê°€ ì œëª©+ë³¸ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­ ìˆ˜í–‰
#     results = system.match_company_jobs(
#         str(data_dir / 'line_jobs.json'),
#         ppr_top_n=20,
#         final_top_k=2,
#     )
# 
#     # DB ì €ì¥ìš© JSON íŒŒì¼ ìƒì„± (1ë“± ê²°ê³¼ë§Œ)
#     json_output_file = f"job_matching_v7_db_results_{timestamp}.json"
#     db_results = []
#     
#     for result in results:
#         if result.get('db_result'):
#             # ì›ë³¸ ì±„ìš©ê³µê³  ì •ë³´ì™€ DB ê²°ê³¼ë¥¼ í•¨ê»˜ ì €ì¥
#             db_entry = {
#                 'company': result['posting'].company,
#                 'title': result['posting'].title,
#                 'url': result['posting'].url,
#                 **result['db_result']  # sim_position, sim_industry, sim_score, sim_skill_matching
#             }
#             db_results.append(db_entry)
#     
#     # JSON íŒŒì¼ë¡œ ì €ì¥
#     with open(json_output_file, 'w', encoding='utf-8') as f:
#         json.dump(db_results, f, ensure_ascii=False, indent=2)
#     
#     print(f"\n{'='*80}")
#     print(f"ë¡œê·¸ íŒŒì¼: {log_file}")
#     print(f"DB ê²°ê³¼ JSON: {json_output_file} ({len(db_results)}ê°œ ê²°ê³¼)")
#     print(f"{'='*80}")
# 
# 
# if __name__ == '__main__':
#     main()