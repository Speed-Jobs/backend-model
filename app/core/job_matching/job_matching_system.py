"""
ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ v7 - SBERT DESCRIPTION MATCHING

ì£¼ìš” ê°œì„ ì‚¬í•­:
- Sentence-BERT ì„ë² ë”©ìœ¼ë¡œ description vs description ì˜ë¯¸ ìœ ì‚¬ë„ ë§¤ì¹­ ì¶”ê°€
- (ë³€ê²½) ì œëª© + ë³¸ë¬¸ ì „ì²´ë¥¼ í•©ì³ì„œ SBERT ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
- Description ìœ ì‚¬ë„ë¥¼ ìµœì¢… ì ìˆ˜ì— 45% ë°˜ì˜ (ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
- ê¸°ì¡´ BM25 ëŒ€ì‹  SBERT cosine similarity ì‚¬ìš©

ì ìˆ˜ êµ¬ì„±:
- 15% PPR (êµ¬ì¡°ì  ìœ ì‚¬ë„)
- 25% Jaccard (ìŠ¤í‚¬ ì§ì ‘ ë§¤ì¹­)
- 15% Cluster (ì»¤ë®¤ë‹ˆí‹° ìœ ì‚¬ë„)
- 45% SBERT (Description ìœ ì‚¬ë„)
"""

import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime

import numpy as np
import networkx as nx
from community import community_louvain
from sentence_transformers import SentenceTransformer

from app.core.job_matching.config import (
    JOB_DESCRIPTION_FILE,
    SBERT_MODEL_NAME,
    TRAINING_DATA_FILES,
)

# ============================================================================
# Output Logger (Terminal + File)
# ============================================================================

class OutputLogger:
    """Terminalê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥"""

    def __init__(self, log_file: str):
        self.terminal = sys.stdout
        self.log = open(log_file, 'w', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

    def close(self):
        self.log.close()

# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class JobDescription:
    """ì§ë¬´ ì •ì˜ (new_job_description.json)"""
    job_name: str
    job_definition: str
    industry: str
    common_skills: List[str]
    specific_skills: List[str]
    all_skills: List[str] = field(default_factory=list)
    skill_set_description: str = ""  # ì£¼ìš” ì—…ë¬´ ì„¤ëª… (SBERTì— ì‚¬ìš©)

    def __post_init__(self):
        self.all_skills = list(set(self.common_skills + self.specific_skills))

@dataclass
class NewJobPosting:
    """ìƒˆë¡œìš´ ì±„ìš©ê³µê³  (ë§¤ì¹­ ëŒ€ìƒ)"""
    posting_id: str
    company: str
    title: str
    skills: List[str]
    url: str = ""
    description: str = ""

@dataclass
class JobPosting:
    """ê¸°ì¡´ ì±„ìš©ê³µê³  (í•™ìŠµ ë°ì´í„°)"""
    posting_id: str
    company: str
    title: str
    url: str
    skills: List[str]

    def __hash__(self):
        return hash(self.posting_id)


@dataclass
class JobMatchResult:
    """ì§ë¬´ ë§¤ì¹­ ê²°ê³¼"""
    job_name: str
    industry: str
    final_score: float
    
    jaccard_score: float = 0.0
    cluster_score: float = 0.0
    pagerank_score: float = 0.0
    sbert_score: float = 0.0  # SBERT ìœ ì‚¬ë„ ì ìˆ˜
    
    matching_skills: List[str] = field(default_factory=list)
    missing_skills: List[str] = field(default_factory=list)
    job_definition: str = ""
    
    reason: str = ""

# ============================================================================
# Graph Infrastructure
# ============================================================================

class JobPostingGraph:
    """ì±„ìš©ê³µê³  ê·¸ë˜í”„"""

    def __init__(self):
        self.G = nx.Graph()
        self.postings: Dict[str, JobPosting] = {}

    def add_posting(self, posting: JobPosting):
        posting_node = f"posting:{posting.posting_id}"
        self.postings[posting_node] = posting
        self.G.add_node(posting_node, type='posting')

        if posting.company:
            company_node = f"company:{posting.company}"
            self.G.add_edge(posting_node, company_node, weight=1.0)

        for skill in posting.skills:
            skill_normalized = self._normalize_skill(skill)
            if skill_normalized:
                skill_node = f"skill:{skill_normalized}"
                self.G.add_edge(posting_node, skill_node, weight=1.0)

    def build_skill_cooccurrence(self, min_cooccur: int = 2):
        skill_pairs = Counter()

        for posting in self.postings.values():
            skills = [f"skill:{self._normalize_skill(s)}" for s in posting.skills]
            for i, skill1 in enumerate(skills):
                for skill2 in skills[i+1:]:
                    pair = tuple(sorted([skill1, skill2]))
                    skill_pairs[pair] += 1

        for (skill1, skill2), count in skill_pairs.items():
            if count >= min_cooccur:
                self.G.add_edge(skill1, skill2, weight=count)

    @staticmethod
    def _normalize_skill(skill: str) -> str:
        return (
            skill.lower()
            .replace('-', '')
            .replace('_', '')
            .replace(' ', '')
            .replace('.', '')
            .strip()
        )


# ============================================================================
# SBERT Description Matcher
# ============================================================================

class SbertDescriptionMatcher:
    """
    Sentence-BERTë¡œ description ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
    - ì§ë¬´ ì •ì˜ í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©í•´ë‘ê³ 
    - ìƒˆ ê³µê³ ì˜ (ì œëª© + ë³¸ë¬¸ ì „ì²´) ì„ë² ë”©ê³¼ cosine similarity ê³„ì‚°
    """

    def __init__(
        self,
        job_descriptions: List[JobDescription],
        model_name: str = None,
    ):
        self.job_descriptions = job_descriptions

        # configì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        if model_name is None:
            model_name = SBERT_MODEL_NAME

        print(f"[SBERT] ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self.model = SentenceTransformer(model_name)

        print(f"[SBERT] ì§ë¬´ definition ì„ë² ë”© ìƒì„± ì¤‘... (ì§ë¬´ ì •ì˜ + industry + skill_set_description)")
        corpus = []
        
        for jd in job_descriptions:
            parts = []
            
            # 1. ì§ë¬´ ì •ì˜ (ê¸°ì¡´)
            if jd.job_definition:
                parts.append(jd.job_definition)
            
            # 2. Industry ì¶”ê°€ (Front-end vs Back-end êµ¬ë¶„ì— í•„ìˆ˜!)
            if jd.industry:
                parts.append(f"ì‚°ì—… ë¶„ì•¼: {jd.industry}")
            
            # 3. Skill Set Description ì¶”ê°€ (êµ¬ì²´ì ì¸ ì—…ë¬´ ì„¤ëª…)
            if jd.skill_set_description:
                parts.append(f"ì£¼ìš” ì—…ë¬´: {jd.skill_set_description}")
            
            # ëª¨ë“  ì •ë³´ ê²°í•© (ì •ë³´ê°€ ì—†ìœ¼ë©´ job_nameë§Œ ì‚¬ìš©)
            combined_text = "\n\n".join(parts) if parts else jd.job_name
            corpus.append(combined_text)

        # normalize_embeddings=True â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì 
        self.job_embeddings = self.model.encode(
            corpus,
            convert_to_numpy=True,
            normalize_embeddings=True,
        )

        print(f"[OK] {len(job_descriptions)}ê°œ ì§ë¬´ ì •ì˜ ì„ë² ë”© ì™„ë£Œ (industry + skill_set_description í¬í•¨)")

    def calculate_similarity(self, query_text: str) -> Dict[str, float]:
        """
        ìƒˆ ê³µê³ ì˜ ì¿¼ë¦¬ í…ìŠ¤íŠ¸(ì œëª©+ë³¸ë¬¸)ì™€ ëª¨ë“  ì§ë¬´ ì •ì˜ì˜ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° (0~1)

        Returns:
            Dict[job_name, normalized_score]
        """
        if not query_text or not query_text.strip():
            return {jd.job_name: 0.0 for jd in self.job_descriptions}

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_emb = self.model.encode(
            [query_text],
            convert_to_numpy=True,
            normalize_embeddings=True,
        )[0]

        # cosine similarity (normalized embeddings â†’ dot product)
        sims = np.dot(self.job_embeddings, query_emb)  # [-1, 1]

        # 1ì°¨ ë³€í™˜: [-1, 1] â†’ [0, 1]
        sims = (sims + 1.0) / 2.0

        # 2ì°¨ ì •ê·œí™”: ìµœëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ 0~1
        max_sim = sims.max() if sims.size > 0 else 1.0
        if max_sim > 0:
            sims = sims / max_sim

        result = {}
        for i, jd in enumerate(self.job_descriptions):
            result[jd.job_name] = float(sims[i])

        return result


# ============================================================================
# Cluster Matcher (Louvain ê¸°ë°˜)
# ============================================================================

class ClusterMatcher:
    """Louvain í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë§¤ì¹­"""

    def __init__(self, graph: nx.Graph):
        self.graph = graph
        self.clusters = {}
        self.cluster_skills = defaultdict(list)
        
        print(f"[Louvain] ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì¤‘...")
        
        # Louvain í´ëŸ¬ìŠ¤í„°ë§
        self.clusters = community_louvain.best_partition(graph, weight='weight')
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ìŠ¤í‚¬ ë…¸ë“œ ì •ë¦¬
        for node, cluster_id in self.clusters.items():
            if node.startswith('skill:'):
                self.cluster_skills[cluster_id].append(node)
        
        num_clusters = len(set(self.clusters.values()))
        num_skill_nodes = len([n for n in graph.nodes() if n.startswith('skill:')])
        
        print(f"[OK] {num_clusters}ê°œ í´ëŸ¬ìŠ¤í„° íƒì§€ (ìŠ¤í‚¬ ë…¸ë“œ: {num_skill_nodes}ê°œ)")

    def get_cluster_distribution(self, skills: List[str], normalize_func) -> np.ndarray:
        """
        ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸ì˜ í´ëŸ¬ìŠ¤í„° ë¶„í¬ ê³„ì‚°
        """
        skill_nodes = [f"skill:{normalize_func(s)}" for s in skills]
        
        cluster_counts = Counter()
        for skill_node in skill_nodes:
            if skill_node in self.clusters:
                cluster_id = self.clusters[skill_node]
                cluster_counts[cluster_id] += 1
        
        num_clusters = max(self.clusters.values()) + 1
        distribution = np.zeros(num_clusters)
        
        for cluster_id, count in cluster_counts.items():
            distribution[cluster_id] = count
        
        total = distribution.sum()
        if total > 0:
            distribution = distribution / total
        
        return distribution
    
    def calculate_similarity(self, skills1: List[str], skills2: List[str], normalize_func) -> float:
        """
        ë‘ ìŠ¤í‚¬ ì§‘í•© ê°„ í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        """
        dist1 = self.get_cluster_distribution(skills1, normalize_func)
        dist2 = self.get_cluster_distribution(skills2, normalize_func)
        
        norm1 = np.linalg.norm(dist1)
        norm2 = np.linalg.norm(dist2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = np.dot(dist1, dist2) / (norm1 * norm2)
        return similarity


# ============================================================================
# Job Matcher (í•µì‹¬ ë¡œì§)
# ============================================================================

class JobMatcher:
    """ìƒˆ ì±„ìš©ê³µê³  â†’ ì§ë¬´ ë§¤ì¹­"""

    def __init__(
        self,
        graph: JobPostingGraph,
        cluster_matcher: ClusterMatcher,
        sbert_matcher: SbertDescriptionMatcher,
        job_descriptions: List[JobDescription],
    ):
        self.graph = graph
        self.cluster_matcher = cluster_matcher
        self.sbert_matcher = sbert_matcher
        self.job_descriptions = job_descriptions

    def match_job(
        self,
        new_posting: NewJobPosting,
        ppr_top_n: int = 20,
        final_top_k: int = 2,
    ) -> List[JobMatchResult]:
        """
        ìƒˆ ì±„ìš©ê³µê³ ë¥¼ ì§ë¬´ì™€ ë§¤ì¹­
        
        3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸:
        1. PPRë¡œ ìƒìœ„ 20ê°œ ì§ë¬´ ì¶”ì¶œ (êµ¬ì¡°ì  ìœ ì‚¬ë„)
        2. SBERTë¡œ (ì œëª©+ë³¸ë¬¸) ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        3. ìŠ¤í‚¬ ë§¤ì¹­ í•„í„°ë§ + ìµœì¢… Top 1~2 ì„ ì •
        """
        print(f"\n[Stage 1] PPR ê¸°ë°˜ 1ì°¨ í•„í„°ë§ (ìƒìœ„ {ppr_top_n}ê°œ ì¶”ì¶œ)")
        
        # Stage 1: PPRë¡œ ì§ë¬´ë³„ ì ìˆ˜ ê³„ì‚° ë° ìƒìœ„ Nê°œ ì¶”ì¶œ
        ppr_candidates = self._get_ppr_top_jobs(new_posting, ppr_top_n)
        
        if not ppr_candidates:
            print("  ! PPR í›„ë³´ ì—†ìŒ, ì „ì²´ ì§ë¬´ ëŒ€ìƒìœ¼ë¡œ ì§„í–‰")
            ppr_candidates = [(jd, 0.0) for jd in self.job_descriptions]
        
        print(f"  [OK] {len(ppr_candidates)} jobs selected")
        
        # ---------- ğŸ”§ ë³€ê²½ í¬ì¸íŠ¸ 1: SBERT ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„± (title + description) ----------
        query_text = f"{new_posting.title}\n\n{new_posting.description}".strip()
        
        # Stage 1.5: SBERT description ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ì§ë¬´ ëŒ€ìƒ)
        print(f"\n[Stage 1.5] SBERT Description ìœ ì‚¬ë„ ê³„ì‚°")
        sbert_scores = self.sbert_matcher.calculate_similarity(query_text)
        
        if query_text:
            top_sbert_job = max(sbert_scores.items(), key=lambda x: x[1])
            print(f"  - SBERT 1ë“±: {top_sbert_job[0]} (ì ìˆ˜: {top_sbert_job[1]:.4f})")
        else:
            print(f"  ! Description/Title ì—†ìŒ - SBERT ì ìˆ˜ ëª¨ë‘ 0")
        
        # Stage 2: ì„ ì •ëœ í›„ë³´ë“¤ì— ëŒ€í•´ì„œë§Œ ìŠ¤í‚¬ ë§¤ì¹­
        print(f"\n[Stage 2] ìŠ¤í‚¬ ë§¤ì¹­ (Jaccard + Cluster) + í•„í„°ë§")
        
        results = []
        filtered_count = 0
        
        for job_desc, ppr_score in ppr_candidates:
            # Jaccard + Cluster ê³„ì‚°
            jaccard = self._calculate_jaccard(new_posting.skills, job_desc.all_skills)
            cluster = self._calculate_cluster_similarity(new_posting.skills, job_desc.all_skills)
            
            # SBERT ì ìˆ˜
            sbert = sbert_scores.get(job_desc.job_name, 0.0)
            
            # ë§¤ì¹­ ìŠ¤í‚¬ ë¶„ì„
            new_skills_norm = set(self.graph._normalize_skill(s) for s in new_posting.skills)
            job_skills_norm = set(self.graph._normalize_skill(s) for s in job_desc.all_skills)
            
            matching_skills = list(new_skills_norm & job_skills_norm)
            missing_skills = list(job_skills_norm - new_skills_norm)
            
            # í•„í„°ë§: ìŠ¤í‚¬ 0ê°œ ë§¤ì¹­ì´ë©´ ì œì™¸ (ì™„ì „ ì˜ë¯¸ ë§¤ì¹­ë§Œìœ¼ë¡œ ì¶”ì²œë˜ëŠ” ê²ƒ ë°©ì§€)
            if len(matching_skills) == 0 and jaccard < 0.05 and cluster < 0.2:
                filtered_count += 1
                continue
            
            # ìµœì¢… ì ìˆ˜
            final_score = (
                0.15 * ppr_score   # êµ¬ì¡°ì  ìœ ì‚¬ë„
                + 0.25 * jaccard   # ìŠ¤í‚¬ ì§ì ‘ ë§¤ì¹­
                + 0.15 * cluster   # í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„
                + 0.45 * sbert     # (ì œëª©+ë³¸ë¬¸) ì˜ë¯¸ ìœ ì‚¬ë„
            )
            
            result = JobMatchResult(
                job_name=job_desc.job_name,
                industry=job_desc.industry,
                final_score=final_score,
                jaccard_score=jaccard,
                cluster_score=cluster,
                pagerank_score=ppr_score,
                sbert_score=sbert,
                matching_skills=matching_skills[:10],
                missing_skills=missing_skills[:5],
                job_definition=job_desc.job_definition,
                reason=self._generate_reason(matching_skills, jaccard, ppr_score, sbert),
            )
            
            results.append(result)
        
        # ì •ë ¬ ë° Top-K ë°˜í™˜
        results.sort(key=lambda x: x.final_score, reverse=True)
        
        if filtered_count > 0:
            print(f"  [FILTER] {filtered_count}ê°œ ì§ë¬´ ì œì™¸ (ìŠ¤í‚¬ ë§¤ì¹­ ë¶€ì¡±)")
        
        print(f"  [OK] Final top {min(final_top_k, len(results))} returned")
        if results:
            print(
                "  - 1ë“±: "
                f"{results[0].job_name}/{results[0].industry}\n"
                "         ì ìˆ˜: "
                f"{results[0].final_score:.4f} "
                f"(PPR:{results[0].pagerank_score:.4f}, "
                f"Jacc:{results[0].jaccard_score:.4f}, "
                f"Clust:{results[0].cluster_score:.4f}, "
                f"SBERT:{results[0].sbert_score:.4f})"
            )
        else:
            print(f"  [WARNING] ë§¤ì¹­ ê°€ëŠ¥í•œ ì§ë¬´ ì—†ìŒ")
        
        return results[:final_top_k]
    
    def _get_ppr_top_jobs(self, new_posting: NewJobPosting, top_n: int) -> List[Tuple[JobDescription, float]]:
        """
        PPRë¡œ ìƒìœ„ Nê°œ ì§ë¬´ ì¶”ì¶œ
        """
        try:
            personalization = {}
            new_skill_nodes = [
                f"skill:{self.graph._normalize_skill(s)}"
                for s in new_posting.skills
            ]
            
            for node in self.graph.G.nodes():
                if node in new_skill_nodes:
                    personalization[node] = 1.0 / len(new_skill_nodes)
                else:
                    personalization[node] = 0.0
            
            ppr = nx.pagerank(
                self.graph.G,
                personalization=personalization,
                alpha=0.85,
                max_iter=100,
                weight='weight',
            )
            
            job_ppr_scores = []
            
            for job_desc in self.job_descriptions:
                skill_nodes = [
                    f"skill:{self.graph._normalize_skill(s)}"
                    for s in job_desc.all_skills
                ]
                
                ppr_scores = [ppr.get(node, 0.0) for node in skill_nodes]
                avg_ppr = np.mean(ppr_scores) if ppr_scores else 0.0
                
                job_ppr_scores.append((job_desc, avg_ppr))
            
            if job_ppr_scores:
                max_ppr = max(score for _, score in job_ppr_scores)
                if max_ppr > 0:
                    job_ppr_scores = [(jd, score / max_ppr) for jd, score in job_ppr_scores]
            
            job_ppr_scores.sort(key=lambda x: x[1], reverse=True)
            top_candidates = job_ppr_scores[:top_n]
            
            if top_candidates:
                print(
                    f"  - PPR 1ë“±: {top_candidates[0][0].job_name}/"
                    f"{top_candidates[0][0].industry} (ì ìˆ˜: {top_candidates[0][1]:.4f})"
                )
                print(
                    f"  - PPR {len(top_candidates)}ë“±: {top_candidates[-1][0].job_name}/"
                    f"{top_candidates[-1][0].industry} (ì ìˆ˜: {top_candidates[-1][1]:.4f})"
                )
            
            return top_candidates
        
        except Exception as e:
            print(f"  ! PPR ê³„ì‚° ì‹¤íŒ¨: {e}")
            return []

    def _calculate_jaccard(self, skills1: List[str], skills2: List[str]) -> float:
        set1 = set(self.graph._normalize_skill(s) for s in skills1)
        set2 = set(self.graph._normalize_skill(s) for s in skills2)
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0

    def _calculate_cluster_similarity(self, skills1: List[str], skills2: List[str]) -> float:
        """í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        return self.cluster_matcher.calculate_similarity(
            skills1, skills2, self.graph._normalize_skill
        )

    def _generate_reason(self, matching_skills: List[str], jaccard: float, ppr: float, sbert: float) -> str:
        num_matches = len(matching_skills)
        
        if sbert > 0.5:
            return f"Description ì˜ë¯¸ ë§¤ì¹­ ê°•í•¨ (SBERT: {sbert:.3f}), ìŠ¤í‚¬ {num_matches}ê°œ"
        elif num_matches >= 5:
            return f"ë§¤ì¹­ ìŠ¤í‚¬ {num_matches}ê°œ (PPR: {ppr:.3f}, Jacc: {jaccard:.2%})"
        elif num_matches >= 3:
            return f"ë§¤ì¹­ ìŠ¤í‚¬ {num_matches}ê°œ: {', '.join(matching_skills[:3])} (PPR: {ppr:.3f})"
        elif num_matches > 0:
            return f"ë§¤ì¹­ ìŠ¤í‚¬: {', '.join(matching_skills)} (PPR: {ppr:.3f})"
        else:
            return f"êµ¬ì¡°ì  ìœ ì‚¬ë„ + ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ (PPR: {ppr:.3f}, SBERT: {sbert:.3f})"


# ============================================================================
# Main System
# ============================================================================

class JobMatchingSystem:
    """í†µí•© ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ"""

    def __init__(self, log_file: Optional[str] = None):
        self.graph = JobPostingGraph()
        self.cluster_matcher: Optional[ClusterMatcher] = None
        self.sbert_matcher: Optional[SbertDescriptionMatcher] = None
        self.job_descriptions: List[JobDescription] = []
        self.matcher: Optional[JobMatcher] = None

        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        if log_file:
            self.logger = OutputLogger(log_file)
            sys.stdout = self.logger

    def __del__(self):
        # ì¢…ë£Œ ì‹œ ë¡œê·¸ íŒŒì¼ ë‹«ê¸°
        if hasattr(self, 'logger'):
            sys.stdout = self.logger.terminal
            self.logger.close()

    def load_job_descriptions(self, filepath: str = None):
        """
        ì§ë¬´ ì •ì˜ ë¡œë“œ
        
        Args:
            filepath: ì§ë¬´ ì •ì˜ JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        """
        if filepath is None:
            filepath = str(JOB_DESCRIPTION_FILE)
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            job_desc = JobDescription(
                job_name=item.get('ì§ë¬´', ''),
                job_definition=item.get('ì§ë¬´ ì •ì˜', ''),
                industry=item.get('industry', ''),
                common_skills=item.get('ê³µí†µ_skill_set', []),
                specific_skills=item.get('skill_set', []),
                skill_set_description=item.get('skill_set_description', ''),
            )
            self.job_descriptions.append(job_desc)
        
        print(f"[OK] Job descriptions loaded: {len(self.job_descriptions)}")

    def load_training_data(self, job_files: List[str] = None):
        """
        ê¸°ì¡´ ì±„ìš©ê³µê³  ë¡œë“œ
        
        Args:
            job_files: í•™ìŠµ ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
            
        Note:
            TODO: ì¶”í›„ DBì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì • í•„ìš”
            ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ í›„ DB ì¿¼ë¦¬ë¡œ ëŒ€ì²´ ì˜ˆì •
        """
        if job_files is None:
            job_files = TRAINING_DATA_FILES
        
        for filepath in job_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                for idx, job in enumerate(data):
                    skills = []
                    skill_info = job.get('skill_set_info', {})
                    if isinstance(skill_info, dict):
                        skill_set = skill_info.get('skill_set', [])
                        if isinstance(skill_set, list):
                            skills = skill_set

                    posting = JobPosting(
                        posting_id=f"{Path(filepath).stem}_{idx}",
                        company=job.get('company', 'Unknown'),
                        title=job.get('title', ''),
                        url=job.get('url', ''),
                        skills=skills,
                    )

                    self.graph.add_posting(posting)

                print(f"[OK] {Path(filepath).name}: {len(data)} loaded")
            except Exception as e:
                print(f"[FAIL] {filepath} load failed: {e}")

    def build_graph(self):
        """ê·¸ë˜í”„ êµ¬ì¶•"""
        print("\n[ê·¸ë˜í”„ êµ¬ì¶•]")
        print(f"  ë…¸ë“œ: {self.graph.G.number_of_nodes()}ê°œ")
        print(f"  ì—£ì§€: {self.graph.G.number_of_edges()}ê°œ")

        print("\n[ìŠ¤í‚¬ ë™ì‹œ ì¶œí˜„ ì—£ì§€ ì¶”ê°€]")
        self.graph.build_skill_cooccurrence(min_cooccur=2)
        print(f"  ì—£ì§€: {self.graph.G.number_of_edges()}ê°œ (ì—…ë°ì´íŠ¸)")

    def build_matchers(self):
        """í´ëŸ¬ìŠ¤í„°ë§ + SBERT ì¸ë±ì‹±"""
        print("\n[Louvain í´ëŸ¬ìŠ¤í„°ë§]")
        self.cluster_matcher = ClusterMatcher(self.graph.G)
        
        print("\n[SBERT Description ì¸ë±ì‹±]")
        self.sbert_matcher = SbertDescriptionMatcher(self.job_descriptions)
        
        self.matcher = JobMatcher(
            self.graph,
            self.cluster_matcher,
            self.sbert_matcher,
            self.job_descriptions,
        )

    def match_new_job(
        self,
        new_posting: NewJobPosting,
        ppr_top_n: int = 20,
        final_top_k: int = 2,
    ) -> List[JobMatchResult]:
        """ìƒˆ ì±„ìš©ê³µê³  ë§¤ì¹­"""
        if not self.matcher:
            raise ValueError("build_matchers()ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        print("\n" + "="*60)
        print("ì§ë¬´ ë§¤ì¹­ ì‹œì‘")
        print("="*60)
        
        results = self.matcher.match_job(
            new_posting,
            ppr_top_n=ppr_top_n,
            final_top_k=final_top_k,
        )
        return results
    
    def _convert_to_db_format(self, match_result: JobMatchResult) -> Dict[str, Any]:
        """
        JobMatchResultë¥¼ DB ì €ì¥ìš© JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Returns:
            {
                "position": "Software Development",
                "industry": "Front-end Development",
                "sim_score": 0.6541,
                "sim_skill_matching": ["react", "git", "css"]
            }
        """
        return {
            "position": match_result.job_name,
            "industry": match_result.industry,
            "sim_score": round(match_result.final_score, 4),
            "sim_skill_matching": match_result.matching_skills,
        }

    def match_company_jobs(
        self,
        company_json_file: str,
        ppr_top_n: int = 20,
        final_top_k: int = 2,
    ) -> List[Dict]:
        """
        íšŒì‚¬ ì „ì²´ ì±„ìš©ê³µê³  ë§¤ì¹­
        
        Returns:
            List[Dict] - ê° í•­ëª©ì€ ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§:
                - 'posting': NewJobPosting ê°ì²´
                - 'matches': List[JobMatchResult] (ì „ì²´ ë§¤ì¹­ ê²°ê³¼)
                - 'db_result': Dict (DB ì €ì¥ìš©, 1ë“±ë§Œ í¬í•¨) ë˜ëŠ” None
        """
        with open(company_json_file, 'r', encoding='utf-8') as f:
            jobs_data = json.load(f)
        
        print(f"\n{'='*80}")
        print(f"[INFO] Company job matching: {len(jobs_data)} postings")
        print(f"íŒŒì¼: {Path(company_json_file).name}")
        print(f"{'='*80}")
        
        all_results = []
        
        for idx, job in enumerate(jobs_data):
            # ìŠ¤í‚¬ ì¶”ì¶œ
            skills = []
            skill_info = job.get('skill_set_info', {})
            if isinstance(skill_info, dict):
                skill_set = skill_info.get('skill_set', [])
                if isinstance(skill_set, list):
                    skills = skill_set
            
            if not skills:
                continue
            
            # ---------- ğŸ”§ ë³€ê²½ í¬ì¸íŠ¸ 2: descriptionì— ë³¸ë¬¸ ì „ì²´ ìš°ì„  ì‚¬ìš© ----------
            # í¬ë¡¤ëŸ¬ì—ì„œ ë³¸ë¬¸ ì „ì²´ë¥¼ job["description"]ì´ë‚˜ ìœ ì‚¬ í‚¤ë¡œ ë„£ì–´ì¤€ë‹¤ ê°€ì •
            raw_body = (
                job.get('description')
                or job.get('content')
                or job.get('ë³¸ë¬¸')
                or ""
            )
            
            new_posting = NewJobPosting(
                posting_id=f"new_{idx}",
                company=job.get('company', 'Unknown'),
                title=job.get('title', ''),
                skills=skills,
                url=job.get('url', ''),
                # ì—¬ê¸°ì—ëŠ” "ë³¸ë¬¸ ì „ì²´"ë¥¼ ë„£ì–´ë‘ê³ , SBERT ì¿¼ë¦¬ì—ì„œëŠ” titleê³¼ í•©ì³ì„œ ì‚¬ìš©
                description=raw_body,
            )
            
            try:
                matches = self.match_new_job(
                    new_posting,
                    ppr_top_n=ppr_top_n,
                    final_top_k=final_top_k,
                )
                
                print(f"\n{'>'*40}")
                print(f"[{idx+1}/{len(jobs_data)}] {new_posting.company} - {new_posting.title}")
                print(f"ìŠ¤í‚¬: {', '.join(new_posting.skills[:5])}{'...' if len(new_posting.skills) > 5 else ''}")
                print()
                
                for i, result in enumerate(matches, 1):
                    print(f"  {i}ìœ„. {result.job_name} / {result.industry}")
                    print(f"       ì ìˆ˜: {result.final_score:.4f} | ë§¤ì¹­: {', '.join(result.matching_skills[:3])}")
                
                # DB ì €ì¥ìš© ë°ì´í„° (1ë“±ë§Œ)
                db_result = None
                if matches and len(matches) > 0:
                    db_result = self._convert_to_db_format(matches[0])
                
                all_results.append({
                    'posting': new_posting,
                    'matches': matches,
                    'db_result': db_result,
                })
                
            except Exception as e:
                print(f"  [ERROR] Matching failed: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ DB ê²°ê³¼ëŠ” Noneìœ¼ë¡œ ì €ì¥
                all_results.append({
                    'posting': new_posting,
                    'matches': [],
                    'db_result': None,
                })
        
        print(f"\n{'='*80}")
        print(f"[DONE] Complete: {len(all_results)} job postings matched")
        print(f"{'='*80}")
        
        self._print_summary(all_results)
        
        return all_results
    
    def _print_summary(self, results: List[Dict]):
        """ë§¤ì¹­ ê²°ê³¼ ìš”ì•½"""
        job_counter = Counter()
        industry_counter = Counter()
        
        for item in results:
            if item['matches']:
                top_match = item['matches'][0]
                job_counter[top_match.job_name] += 1
                industry_counter[top_match.industry] += 1
        
        print(f"\n{'='*80}")
        print("[SUMMARY] Matching results")
        print(f"{'='*80}")
        
        print("\n[ì§ë¬´ë³„ ë¶„í¬ (Top 10)]")
        for job_name, count in job_counter.most_common(10):
            print(f"  {job_name}: {count}ê°œ ({count/len(results)*100:.1f}%)")
        
        print("\n[ì‚°ì—…ë³„ ë¶„í¬]")
        for industry, count in industry_counter.most_common():
            print(f"  {industry}: {count}ê°œ ({count/len(results)*100:.1f}%)")
        
        print(f"\n{'='*80}")


# ============================================================================
# Main Execution (ì£¼ì„ ì²˜ë¦¬ - FastAPIì—ì„œ ì‚¬ìš©í•  ì˜ˆì •)
# ============================================================================

# def main():
#     """ë©”ì¸ ì‹¤í–‰"""
# 
#     # ë¡œê·¸ íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     log_file = f"job_matching_v7_results_{timestamp}.txt"
# 
#     print("="*80)
#     print("ì§ë¬´ ë§¤ì¹­ ì‹œìŠ¤í…œ v7 - SBERT DESCRIPTION MATCHING")
#     print(f"ë¡œê·¸ íŒŒì¼: {log_file}")
#     print("="*80)
# 
#     # app/core/job_matchingì—ì„œ AI_Lab/dataë¡œ ì ‘ê·¼
#     base_dir = Path(__file__).parent.parent.parent.parent
#     data_dir = base_dir / "AI_Lab" / "data"
# 
#     # ë¡œê·¸ íŒŒì¼ í™œì„±í™”
#     system = JobMatchingSystem(log_file=log_file)
# 
#     print("\n[1/4] ì§ë¬´ ì •ì˜ ë¡œë“œ")
#     system.load_job_descriptions(str(data_dir / 'new_job_description.json'))
# 
#     print("\n[2/4] í•™ìŠµ ë°ì´í„° ë¡œë“œ")
#     training_files = [
#         str(data_dir / 'hanwha_jobs.json'),
#         str(data_dir / 'kakao_jobs.json'),
#         str(data_dir / 'line_jobs.json'),
#         str(data_dir / 'naver_jobs.json'),
#     ]
#     system.load_training_data(training_files)
# 
#     print("\n[3/4] ê·¸ë˜í”„ êµ¬ì¶•")
#     system.build_graph()
# 
#     print("\n[4/4] Matchers ì´ˆê¸°í™”")
#     system.build_matchers()
# 
#     print("\n" + "="*80)
#     print("[OK] System ready!")
#     print("="*80)
# 
#     # line_jobs.json ì•ˆì— 'description'(ë³¸ë¬¸ ì „ì²´) í•„ë“œê¹Œì§€ ë“¤ì–´ê°€ ìˆìœ¼ë©´
#     # SBERTê°€ ì œëª©+ë³¸ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­ ìˆ˜í–‰
#     results = system.match_company_jobs(
#         str(data_dir / 'line_jobs.json'),
#         ppr_top_n=20,
#         final_top_k=2,
#     )
# 
#     # DB ì €ì¥ìš© JSON íŒŒì¼ ìƒì„± (1ë“± ê²°ê³¼ë§Œ)
#     json_output_file = f"job_matching_v7_db_results_{timestamp}.json"
#     db_results = []
#     
#     for result in results:
#         if result.get('db_result'):
#             # ì›ë³¸ ì±„ìš©ê³µê³  ì •ë³´ì™€ DB ê²°ê³¼ë¥¼ í•¨ê»˜ ì €ì¥
#             db_entry = {
#                 'company': result['posting'].company,
#                 'title': result['posting'].title,
#                 'url': result['posting'].url,
#                 **result['db_result']  # sim_position, sim_industry, sim_score, sim_skill_matching
#             }
#             db_results.append(db_entry)
#     
#     # JSON íŒŒì¼ë¡œ ì €ì¥
#     with open(json_output_file, 'w', encoding='utf-8') as f:
#         json.dump(db_results, f, ensure_ascii=False, indent=2)
#     
#     print(f"\n{'='*80}")
#     print(f"ë¡œê·¸ íŒŒì¼: {log_file}")
#     print(f"DB ê²°ê³¼ JSON: {json_output_file} ({len(db_results)}ê°œ ê²°ê³¼)")
#     print(f"{'='*80}")
# 
# 
# if __name__ == '__main__':
#     main()