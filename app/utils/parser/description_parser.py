import fitz  # PyMuPDF
import os
import json
import csv
import re
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


'''
ì§ë¬´ ê¸°ìˆ ì„œì˜ ë‚´ìš©ì„ parsingí•˜ëŠ” .py íŒŒì¼
'''

# Load environment variables from .env file
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

pdf_path = r"data\SKAX_Jobdescription.pdf"

# ChatOpenAI ì‚¬ìš©
llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=openai_api_key)

prompt_template = """
[ì›ë¬¸]
{context}

[ìŠ¤í‚¤ë§ˆ]
ì§ë¬´: string
ì§ë¬´ ì •ì˜: string
industry: string 
ê³µí†µ_skill_set_description: string
skill_set_description: string
ê³µí†µ_skill_set: string[]
skill_set: string[]

[ìš”êµ¬ì‚¬í•­]
1) ì›ë¬¸ì—ì„œ ëª¨ë“  ì§ë¬´ì˜ industryë³„ ë¸”ë¡ì„ ì‹ë³„í•˜ì—¬ ê°ê° 1ê°œì˜ JSON ê°ì²´ë¡œ ì¶œë ¥
2) ê° ì§ë¬´ëŠ” ì—¬ëŸ¬ ê°œì˜ industryë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ì›ë¬¸ì— ëª…ì‹œëœ ë§Œí¼ ëª¨ë‘ ì¶”ì¶œ
3) ê³µí†µ_skill_set_descriptionì€ í•´ë‹¹ ì§ë¬´ì˜ 'ê³µí†µ' ì„¹ì…˜ í•­ëª©ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œ stringìœ¼ë¡œ ì €ì¥
   - 'ê³µí†µ', 'ê³µí†µ ì—­ëŸ‰', 'ê³µí†µ ìš”êµ¬ì‚¬í•­', 'ê³µí†µ ìŠ¤í‚¬' ë“±ì˜ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ì„ ì°¾ìœ¼ì„¸ìš”
   - ì§ë¬´ ì •ì˜ ì§í›„ì— ë‚˜ì˜¤ëŠ” ê³µí†µì ì¸ ë‚´ìš©ë„ í¬í•¨í•˜ì„¸ìš”
   - ëª¨ë“  industryì— ê³µí†µìœ¼ë¡œ ì ìš©ë˜ëŠ” ë‚´ìš©ì„ ì°¾ìœ¼ì„¸ìš”
   - ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì €ì¥
4) skill_set_descriptionì€ í•´ë‹¹ industry ì„¹ì…˜ì˜ ê¸°ìˆ ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œ stringìœ¼ë¡œ ì €ì¥ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
5) ê³µí†µ_skill_setì€ 'ê³µí†µ' ì„¹ì…˜ì˜ í•­ëª©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘ í›„ ITìš©ì–´ ì¤‘ë³µ/ë³€í˜•ì–´ ì •ê·œí™”
   - ê³µí†µ_skill_set_descriptionì—ì„œ ê¸°ìˆ , ë„êµ¬, ì—­ëŸ‰ ë“±ì„ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“œì„¸ìš”
   - ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ë¡œ ì €ì¥
6) skill_setì€ í•´ë‹¹ industry ì„¹ì…˜ì˜ ê¸°ìˆ ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘ í›„ ITìš©ì–´ ì¤‘ë³µ/ë³€í˜•ì–´ ì •ê·œí™” (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
7) ë°˜ë“œì‹œ ì›ë¬¸ì— ìˆëŠ” ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”. ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê°’ìœ¼ë¡œë¼ë„ í•„ë“œë¥¼ ì±„ì›Œì£¼ì„¸ìš”.
8) íŠ¹ë³„íˆ ì£¼ì˜: Domain Expert ê°™ì€ ì§ë¬´ëŠ” ê³µí†µ ì„¹ì…˜ì´ ëª…ì‹œì ìœ¼ë¡œ ì—†ì„ ìˆ˜ ìˆì§€ë§Œ, ì§ë¬´ ì •ì˜ë‚˜ ì „ì²´ êµ¬ì¡°ì—ì„œ ê³µí†µì ìœ¼ë¡œ ìš”êµ¬ë˜ëŠ” ì—­ëŸ‰ì„ ì°¾ì•„ì„œ ê³µí†µ_skill_setì— í¬í•¨ì‹œí‚¤ì„¸ìš”.

[ì§ë¬´ë³„ Industry êµ¬ì„± ì°¸ê³ ]
- Software Development: Front-end Development, Back-end Development, Mobile Development
- Factory AX Engineering: Simulation, ê¸°êµ¬ì„¤ê³„, ì „ì¥/ì œì–´
- Solution Development: ERP_FCM, ERP_SCM, ERP_HCM, ERP_T&E, Biz. Solution
- Cloud/Infra Engineering: System/Network Engineering, Middleware/Database Engineering, Data Center Engineering
- Architect: Software Architect, Data Architect, Infra Architect, AI Architect, Automation Architect
- Project Management: Application PM, Infra PM, Solution PM, AI PM, Automation PM
- Quality Management: PMO, Quality Engineering, Offshoring Service Professional
- AI: AI/Data Development, Generative AI Development, Physical AI Development
- ì •ë³´ë³´í˜¸: ë³´ì•ˆ Governance/Compliance, ë³´ì•ˆ ì§„ë‹¨/Consulting, ë³´ì•ˆ Solution Service
- Sales: ì œ1ê¸ˆìœµ, ì œ2ê¸ˆìœµ, ì œì¡° ëŒ€ì™¸, ì œì¡° ëŒ€ë‚´Hi-Tech, ì œì¡° ëŒ€ë‚´Process, í†µì‹ , ìœ í†µ/ë¬¼ë¥˜/ì„œë¹„ìŠ¤, ë¯¸ë””ì–´/ì½˜í…ì¸ , ê³µê³µ, Global
- Domain Expert: (Salesì™€ ë™ì¼í•œ êµ¬ì¡°)
- Consulting: ESG, SHE, ERP, SCM, CRM, AI
- Biz. Supporting: Strategy Planning, New Biz. Development, Financial Management, Human Resource Management, Stakeholder Management, Governance & Public Management


[ê²°ê³¼ ì˜ˆì‹œ]
{{
  "ì§ë¬´": "Software Development",
  "ì§ë¬´ ì •ì˜": "ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì™€ Industryê´€ë ¨ ì§€ì‹ê³¼ ê²½í—˜ì„ í™œìš©í•˜ì—¬, ê³ ê° Needsì— ë§ëŠ” ì†Œí”„íŠ¸ì›¨ì–´/ì‹œìŠ¤í…œ/ê¸°ëŠ¥ êµ¬í˜„",
  "industry": "Front-end Development",
  "ê³µí†µ_skill_set_description": "â€¢ [í”„ë¡œê·¸ë˜ë°ì–¸ì–´] Java, node.js, Python, C / C++, Go, ASP.NET, Perl, Ruby, C#, PHP, Visual Basic ë“±\\nâ€¢ [ë²„ì „ê´€ë¦¬ë„êµ¬] Git, Github, SVN, Bitbucket ë“±\\nâ€¢ [í˜‘ì—…Tool] Jira, Confluence, Slack, Teams, Notion, Google Docs ë“±\\nâ€¢ [AI í™œìš©] AI Literacy / Collaboration ì—­ëŸ‰\\nâ€¢ [IndustryKnowledge] 1/2ê¸ˆìœµ, ëŒ€ì™¸ì œì¡°, ëŒ€ë‚´Hi-Tech, ëŒ€ë‚´Process, í†µì‹ , ìœ í†µ/ë¬¼ë¥˜/ì„œë¹„ìŠ¤, ë¯¸ë””ì–´/ì½˜í…ì¸ , ê³µê³µ, Global ë“±",
  "skill_set_description": "â€¢ [UI/UX_ë””ìì¸ë„êµ¬] Sketch, Adobe XD, Figma ë“±\\nâ€¢ [ì›¹í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬] React, Angular, Vue.js, Node.js, Next.js, Nust.js, jQuery ë“±\\nâ€¢ [ì›¹í¼ë¸”ë¦¬ì‹±] HTML, CSS, CSS í”„ë ˆì„ì›Œí¬(Bootstrap, MaterialUI ë“±), CSS ì „ì²˜ë¦¬ê¸°(sass, scss, less ë“±)",
  "ê³µí†µ_skill_set": [
    "Java",
    "Node.js",
    "Python",
    "C/C++",
    "Go",
    "ASP.NET",
    "Git",
    "GitHub",
    "SVN",
    "Jira",
    "Confluence",
    "Slack",
    "Microsoft Teams",
    "Notion",
    "Google Docs",
    "AI Literacy"
  ],
  "skill_set": [
    "Sketch",
    "Adobe XD",
    "Figma",
    "React",
    "Angular",
    "Vue.js",
    "Next.js",
    "Nuxt.js",
    "jQuery",
    "HTML",
    "CSS",
    "Bootstrap",
    "Material-UI",
    "Sass",
    "SCSS",
    "Less"
  ]
}}

ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ê° industryë³„ë¡œ JSON ê°ì²´ë¥¼ ìƒì„±í•˜ë˜, ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.
"""

# LCEL ë°©ì‹: prompt | llm | output_parser
prompt = ChatPromptTemplate.from_template(prompt_template)
output_parser = StrOutputParser()
chain = prompt | llm | output_parser


def extract_job_sections(doc):
    """
    PDFì—ì„œ ì§ë¬´ë³„ ì„¹ì…˜ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    ì§ë¬´ ì œëª©ì„ ê¸°ì¤€ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    """
    # ì§ë¬´ ì œëª© íŒ¨í„´ (ì˜ˆ: "Software Development", "Data Science" ë“±)
    # ì¼ë°˜ì ìœ¼ë¡œ í° í°íŠ¸ë¡œ ì‘ì„±ë˜ê±°ë‚˜ íŠ¹ì • íŒ¨í„´ì„ ê°€ì§
    job_patterns = [
        r'^[A-Z][a-zA-Z\s&/\-]+$',  # ì˜ë¬¸ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì§ë¬´ëª…
        r'^\d+\.\s*[A-Z][a-zA-Z\s&/\-]+',  # ë²ˆí˜¸ê°€ ë¶™ì€ ì§ë¬´ëª…
        r'^â– \s*[A-Zê°€-í£][a-zA-Zê°€-í£\s&/\-]+',  # â–  ê¸°í˜¸ë¡œ ì‹œì‘
    ]
    
    job_sections = []
    current_job = None
    current_pages = []
    current_text = []
    
    total_pages = len(doc)
    
    for page_num in range(total_pages):
        page = doc[page_num]
        page_text = page.get_text()
        lines = page_text.split('\n')
        
        # í˜ì´ì§€ì˜ ì²« ëª‡ ì¤„ì—ì„œ ì§ë¬´ ì œëª© ì°¾ê¸°
        found_new_job = False
        for i, line in enumerate(lines[:10]):  # ìƒìœ„ 10ì¤„ë§Œ ì²´í¬
            line_stripped = line.strip()
            
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì§ë¬´ ì œëª© ê°ì§€
            for pattern in job_patterns:
                if re.match(pattern, line_stripped) and len(line_stripped) > 5:
                    # ìƒˆë¡œìš´ ì§ë¬´ ë°œê²¬
                    if current_job is not None:
                        # ì´ì „ ì§ë¬´ ì €ì¥
                        job_sections.append({
                            'job_title': current_job,
                            'pages': current_pages.copy(),
                            'text': '\n\n'.join(current_text)
                        })
                    
                    # ìƒˆ ì§ë¬´ ì‹œì‘
                    current_job = line_stripped
                    current_pages = [page_num]
                    current_text = [page_text]
                    found_new_job = True
                    print(f"ğŸ“Œ ìƒˆ ì§ë¬´ ë°œê²¬: '{current_job}' (í˜ì´ì§€ {page_num + 1})")
                    break
            
            if found_new_job:
                break
        
        # ê¸°ì¡´ ì§ë¬´ì— í˜ì´ì§€ ì¶”ê°€
        if not found_new_job and current_job is not None:
            current_pages.append(page_num)
            current_text.append(page_text)
    
    # ë§ˆì§€ë§‰ ì§ë¬´ ì €ì¥
    if current_job is not None:
        job_sections.append({
            'job_title': current_job,
            'pages': current_pages,
            'text': '\n\n'.join(current_text)
        })
    
    return job_sections


def parse_llm_response(response):
    """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    response_cleaned = response.strip()
    
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    if response_cleaned.startswith("```json"):
        response_cleaned = response_cleaned[7:]
    if response_cleaned.startswith("```"):
        response_cleaned = response_cleaned[3:]
    if response_cleaned.endswith("```"):
        response_cleaned = response_cleaned[:-3]
    response_cleaned = response_cleaned.strip()
    
    parsed_items = []
    
    # ë°©ë²• 1: JSON ë°°ì—´ë¡œ íŒŒì‹±
    try:
        items = json.loads(response_cleaned)
        if isinstance(items, list):
            parsed_items = items
            print(f"  âœ… JSON ë°°ì—´ íŒŒì‹± ì„±ê³µ: {len(parsed_items)}ê°œ í•­ëª©")
        elif isinstance(items, dict):
            parsed_items = [items]
            print(f"  âœ… JSON ê°ì²´ íŒŒì‹± ì„±ê³µ (ë‹¨ì¼ í•­ëª©)")
        return parsed_items
    except json.JSONDecodeError:
        pass
    
    # ë°©ë²• 2: JSONL í˜•ì‹
    print(f"  âš ï¸ JSON ë°°ì—´ íŒŒì‹± ì‹¤íŒ¨, JSONL í˜•ì‹ìœ¼ë¡œ ì‹œë„...")
    lines = [line.strip() for line in response_cleaned.splitlines() if line.strip()]
    
    for idx, line in enumerate(lines, 1):
        try:
            item = json.loads(line)
            if isinstance(item, dict):
                parsed_items.append(item)
                print(f"    âœ… ì¤„ {idx}: íŒŒì‹± ì„±ê³µ")
            elif isinstance(item, list):
                parsed_items.extend(item)
                print(f"    âœ… ì¤„ {idx}: ë°°ì—´ íŒŒì‹± ì„±ê³µ - {len(item)}ê°œ í•­ëª©")
        except json.JSONDecodeError:
            continue
    
    # ë°©ë²• 3: ë¶€ë¶„ ë¬¸ìì—´ ì¶”ì¶œ
    if not parsed_items:
        print(f"  âš ï¸ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹œë„...")
        try:
            start_idx = response_cleaned.find('[')
            end_idx = response_cleaned.rfind(']')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response_cleaned[start_idx:end_idx+1]
                items = json.loads(json_str)
                if isinstance(items, list):
                    parsed_items = items
                    print(f"    âœ… JSON ë°°ì—´ ì¶”ì¶œ ì„±ê³µ: {len(parsed_items)}ê°œ í•­ëª©")
        except Exception:
            pass
    
    return parsed_items


STRING_COMMON_FIELDS = ["ì§ë¬´ ì •ì˜", "ê³µí†µ_skill_set_description"]
LIST_COMMON_FIELDS = ["ê³µí†µ_skill_set"]


def normalize_skill_list(value):
    """ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜"""
    items = set()
    if isinstance(value, list):
        for item in value:
            if isinstance(item, str):
                normalized = item.strip()
                if normalized:
                    items.add(normalized)
    elif isinstance(value, str):
        candidates = re.split(r'[,\\n]+', value)
        for candidate in candidates:
            normalized = candidate.strip()
            if normalized:
                items.add(normalized)
    return sorted(items)


def select_preferred_string(values):
    """ì—¬ëŸ¬ ë¬¸ìì—´ ì¤‘ ê°€ì¥ ì •ë³´ê°€ ë§ì€ ê°’ì„ ì„ íƒ"""
    cleaned = [
        v.strip()
        for v in values
        if isinstance(v, str) and v.strip()
    ]
    if not cleaned:
        return ""
    cleaned.sort(key=len, reverse=True)
    return cleaned[0]


def ensure_job_common_fields(all_results, job_title):
    """ê°™ì€ ì§ë¬´ì— ëŒ€í•´ ê³µí†µ í•„ë“œê°€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ë„ë¡ ì¡°ì •"""
    job_items = [
        item for item in all_results
        if item.get('ì§ë¬´') == job_title
    ]
    if not job_items:
        return

    # ê³µí†µ ë¬¸ìì—´ í•„ë“œ ì„ íƒ
    canonical_strings = {}
    for field in STRING_COMMON_FIELDS:
        field_values = [item.get(field, "") for item in job_items]
        canonical_strings[field] = select_preferred_string(field_values)

    # ê³µí†µ ë¦¬ìŠ¤íŠ¸ í•„ë“œ ë³‘í•©
    canonical_lists = {}
    for field in LIST_COMMON_FIELDS:
        merged_items = set()
        for item in job_items:
            merged_items.update(normalize_skill_list(item.get(field, [])))
        canonical_lists[field] = sorted(merged_items)

    # ëª¨ë“  í•­ëª©ì— canonical ê°’ ì ìš©
    for item in job_items:
        for field, value in canonical_strings.items():
            item[field] = value
        for field, value in canonical_lists.items():
            item[field] = value[:]


def merge_duplicate_items(all_results, new_item):
    """ì¤‘ë³µ í•­ëª©ì„ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜"""
    ì§ë¬´ = new_item.get('ì§ë¬´', '')
    industry = new_item.get('industry', '')
    
    if not ì§ë¬´ or not industry:
        print(f"  âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: ì§ë¬´={ì§ë¬´}, industry={industry}")
        return False
    
    # ì¤‘ë³µ ì²´í¬
    for existing in all_results:
        if (existing.get('ì§ë¬´') == ì§ë¬´ and 
            existing.get('industry') == industry):
            # ì¤‘ë³µ ë°œê²¬ - ë³‘í•©
            if new_item.get('ê³µí†µ_skill_set_description') and not existing.get('ê³µí†µ_skill_set_description'):
                existing['ê³µí†µ_skill_set_description'] = new_item.get('ê³µí†µ_skill_set_description', '')
            if new_item.get('skill_set_description') and not existing.get('skill_set_description'):
                existing['skill_set_description'] = new_item.get('skill_set_description', '')
            
            # ë¦¬ìŠ¤íŠ¸ ë³‘í•©
            existing_common = set(existing.get('ê³µí†µ_skill_set', []))
            new_common = set(new_item.get('ê³µí†µ_skill_set', []))
            if new_common:
                existing['ê³µí†µ_skill_set'] = list(existing_common | new_common)
            
            existing_skill = set(existing.get('skill_set', []))
            new_skill = set(new_item.get('skill_set', []))
            if new_skill:
                existing['skill_set'] = list(existing_skill | new_skill)
            
            print(f"  ğŸ”„ ì¤‘ë³µ í•­ëª© ì—…ë°ì´íŠ¸: ì§ë¬´={ì§ë¬´}, industry={industry}")
            ensure_job_common_fields(all_results, ì§ë¬´)
            return True
    
    # ì¤‘ë³µ ì•„ë‹˜ - ìƒˆë¡œ ì¶”ê°€
    all_results.append(new_item)
    print(f"  âœ… í•­ëª© ì¶”ê°€: ì§ë¬´={ì§ë¬´}, industry={industry}")
    ensure_job_common_fields(all_results, ì§ë¬´)
    return True


# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
output_dir = os.path.dirname(pdf_path)
csv_file_path = os.path.join(output_dir, "output.csv")
json_file_path = os.path.join(output_dir, "output.json")

all_results = []

print(f"[í™•ì¸] ì¶œë ¥ íŒŒì¼ ê²½ë¡œ:")
print(f"  CSV: {csv_file_path}")
print(f"  JSON: {json_file_path}\n")

with fitz.open(pdf_path) as doc:
    total_pages = len(doc)
    print(f"ì´ í˜ì´ì§€ ìˆ˜: {total_pages}\n")
    print(f"{'='*80}")
    print("1ë‹¨ê³„: ì§ë¬´ë³„ ì„¹ì…˜ ì¶”ì¶œ")
    print(f"{'='*80}\n")
    
    # ì§ë¬´ë³„ ì„¹ì…˜ ì¶”ì¶œ
    job_sections = extract_job_sections(doc)
    
    if not job_sections:
        print("âš ï¸ ì§ë¬´ ì„¹ì…˜ì„ ìë™ìœ¼ë¡œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        full_text = ""
        for page_num in range(total_pages):
            page = doc[page_num]
            full_text += page.get_text() + "\n\n"
        
        job_sections = [{
            'job_title': 'Unknown Job',
            'pages': list(range(total_pages)),
            'text': full_text
        }]
    
    print(f"\nì´ {len(job_sections)}ê°œì˜ ì§ë¬´ ì„¹ì…˜ ë°œê²¬\n")
    print(f"{'='*80}")
    print("2ë‹¨ê³„: ê° ì§ë¬´ë³„ LLM ì²˜ë¦¬")
    print(f"{'='*80}\n")
    
    # ê° ì§ë¬´ë³„ë¡œ LLM ì²˜ë¦¬
    for idx, job_section in enumerate(job_sections, 1):
        job_title = job_section['job_title']
        pages = job_section['pages']
        text = job_section['text']
        
        if not text.strip():
            print(f"[{idx}/{len(job_sections)}] ê±´ë„ˆëœ€: '{job_title}' - ë¹ˆ ë‚´ìš©")
            continue
        
        print(f"\n[{idx}/{len(job_sections)}] ì²˜ë¦¬ ì¤‘: '{job_title}'")
        print(f"  í˜ì´ì§€: {pages[0]+1}-{pages[-1]+1} (ì´ {len(pages)}í˜ì´ì§€)")
        print(f"  í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        try:
            # LLM í˜¸ì¶œ
            response = chain.invoke({"context": text})
            
            # ì‘ë‹µ íŒŒì‹±
            parsed_items = parse_llm_response(response)
            
            # ê²°ê³¼ ë³‘í•©
            if parsed_items:
                for item in parsed_items:
                    if isinstance(item, dict):
                        merge_duplicate_items(all_results, item)
            else:
                print(f"  âš ï¸ íŒŒì‹±ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        
        print("-" * 80)

# ê²°ê³¼ ì €ì¥
print(f"\n{'='*80}")
print(f"ì´ {len(all_results)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘ë¨")
print(f"{'='*80}\n")

if all_results:
    # JSON ì €ì¥
    with open(json_file_path, "w", encoding="utf-8") as jf:
        json.dump(all_results, jf, ensure_ascii=False, indent=2)
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_file_path}")
    print(f"   ë ˆì½”ë“œ ìˆ˜: {len(all_results)}")
    
    # CSV ì €ì¥
    keys = set()
    for item in all_results:
        keys.update(item.keys())
    keys = list(keys)
    
    with open(csv_file_path, "w", newline='', encoding="utf-8-sig") as cf:
        writer = csv.DictWriter(cf, fieldnames=keys)
        writer.writeheader()
        for item in all_results:
            item_row = item.copy()
            for k, v in item_row.items():
                if isinstance(v, list):
                    item_row[k] = ", ".join(v)
            writer.writerow(item_row)
    
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_file_path}")
    print(f"   í–‰ ìˆ˜: {len(all_results)}, ì—´ ìˆ˜: {len(keys)}")
else:
    print("âŒ ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤!")