"""
LG ì±„ìš©ê³µê³  ë¹„ë™ê¸° í†µí•© í¬ë¡¤ëŸ¬ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
1. APIë¡œ ê³µê³  ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
2. ë¹„ë™ê¸°ë¡œ ì—¬ëŸ¬ ìƒì„¸ í˜ì´ì§€ë¥¼ ë™ì‹œ í¬ë¡¤ë§
3. ë³‘ë ¬ LLM ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
"""

import json
import re
import os
import asyncio
from typing import List, Dict, Optional, Any
from pathlib import Path
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from playwright.async_api import async_playwright, Browser, Page

try:
    from openai import AsyncOpenAI
except Exception:
    AsyncOpenAI = None


class LGCareerAsyncCrawler:
    def __init__(self, max_concurrent: int = 5):
        """
        ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            max_concurrent: ë™ì‹œ í¬ë¡¤ë§ ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸: 5ê°œ)
        """
        load_dotenv()
        
        # API ì„¤ì •
        self.base_url = "https://api.careers.lg.com"
        self.list_endpoint = "/rmk/job/retrieveJobNoticesList"
        self.detail_url_template = "https://careers.lg.com/apply/detail?id={}"
        
        # ìµœì†Œí•œì˜ í—¤ë”ë§Œ ì‚¬ìš©
        self.headers = {
            'accept': 'application/json',
            'content-type': 'application/json',
            'referer': 'https://careers.lg.com/',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ë™ì‹œ ì‹¤í–‰ ì œì–´
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = self._get_openai_client()
    
    def _get_openai_client(self) -> Optional[Any]:
        """OpenAI ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or AsyncOpenAI is None:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„¸ ì •ë³´ ì¶”ì¶œì´ ì œí•œë©ë‹ˆë‹¤.")
            return None
        return AsyncOpenAI(api_key=api_key)
    
    # ==================== 1ë‹¨ê³„: APIë¡œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ====================
    
    def get_job_list_from_api(self) -> List[Dict]:
        """APIì—ì„œ ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ë™ê¸°)"""
        url = f"{self.base_url}{self.list_endpoint}"
        
        payload = {
            "lnbSearch": "",
            "hashTagText": "",
            "recDate": "CREATION_DATE",
            "order": "DESC",
            "careerList": [],
            "companyCodeList": [],
            "desireLocList": [],
            "jobGroupList": []
        }
        
        try:
            print("ğŸ“¡ [1/4] API í˜¸ì¶œ ì¤‘...")
            response = requests.post(url, headers=self.headers, json=payload, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                job_list = data.get('data', {}).get('jobNoticeList', [])
                
                jobs = []
                for job in job_list:
                    job_id = job.get('jobNoticeId')
                    
                    # APIì—ì„œ URL í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í…œí”Œë¦¿ìœ¼ë¡œ ìƒì„±
                    job_url = job.get('jobNoticeUrl') or job.get('url')
                    if not job_url:
                        job_url = self.detail_url_template.format(job_id)
                    
                    job_info = {
                        'id': job_id,
                        'title': job.get('jobNoticeName'),
                        'company': job.get('companyName'),
                        'career_type': job.get('careerTypeName'),
                        'job_group': job.get('jobGroupName'),
                        'status': job.get('noticeStatusName'),
                        'deadline': job.get('recEndDateTime'),
                        'url': job_url
                    }
                    jobs.append(job_info)
                
                print(f"âœ… APIì—ì„œ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
                return jobs
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
    
    # ==================== 2ë‹¨ê³„: ë¹„ë™ê¸° í¬ë¡¤ë§ ====================
    
    def _clean_html_text(self, html: str) -> str:
        """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ"""
        soup = BeautifulSoup(html, "html.parser")
        
        # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        
        text = soup.get_text("\n")
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = re.sub(r"[ \t\x0b\x0c\r]+", " ", text)
        
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return "\n".join(lines)
    
    async def _extract_detail_with_llm(self, raw_text: str, basic_info: Dict, retry_count: int = 3) -> Dict:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        if self.openai_client is None:
            return {}
        
        system_prompt = """
ë„ˆëŠ” ì±„ìš©ê³µê³ ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì—ì´ì „íŠ¸ì•¼.

**í•µì‹¬ ê·œì¹™: ëª¨ë“  ë‚´ìš©ì€ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ì•¼ í•´. ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ì¬ì‘ì„±í•˜ì§€ ë§ˆ!**

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´:

{
  "description": string,           // ì£¼ìš” ì—…ë¬´/ë‹´ë‹¹ì—…ë¬´/ì—…ë¬´ë‚´ìš© ì„¹ì…˜ì˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ
  "requirements": string,          // ìê²©ìš”ê±´/í•„ìˆ˜ìš”ê±´/ì§€ì›ìê²© ì„¹ì…˜ì˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ
  "preferred": string,             // ìš°ëŒ€ì‚¬í•­/ìš°ëŒ€ì¡°ê±´ ì„¹ì…˜ì˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ
  "benefits": string,              // ë³µë¦¬í›„ìƒ/ê·¼ë¬´ì¡°ê±´/í˜œíƒ ì„¹ì…˜ì˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ
  "process": string,               // ì „í˜•ì ˆì°¨/ì±„ìš©ì ˆì°¨/ì „í˜•ë‹¨ê³„ ì„¹ì…˜ì˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ
  "location": string,              // ê·¼ë¬´ì§€/ê·¼ë¬´ì§€ì—­ (ê°„ë‹¨í•œ ìœ„ì¹˜ ì •ë³´)
  "contact": string                // ë‹´ë‹¹ì/ë¬¸ì˜ì²˜/ì—°ë½ì²˜
}

**ì¶”ì¶œ ë°©ë²•**:
1. ê° í•­ëª©ì— í•´ë‹¹í•˜ëŠ” ì„¹ì…˜ì„ ì°¾ì•„ì„œ ë‚´ìš©ì„ **ìˆëŠ” ê·¸ëŒ€ë¡œ ë³µì‚¬**
2. ë¶ˆë¦¿ í¬ì¸íŠ¸(â€¢, -, 1. ë“±)ì™€ ì¤„ë°”ê¿ˆë„ **ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€**
3. ìš”ì•½í•˜ê±°ë‚˜ ì˜ì—­í•˜ì§€ ë§ê³  **ì „ì²´ ë‚´ìš©ì„ ë‹¤ í¬í•¨**
4. í•´ë‹¹ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ null ì²˜ë¦¬
5. ì—¬ëŸ¬ ë¬¸ë‹¨ì´ë©´ ëª¨ë‘ í¬í•¨

**ì˜ˆì‹œ**:
ì›ë¬¸: "â€¢ Python ê°œë°œ ê²½í—˜ 3ë…„ ì´ìƒ\nâ€¢ Django/FastAPI í”„ë ˆì„ì›Œí¬ ì‚¬ìš© ê²½í—˜"
ì¶”ì¶œ: "â€¢ Python ê°œë°œ ê²½í—˜ 3ë…„ ì´ìƒ\nâ€¢ Django/FastAPI í”„ë ˆì„ì›Œí¬ ì‚¬ìš© ê²½í—˜"  (ê·¸ëŒ€ë¡œ!)
        """.strip()
        
        user_prompt = f"""
ì±„ìš©ê³µê³  ì œëª©: {basic_info.get('title', 'N/A')}
íšŒì‚¬: {basic_info.get('company', 'N/A')}

ì•„ë˜ëŠ” ì±„ìš©ê³µê³  í˜ì´ì§€ì˜ ì „ì²´ í…ìŠ¤íŠ¸ì•¼. ê° ì„¹ì…˜ì„ ì°¾ì•„ì„œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ì¤˜:

{raw_text[:8000]}
        """.strip()
        
        for attempt in range(retry_count):
            try:
                response = await self.openai_client.chat.completions.create(
                    model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1,  # ë” ì •í™•í•œ ì¶”ì¶œì„ ìœ„í•´ ë‚®ì¶¤
                    max_tokens=3000  # ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•˜ë¯€ë¡œ í† í° ì¦ê°€
                )
                
                content = response.choices[0].message.content
                
                # JSON ì¶”ì¶œ
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    detail_info = json.loads(json_match.group(0))
                    return detail_info
                
            except Exception as e:
                if attempt == retry_count - 1:
                    print(f"  âš ï¸ LLM ì¶”ì¶œ ì‹¤íŒ¨ (ìµœì¢…): {e}")
                else:
                    await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
        
        return {}
    
    async def crawl_single_job(self, browser: Browser, job: Dict, index: int, total: int) -> Dict:
        """ê°œë³„ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ë¹„ë™ê¸°)"""
        async with self.semaphore:  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            job_id = str(job['id'])
            url = job['url']
            
            print(f"[{index}/{total}] ğŸ” {job['title']}")
            
            try:
                # ìƒˆ í˜ì´ì§€ ìƒì„±
                page: Page = await browser.new_page()
                
                # í˜ì´ì§€ ì ‘ì†
                await page.goto(url, timeout=30000, wait_until="domcontentloaded")
                await page.wait_for_timeout(2000)
                
                # HTML ìˆ˜ì§‘
                html = await page.content()
                clean_text = self._clean_html_text(html)
                
                # í˜ì´ì§€ ë‹«ê¸°
                await page.close()
                
                # LLMìœ¼ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬)
                detail_info = await self._extract_detail_with_llm(clean_text, job)
                
                # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
                merged = {**job, **detail_info}
                
                print(f"[{index}/{total}] âœ… ì™„ë£Œ: {job_id}")
                return merged
                
            except Exception as e:
                print(f"[{index}/{total}] âŒ ì‹¤íŒ¨ ({job_id}): {str(e)[:100]}")
                return job  # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ë°˜í™˜
    
    async def crawl_all_async(self, jobs: List[Dict]) -> List[Dict]:
        """ëª¨ë“  ê³µê³ ë¥¼ ë¹„ë™ê¸°ë¡œ í¬ë¡¤ë§"""
        print(f"\nğŸ” [2/4] ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹œì‘ ({len(jobs)}ê°œ, ë™ì‹œ {self.max_concurrent}ê°œ)\n")
        
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰ (ì¬ì‚¬ìš©)
            browser: Browser = await p.chromium.launch(headless=True)
            
            # ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            tasks = [
                self.crawl_single_job(browser, job, idx + 1, len(jobs))
                for idx, job in enumerate(jobs)
            ]
            
            # ëª¨ë“  ì‘ì—… ë™ì‹œ ì‹¤í–‰
            results = await asyncio.gather(*tasks)
            
            await browser.close()
        
        return results
    
    # ==================== 3ë‹¨ê³„: í†µí•© ì‹¤í–‰ ====================
    
    def crawl_all(self, max_jobs: Optional[int] = None) -> List[Dict]:
        """ì „ì²´ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("="*80)
        print("ğŸš€ LG ì±„ìš©ê³µê³  ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*80 + "\n")
        
        # 1ë‹¨ê³„: APIë¡œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        basic_jobs = self.get_job_list_from_api()
        
        if not basic_jobs:
            print("âŒ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        if max_jobs:
            basic_jobs = basic_jobs[:max_jobs]
            print(f"ğŸ“Š ìµœëŒ€ {max_jobs}ê°œë¡œ ì œí•œ\n")
        
        # 2ë‹¨ê³„: ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰
        detailed_jobs = asyncio.run(self.crawl_all_async(basic_jobs))
        
        print(f"\nâœ… [3/4] ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(detailed_jobs)}ê°œ")
        return detailed_jobs
    
    # ==================== 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ====================
    
    def save_results(self, jobs: List[Dict], output_dir: str = "output"):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ [4/4] ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_file = output_path / f"lg_jobs_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON ì €ì¥: {json_file}")
        
        # CSV ì €ì¥
        try:
            import csv
            csv_file = output_path / f"lg_jobs_{timestamp}.csv"
            
            if jobs:
                # ëª¨ë“  í‚¤ ìˆ˜ì§‘
                all_keys = set()
                for job in jobs:
                    all_keys.update(job.keys())
                
                with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
                    writer.writeheader()
                    writer.writerows(jobs)
                
                print(f"  âœ… CSV ì €ì¥: {csv_file}")
        except Exception as e:
            print(f"  âš ï¸ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print("\n" + "="*80)
        print(f"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! (ë™ì‹œ ì²˜ë¦¬: {self.max_concurrent}ê°œ)")
        print("="*80)
    
    def print_summary(self, jobs: List[Dict]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        for idx, job in enumerate(jobs[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ
            print(f"\n{idx}. {job.get('title', 'N/A')}")
            print(f"   íšŒì‚¬: {job.get('company', 'N/A')}")
            print(f"   ì§êµ°: {job.get('job_group', 'N/A')}")
            print(f"   ë§ˆê°: {job.get('deadline', 'N/A')}")
            
            if job.get('description'):
                desc = job['description'][:80] + "..." if len(job.get('description', '')) > 80 else job.get('description', '')
                print(f"   ğŸ“ {desc}")
        
        if len(jobs) > 10:
            print(f"\n... ì™¸ {len(jobs) - 10}ê°œ (ì „ì²´ ê²°ê³¼ëŠ” íŒŒì¼ ì°¸ì¡°)")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LG ì±„ìš©ê³µê³  ë¹„ë™ê¸° í¬ë¡¤ëŸ¬")
    parser.add_argument("--max-jobs", type=int, help="ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜")
    parser.add_argument("--concurrent", type=int, default=30, help="ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸: 5)")
    parser.add_argument("--output-dir", default="output", help="ì¶œë ¥ í´ë”")
    parser.add_argument("--no-summary", action="store_true", help="ìš”ì•½ ì¶œë ¥ ìƒëµ")
    args = parser.parse_args()
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = LGCareerAsyncCrawler(max_concurrent=args.concurrent)
    jobs = crawler.crawl_all(max_jobs=args.max_jobs)
    
    if jobs:
        # ê²°ê³¼ ì €ì¥
        crawler.save_results(jobs, output_dir=args.output_dir)
        
        # ìš”ì•½ ì¶œë ¥
        if not args.no_summary:
            crawler.print_summary(jobs)
    else:
        print("âŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()