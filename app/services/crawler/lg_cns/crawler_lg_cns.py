"""
LG ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (ë¦¬íŒ©í† ë§)
1. APIë¡œ ê³µê³  URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
2. Async Playwrightë¡œ ë³‘ë ¬ í¬ë¡¤ë§ (ë‹¨ì¼ ë¸Œë¼ìš°ì € + ì—¬ëŸ¬ í˜ì´ì§€)

ì£¼ìš” ê°œì„ ì‚¬í•­:
- AsyncOpenAI context manager ì‚¬ìš©ìœ¼ë¡œ í™•ì‹¤í•œ cleanup
- Playwright ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ê°•í™”
- ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 
"""

import json
import re
import os
import asyncio
from typing import List, Dict, Optional, Any
from pathlib import Path
from datetime import datetime

import requests
from dotenv import load_dotenv
try:
    from dotenv import find_dotenv  # type: ignore
except Exception:
    find_dotenv = None  # type: ignore
from playwright.async_api import async_playwright, BrowserContext, Page
from tenacity import retry, stop_after_attempt, wait_exponential

try:
    from app.services import resolve_dir, get_output_dir, get_img_dir
except ModuleNotFoundError:
    import sys
    _p = Path(__file__).resolve().parents[4]
    if str(_p) not in sys.path:
        sys.path.append(str(_p))
    from app.services import resolve_dir, get_output_dir, get_img_dir

try:
    from openai import AsyncOpenAI
except Exception:
    AsyncOpenAI = None

try:
    from app.utils.s3_uploader import upload_screenshot_to_s3
except ModuleNotFoundError:
    import sys
    _p = Path(__file__).resolve().parents[4]
    if str(_p) not in sys.path:
        sys.path.append(str(_p))
    from app.utils.s3_uploader import upload_screenshot_to_s3


def load_env() -> None:
    """Load environment variables from .env with fallbacks."""
    try:
        if find_dotenv is not None:
            found = find_dotenv(usecwd=True)
            if found:
                load_dotenv(found, override=False)
    except Exception:
        pass

    try:
        proj_env = Path(__file__).resolve().parents[5] / ".env"
        if proj_env.exists():
            load_dotenv(dotenv_path=proj_env, override=False)
    except Exception:
        pass

    try:
        backend_env = Path(__file__).resolve().parents[4] / ".env"
        if backend_env.exists():
            load_dotenv(dotenv_path=backend_env, override=False)
    except Exception:
        pass


load_env()


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
async def summarize_with_llm(raw_text: str, url: str, model: str = "gpt-4o-mini") -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì±„ìš©ê³µê³ ì—ì„œ ì •ë³´ ì¶”ì¶œ - AsyncOpenAI context manager ì‚¬ìš©"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key or AsyncOpenAI is None:
        return {}

    system_prompt = """
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³  ì›¹í˜ì´ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ HTML ì½˜í…ì¸ ì—ì„œ ë‹¤ìŒ í•„ë“œë“¤ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

# ì¶”ì¶œí•  í•„ë“œ
- title: ê³µê³  ì œëª©
- location: ê·¼ë¬´ ìœ„ì¹˜
- employment_type: ê³ ìš© í˜•íƒœ (ì •ê·œì§, ê³„ì•½ì§, íŒŒíŠ¸íƒ€ì„ ë“±)
- experience: ê²½ë ¥ ìš”êµ¬ì‚¬í•­ (ì‹ ì…, ê²½ë ¥, ê²½ë ¥ë¬´ê´€, ì¸í„´ ë“±)
- crawl_date: í¬ë¡¤ë§ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
- posted_date: ê³µê³  ê²Œì‹œì¼ (YYYY-MM-DD í˜•ì‹, ìƒì‹œì±„ìš©ì¸ ê²½ìš° í¬ë¡¤ë§ ë‚ ì§œì™€ ë™ì¼)
- expired_date: ê³µê³  ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì—†ìœ¼ë©´ null)
- meta_data: ìœ„ í•„ë“œ ì™¸ ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ JSON ê°ì²´

â€» company, description, html, urlì€ ë³„ë„ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì¶”ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

# ì¤‘ìš” ì§€ì¹¨
1. ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í†µì¼
2. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ë°˜í™˜ (ë¹ˆ ë¬¸ìì—´ X)
3. â­ **meta_dataëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ í‚¤ë¡œë§Œ êµ¬ì„±** (ì˜ˆ: "ì§ë¬´ë¶„ì•¼", "ìš°ëŒ€ì‚¬í•­", "ë³µë¦¬í›„ìƒ")
4. â­ **meta_dataì—ëŠ” ìœ„ì˜ ê¸°ë³¸ í•„ë“œ(title, company, location, employment_type, experience, crawl_date, posted_date, expired_date, description)ì™€ ì¤‘ë³µë˜ëŠ” ì •ë³´ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ**
5. â­ meta_dataì—ëŠ” ì˜¤ì§ ì¶”ê°€ì ì¸ ë³´ì¡° ì •ë³´ë§Œ í¬í•¨ (ì˜ˆ: ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, ë³µë¦¬í›„ìƒ, ë‹´ë‹¹ì—…ë¬´, í•™ë ¥ìš”ê±´, ì „í˜•ì ˆì°¨ ë“±)
6. â­ meta_dataì—ëŠ” ê¸°ìˆ ìŠ¤íƒ/ì†Œí”„íŠ¸ìŠ¤í‚¬ì„ ë„£ì§€ ì•ŠëŠ”ë‹¤ (ì˜ˆ: Python, Django, AWS, Docker ë“± ì œì™¸)
7. â­ meta_data í‚¤ëŠ” ì ˆëŒ€ ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ
8. ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ê³µë°± ì •ë¦¬ ë° ì •ê·œí™”

---
# Example (í•œêµ­ì–´ í‚¤ í•„ìˆ˜!)
{
    "title": "ë°±ì—”ë“œ ê°œë°œì (Python/Django)",
    "location": "ì„œìš¸ ê°•ë‚¨êµ¬",
    "employment_type": "ì •ê·œì§",
    "experience": "ê²½ë ¥ 3~5ë…„",
    "crawl_date": "2025-12-13",
    "posted_date": "2025-11-28",
    "expired_date": "2025-12-31",
    "meta_data": {
        "ì§ë¬´ë¶„ì•¼": "IT/ê°œë°œ",
        "ìš°ëŒ€ì‚¬í•­": ["AWS ê²½í—˜", "Docker/K8s ì‚¬ìš© ê²½í—˜", "MSA ì•„í‚¤í…ì²˜ ì´í•´"],
        "ë³µë¦¬í›„ìƒ": ["ê±´ê°•ê²€ì§„", "ìê¸°ê³„ë°œë¹„ ì§€ì›", "ìœ ì—°ê·¼ë¬´ì œ"],
        "í•™ë ¥ìš”ê±´": "í•™ì‚¬ ì´ìƒ",
        "ì „í˜•ì ˆì°¨": "ì„œë¥˜ì „í˜• > 1ì°¨ ë©´ì ‘ > 2ì°¨ ë©´ì ‘ > ìµœì¢…í•©ê²©",
        "ë‹´ë‹¹ì—…ë¬´": "ë°±ì—”ë“œ API ì„¤ê³„ ë° ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„",
        "ìê²©ìš”ê±´": "Python 3ë…„ ì´ìƒ ê²½í—˜"
    }
}
---

âš ï¸ ê²½ê³ : meta_dataì˜ í‚¤ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤!
âŒ ì˜ëª»ëœ ì˜ˆ: "job_category", "tech_stack", "benefits"
âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: "ì§ë¬´ë¶„ì•¼", "ìš°ëŒ€ì‚¬í•­", "ë³µë¦¬í›„ìƒ"
"""
    
    user_prompt = (
        f"ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().strftime('%Y-%m-%d')}ì´ê³ , ì´ ë‚ ì§œë¥¼ crawl_dateë¡œ ì‚¬ìš©í•´. "
        f"ê³µê³  URL: {url}\n\n"
        f"ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜:\n\n{raw_text[:8000]}"
    )

    # AsyncOpenAIë¥¼ async withë¡œ ì‚¬ìš©í•˜ì—¬ í™•ì‹¤í•œ cleanup
    async with AsyncOpenAI(api_key=api_key) as client:
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2,
            max_tokens=8000,
        )
        
        content = response.choices[0].message.content if response and response.choices else "{}"
        
        # JSONë§Œ ë‚¨ë„ë¡ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        json_text = json_match.group(0) if json_match else content
        
        try:
            data = json.loads(json_text)
            return data
        except Exception:
            return {}


async def crawl_single_job(
    context: BrowserContext, 
    job: Dict, 
    index: int, 
    total: int, 
    semaphore: asyncio.Semaphore, 
    screenshot_dir: Path = None
) -> Optional[Dict[str, Any]]:
    """ê°œë³„ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ë¹„ë™ê¸°) - context ê³µìœ , ê°ì page ìƒì„±, semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ"""
    page = None
    
    # Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ê°œìˆ˜ ì œí•œ
    async with semaphore:
        try:
            job_url = job['url']

            # ìƒˆ í˜ì´ì§€ ìƒì„± (contextëŠ” ê³µìœ )
            page = await context.new_page()

            print(f"  [{index}/{total}] ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            await page.goto(job_url, timeout=30000)
            await page.wait_for_timeout(2000)

            today = datetime.now().strftime('%Y-%m-%d')

            # ê¸°ë³¸ ì •ë³´ ì„¤ì •
            job_info = {
                "title": None,
                "company": "LG CNS",
                "location": None,
                "employment_type": None,
                "experience": None,
                "crawl_date": today,
                "posted_date": None,
                "expired_date": None,
                "description": None,
                "url": job_url,
                "meta_data": {},
                "screenshots": {},
            }

            # description ì¶”ì¶œ - ë‘ ê°œì˜ CSS Selectorë¥¼ ìˆœì„œëŒ€ë¡œ í•©ì¹˜ê¸°
            try:
                description_parts = []
                
                # ì²« ë²ˆì§¸: MuiCollapse-root
                collapse_element = await page.query_selector("div.MuiCollapse-root.MuiCollapse-vertical.MuiCollapse-entered.css-c4sutr")
                if collapse_element:
                    text1 = await collapse_element.inner_text()
                    if text1:
                        description_parts.append(text1.strip())
                
                # ë‘ ë²ˆì§¸: MuiBox-root css-h38rax
                box_element = await page.query_selector("div.MuiBox-root.css-h38rax")
                if box_element:
                    text2 = await box_element.inner_text()
                    if text2:
                        description_parts.append(text2.strip())
                
                # ë‘ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
                if description_parts:
                    job_info["description"] = "\n\n".join(description_parts)
                    print(f"  [{index}/{total}] description ì¶”ì¶œ ì™„ë£Œ ({len(description_parts)}ê°œ ì„¹ì…˜)")
                else:
                    print(f"  [{index}/{total}] description selectorë¥¼ ì°¾ì§€ ëª»í•¨, body ì „ì²´ ì‚¬ìš©")
                    job_info["description"] = await page.inner_text("body")
            except Exception as e:
                print(f"  [{index}/{total}] description ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ ì‚¬ìš©: {e}")
                job_info["description"] = await page.inner_text("body")

            # ìŠ¤í¬ë¦°ìƒ· S3 ì—…ë¡œë“œ
            if screenshot_dir:
                try:
                    job_id = job.get('id', f"job_{index}")
                    screenshot_filename = f"lg_cns_job_{job_id}.png"

                    # S3ì— ì§ì ‘ ì—…ë¡œë“œ
                    screenshot_bytes = await page.screenshot(full_page=True)
                    s3_url = upload_screenshot_to_s3(screenshot_bytes, screenshot_filename)

                    if s3_url:
                        job_info["screenshots"]["combined"] = s3_url
                        print(f"  [{index}/{total}] S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_url}")
                    else:
                        print(f"  [{index}/{total}] S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
                except Exception as e:
                    print(f"  [{index}/{total}] ìŠ¤í¬ë¦°ìƒ· ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            # LLMìœ¼ë¡œ ë‚˜ë¨¸ì§€ í•„ë“œ íŒŒì‹± (company, description, html, url ì œì™¸)
            try:
                # LLMì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                info_box = await page.query_selector("div.MuiBox-root.css-fflez4")
                if info_box:
                    llm_text = await info_box.inner_text()
                else:
                    llm_text = await page.inner_text("body")
                
                parsed_data = await summarize_with_llm(llm_text, job_url)
                
                if parsed_data:
                    for key in ["title", "location", "employment_type", "experience",
                               "posted_date", "expired_date", "meta_data"]:
                        if key in parsed_data and parsed_data[key] is not None:
                            job_info[key] = parsed_data[key]
                    print(f"  [{index}/{total}] LLM íŒŒì‹± ì™„ë£Œ")
            except Exception as e:
                print(f"  [{index}/{total}] LLM íŒŒì‹± ì‹¤íŒ¨ (descriptionì€ ì €ì¥ë¨): {e}")

            print(f"  [{index}/{total}] âœ… ì™„ë£Œ: {job_info.get('title', 'N/A')}")
            return job_info

        except Exception as e:
            print(f"  [{index}/{total}] âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
            
        finally:
            # í˜ì´ì§€ í™•ì‹¤í•˜ê²Œ ë‹«ê¸°
            if page:
                try:
                    await page.close()
                except Exception as e:
                    print(f"  [{index}/{total}] í˜ì´ì§€ ë‹«ê¸° ì‹¤íŒ¨: {e}")


class LGCareerCrawler:
    def __init__(self, max_concurrent: int = 30):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            max_concurrent: ë™ì‹œ í¬ë¡¤ë§ ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸: 30)
        """
        load_dotenv()
        
        # API ì„¤ì •
        self.base_url = "https://api.careers.lg.com"
        self.list_endpoint = "/rmk/job/retrieveJobNoticesList"
        self.detail_url_template = "https://careers.lg.com/apply/detail?id={}"
        
        # ìµœì†Œí•œì˜ í—¤ë”ë§Œ ì‚¬ìš©
        self.headers = {
            'accept': 'application/json',
            'content-type': 'application/json',
            'referer': 'https://careers.lg.com/',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ë™ì‹œ ì‹¤í–‰ ì œì–´
        self.max_concurrent = max_concurrent
    
    def get_job_list_from_api(self) -> List[Dict]:
        """APIì—ì„œ ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}{self.list_endpoint}"
        
        payload = {
            "lnbSearch": "",
            "hashTagText": "",
            "recDate": "CREATION_DATE",
            "order": "DESC",
            "careerList": [],
            "companyCodeList": [],
            "desireLocList": [],
            "jobGroupList": []
        }
        
        try:
            print("ğŸ“¡ [1/3] API í˜¸ì¶œ ì¤‘...")
            response = requests.post(url, headers=self.headers, json=payload, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                job_list = data.get('data', {}).get('jobNoticeList', [])
                
                jobs = []
                for job in job_list:
                    job_id = job.get('jobNoticeId')
                    
                    # APIì—ì„œ URL í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í…œí”Œë¦¿ìœ¼ë¡œ ìƒì„±
                    job_url = job.get('jobNoticeUrl') or job.get('url')
                    if not job_url:
                        job_url = self.detail_url_template.format(job_id)
                    
                    # ê¸°ë³¸ ì •ë³´ (APIì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ)
                    job_info = {
                        'id': job_id,
                        'url': job_url,
                        'api_title': job.get('jobNoticeName'),
                        'api_company': job.get('companyName'),
                    }
                    jobs.append(job_info)
                
                print(f"âœ… APIì—ì„œ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
                return jobs
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []

    async def crawl_details_async(self, jobs: List[Dict], screenshot_dir: Path = None) -> List[Dict]:
        """ë¹„ë™ê¸°ë¡œ ëª¨ë“  ê³µê³ ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        if not jobs:
            return []

        print(f"\nğŸ” [2/3] ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ ({len(jobs)}ê°œ, ìµœëŒ€ ë™ì‹œ {self.max_concurrent}ê°œ)\n")

        browser = None
        context = None
        
        try:
            async with async_playwright() as p:
                # ë‹¨ì¼ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()

                # Semaphore ìƒì„± (ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ)
                semaphore = asyncio.Semaphore(self.max_concurrent)

                # ëª¨ë“  ì‘ì—…ì„ asyncio.gatherë¡œ ë™ì‹œ ì‹¤í–‰
                tasks = [
                    crawl_single_job(context, job, idx + 1, len(jobs), semaphore, screenshot_dir)
                    for idx, job in enumerate(jobs)
                ]

                # ëª¨ë“  ì‘ì—… ì‹¤í–‰
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§ (ì˜ˆì™¸ ì œì™¸)
                detailed_jobs = [
                    job for job in results 
                    if job is not None and not isinstance(job, Exception)
                ]

                print(f"\nâœ… [2/3] ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(detailed_jobs)}/{len(jobs)}ê°œ ì„±ê³µ")
                return detailed_jobs
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return []
            
        finally:
            # ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
            if context:
                try:
                    await context.close()
                except Exception as e:
                    print(f"âš ï¸ Context ë‹«ê¸° ì‹¤íŒ¨: {e}")
            
            if browser:
                try:
                    await browser.close()
                except Exception as e:
                    print(f"âš ï¸ Browser ë‹«ê¸° ì‹¤íŒ¨: {e}")

    def crawl_details(self, jobs: List[Dict], screenshot_dir: Path = None) -> List[Dict]:
        """ë™ê¸° ë˜í¼: ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰"""
        return asyncio.run(self.crawl_details_async(jobs, screenshot_dir))

    def save_results(self, jobs: List[Dict], output_dir: str = "../../output"):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ [3/3] ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        output_path = resolve_dir(Path(output_dir), get_output_dir())
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_file = output_path / f"lg_jobs.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON ì €ì¥: {json_file}")
        
        # CSV ì €ì¥
        try:
            import csv
            csv_file = output_path / f"lg_jobs.csv"

            if jobs:
                simple_jobs = []
                for job in jobs:
                    simple_job = job.copy()
                    simple_job.pop('html', None)
                    
                    if 'meta_data' in simple_job:
                        simple_job['meta_data'] = json.dumps(simple_job['meta_data'], ensure_ascii=False)
                    if 'screenshots' in simple_job:
                        simple_job['screenshots'] = json.dumps(simple_job['screenshots'], ensure_ascii=False)
                    
                    simple_jobs.append(simple_job)
                
                all_keys = set()
                for job in simple_jobs:
                    all_keys.update(job.keys())
                
                with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
                    writer.writeheader()
                    writer.writerows(simple_jobs)
                
                print(f"  âœ… CSV ì €ì¥: {csv_file}")
        except Exception as e:
            print(f"  âš ï¸ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print("\n" + "="*80)
        print(f"ğŸ‰ LG í¬ë¡¤ë§ ì™„ë£Œ! (ë¹„ë™ê¸° ë™ì‹œ ì²˜ë¦¬: ìµœëŒ€ {self.max_concurrent}ê°œ)")
        print("="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="LG ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (Refactored)")
    parser.add_argument("--max-jobs", type=int, help="ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)")
    parser.add_argument("--max-concurrent", type=int, default=30, help="ìµœëŒ€ ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸: 30)")
    parser.add_argument("--output-dir", default="../../output", help="ì¶œë ¥ í´ë” (ê¸°ë³¸: ../../output)")
    parser.add_argument("--screenshot-dir", default="../../img", help="ìŠ¤í¬ë¦°ìƒ· í´ë” (ê¸°ë³¸: ../../img)")
    args = parser.parse_args()

    print("="*80)
    print("ğŸš€ LG ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì‹œì‘ (Refactored)")
    print("="*80 + "\n")

    crawler = LGCareerCrawler(max_concurrent=args.max_concurrent)

    # 1ë‹¨ê³„: APIë¡œ URL ìˆ˜ì§‘
    basic_jobs = crawler.get_job_list_from_api()

    if not basic_jobs:
        print("âŒ ì±„ìš©ê³µê³  ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    if args.max_jobs:
        basic_jobs = basic_jobs[:args.max_jobs]
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸: ìµœëŒ€ {args.max_jobs}ê°œë¡œ ì œí•œ\n")

    # ìŠ¤í¬ë¦°ìƒ· ë””ë ‰í† ë¦¬ ì¤€ë¹„
    screenshot_dir = resolve_dir(Path(args.screenshot_dir), get_img_dir())
    screenshot_dir.mkdir(parents=True, exist_ok=True)

    # 2ë‹¨ê³„: ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§
    detailed_jobs = crawler.crawl_details(basic_jobs, screenshot_dir=screenshot_dir)

    if not detailed_jobs:
        print("âŒ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    crawler.save_results(detailed_jobs, output_dir=args.output_dir)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ (ì²˜ìŒ 3ê°œ)")
    print("="*80)
    
    for idx, job in enumerate(detailed_jobs[:3], 1):
        print(f"\n{idx}. {job.get('title', 'N/A')}")
        print(f"   íšŒì‚¬: {job.get('company', 'N/A')}")
        print(f"   ìœ„ì¹˜: {job.get('location', 'N/A')}")
        print(f"   ê²½ë ¥: {job.get('experience', 'N/A')}")
        print(f"   ë§ˆê°: {job.get('expired_date', 'N/A')}")
        print(f"   URL: {job.get('url', 'N/A')}")
    
    if len(detailed_jobs) > 3:
        print(f"\n... ì™¸ {len(detailed_jobs) - 3}ê°œ (ì „ì²´ ê²°ê³¼ëŠ” íŒŒì¼ ì°¸ì¡°)")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    main()