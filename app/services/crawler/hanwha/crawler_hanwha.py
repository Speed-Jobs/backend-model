"""
í•œí™”ì‹œìŠ¤í…œ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (ë¦¬íŒ©í† ë§)
1. HTML íŒŒì‹±ìœ¼ë¡œ ê³µê³  URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
2. ThreadPoolExecutorë¡œ ë³‘ë ¬ í¬ë¡¤ë§ (ê° ìŠ¤ë ˆë“œê°€ ë…ë¦½ ë¸Œë¼ìš°ì €)

ì£¼ìš” ê°œì„ ì‚¬í•­:
- OpenAI í´ë¼ì´ì–¸íŠ¸ ëª…ì‹œì  ê´€ë¦¬
- Playwright ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•œ cleanup
- ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 
"""

import argparse
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from bs4 import BeautifulSoup
from dotenv import load_dotenv
try:
    from dotenv import find_dotenv  # type: ignore
except Exception:
    find_dotenv = None  # type: ignore
from tenacity import retry, stop_after_attempt, wait_exponential

from playwright.sync_api import sync_playwright, Browser, Page, BrowserContext
try:
    from app.services import resolve_dir, get_output_dir, get_img_dir
except ModuleNotFoundError:
    import sys
    _p = Path(__file__).resolve().parents[4]
    if str(_p) not in sys.path:
        sys.path.append(str(_p))
    from app.services import resolve_dir, get_output_dir, get_img_dir
import concurrent.futures

try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore


def load_env() -> None:
    """Load environment variables from .env with fallbacks."""
    try:
        if find_dotenv is not None:
            found = find_dotenv(usecwd=True)
            if found:
                load_dotenv(found, override=False)
    except Exception:
        pass

    try:
        proj_env = Path(__file__).resolve().parents[5] / ".env"
        if proj_env.exists():
            load_dotenv(dotenv_path=proj_env, override=False)
    except Exception:
        pass

    try:
        legacy_env = Path(__file__).resolve().parents[4] / ".env"
        if legacy_env.exists():
            load_dotenv(dotenv_path=legacy_env, override=False)
    except Exception:
        pass


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


load_env()


def clean_text_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")

    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    text = soup.get_text("\n")
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t\x0b\x0c\r]+", " ", text)
    lines = [ln.strip() for ln in text.splitlines()]
    lines = [ln for ln in lines if ln]
    return "\n".join(lines)


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(5))
def summarize_with_llm(raw_text: str, model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key or OpenAI is None:
        return []

    system_prompt = """
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³  ì›¹í˜ì´ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ HTML ì½˜í…ì¸ ì—ì„œ ë‹¤ìŒ í•„ë“œë“¤ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

# ì¶”ì¶œí•  í•„ë“œ
- title: ê³µê³  ì œëª©
- company: íšŒì‚¬ ì´ë¦„
- location: ê·¼ë¬´ ìœ„ì¹˜
- employment_type: ê³ ìš© í˜•íƒœ (ì •ê·œì§, ê³„ì•½ì§, íŒŒíŠ¸íƒ€ì„ ë“±)
- experience: ê²½ë ¥ ìš”êµ¬ì‚¬í•­ (ì‹ ì…, ê²½ë ¥, ê²½ë ¥ë¬´ê´€, ì¸í„´ ë“±)
- crawl_date: í¬ë¡¤ë§ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
- posted_date: ê³µê³  ê²Œì‹œì¼ (YYYY-MM-DD í˜•ì‹, ìƒì‹œì±„ìš©ì¸ ê²½ìš° í¬ë¡¤ë§ ë‚ ì§œì™€ ë™ì¼)
- expired_date: ê³µê³  ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì—†ìœ¼ë©´ null)
- description: ì±„ìš©ê³µê³  ì „ë¬¸ í…ìŠ¤íŠ¸ (HTML íƒœê·¸ ì œê±°)
- meta_data: ìœ„ í•„ë“œ ì™¸ ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ JSON ê°ì²´ (ì˜ˆ: ì§êµ°, ì—°ë´‰ì •ë³´, ë³µë¦¬í›„ìƒ, ìš°ëŒ€ì‚¬í•­, ê¸°ìˆ ìŠ¤íƒ ë“±)

# ì¤‘ìš” ì§€ì¹¨
1. ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í†µì¼
2. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ë°˜í™˜ (ë¹ˆ ë¬¸ìì—´ X)
3. descriptionì€ HTML íƒœê·¸ë¥¼ ì œê±°í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸
4. meta_dataëŠ” ì˜ë¯¸ìˆëŠ” í‚¤ ì´ë¦„ìœ¼ë¡œ êµ¬ì¡°í™” (ì˜ë¬¸ snake_case ì‚¬ìš©)

---
# Example
{
    "title": "ë°±ì—”ë“œ ê°œë°œì (Python/Django)",
    "company": "(ì£¼)í…Œí¬ìŠ¤íƒ€íŠ¸ì—…",
    "location": "ì„œìš¸ ê°•ë‚¨êµ¬",
    "employment_type": "ì •ê·œì§",
    "experience": "ê²½ë ¥ 3~5ë…„",
    "crawl_date": "2025-11-05",
    "posted_date": "2025-10-28",
    "expired_date": "2025-11-30",
    "description": "ì£¼ìš”ì—…ë¬´...",
    "meta_data": {"job_category": "IT/ê°œë°œ"}
}
---
"""
    
    user_prompt = (
        f"ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().strftime('%Y-%m-%d')}ì´ê³ , ì´ ë‚ ì§œë¥¼ crawl_dateë¡œ ì‚¬ìš©í•´. "
        f"ê³µê³ ë“¤ì„ ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ì¶° ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.\n\n" + raw_text
    )

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì‚¬ìš©
    client = OpenAI(api_key=api_key)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2,
            max_tokens=8000,
        )
        content = response.choices[0].message.content if response and response.choices else "[]"

        # JSONë§Œ ë‚¨ë„ë¡ íŠ¸ë¦¬ë°
        json_text_match = re.search(r"(\[.*\])", content, re.DOTALL)
        json_text = json_text_match.group(1) if json_text_match else content
        
        try:
            data = json.loads(json_text)
            if isinstance(data, list):
                return data
        except Exception:
            pass
        return []
    finally:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ëª…ì‹œì  ì •ë¦¬
        client.close()


def wait_for_results(page: Page, timeout_ms: int = 10000) -> None:
    """í•œí™”ì‹œìŠ¤í…œ ì±„ìš© í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°"""
    print(f"[3/10] ê²€ìƒ‰ ê²°ê³¼ ë Œë”ë§ ëŒ€ê¸° ì¤‘... (ìµœëŒ€ {timeout_ms}ms)")
    selectors = [
        "li[onclick*='goView']",
        ".recruit_list",
        "ul li",
    ]
    elapsed = 0
    step_ms = 700
    while elapsed < timeout_ms:
        for sel in selectors:
            try:
                page.wait_for_selector(sel, timeout=900)
                print(f"[3/10] ëŒ€í‘œ ì…€ë ‰í„° ê°ì§€: {sel}")
                return
            except Exception:
                continue
        try:
            page.mouse.wheel(0, 800)
        except Exception:
            pass
        page.wait_for_timeout(step_ms)
        elapsed += step_ms
    print("[3/10] íƒ€ì„ì•„ì›ƒ ë„ë‹¬: ì…€ë ‰í„° ë¯¸ê°ì§€ - ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰")


def extract_job_detail_from_url(job_url: str, job_index: int, screenshot_dir: Path = None) -> Dict[str, Any]:
    """URLë¡œ ì§ì ‘ ì ‘ì†í•˜ì—¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ìš© - ë…ë¦½ ë¸Œë¼ìš°ì €)"""
    playwright_instance = None
    browser = None
    context = None
    page = None
    
    try:
        # ê° ìŠ¤ë ˆë“œì—ì„œ ë…ë¦½ì ì¸ playwrightì™€ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        playwright_instance = sync_playwright().start()
        browser = playwright_instance.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        print(f"  [{job_index}] ìƒì„¸ í˜ì´ì§€ ë¡œë”©...")
        page.goto(job_url, timeout=30000)
        page.wait_for_timeout(2000)

        today = datetime.now().strftime('%Y-%m-%d')

        job_info = {
            "title": None,
            "company": "í•œí™”ì‹œìŠ¤í…œ",
            "location": None,
            "employment_type": None,
            "experience": None,
            "crawl_date": today,
            "posted_date": None,
            "expired_date": None,
            "description": None,
            "url": job_url,
            "meta_data": "{}",
            "screenshots": {},
        }

        # description ì¶”ì¶œ - div.recruit1ê³¼ div.recruit2ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        full_text = page.inner_text("body")
        try:
            recruit1 = page.query_selector("div.recruit1")
            recruit2 = page.query_selector("div.recruit2")

            description_parts = []

            if recruit1:
                recruit1_text = recruit1.inner_text()
                if recruit1_text:
                    description_parts.append(recruit1_text)

            if recruit2:
                recruit2_text = recruit2.inner_text()
                if recruit2_text:
                    description_parts.append(recruit2_text)

            if description_parts:
                job_info["description"] = "\n\n".join(description_parts)
            else:
                job_info["description"] = full_text

            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            if screenshot_dir:
                try:
                    seq_match = re.search(r'rtSeq=(\d+)', job_url)
                    seq_id = seq_match.group(1) if seq_match else f"job_{job_index}"

                    screenshot_filename = f"hanwha_job_{seq_id}.png"
                    screenshot_path = screenshot_dir / screenshot_filename

                    page.screenshot(path=str(screenshot_path), full_page=True)
                    job_info["screenshots"]["combined"] = str(screenshot_path)
                    print(f"  [{job_index}] ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_filename}")
                except Exception as e:
                    print(f"  [{job_index}] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")

        except Exception as e:
            print(f"  [{job_index}] recruit1/recruit2 ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ ì‚¬ìš©: {e}")
            job_info["description"] = full_text

        # LLMìœ¼ë¡œ íŒŒì‹± ì‹œë„ (ë‹¤ë¥¸ í•„ë“œë§Œ ì¶”ì¶œ)
        try:
            parsed = summarize_with_llm(full_text)
            if parsed and len(parsed) > 0:
                parsed_data = parsed[0]
                for key in ["title", "company", "location", "employment_type", "experience",
                           "posted_date", "expired_date", "meta_data"]:
                    if key in parsed_data and parsed_data[key]:
                        job_info[key] = parsed_data[key]
        except Exception as e:
            print(f"  [{job_index}] LLM íŒŒì‹± ì‹¤íŒ¨ (descriptionì€ ì €ì¥ë¨): {e}")

        print(f"  [{job_index}] ì™„ë£Œ: {job_info.get('title', 'N/A')}")
        return job_info

    except Exception as e:
        print(f"  [{job_index}] ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
        if page:
            try:
                page.close()
            except Exception:
                pass
        if context:
            try:
                context.close()
            except Exception:
                pass
        if browser:
            try:
                browser.close()
            except Exception:
                pass
        if playwright_instance:
            try:
                playwright_instance.stop()
            except Exception:
                pass


def run_scrape(
    out_dir: Path = None,
    screenshot_dir: Path = None,
    fast: bool = False
) -> Dict[str, Path]:
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    out_dir = resolve_dir(out_dir, get_output_dir())
    screenshot_dir = resolve_dir(screenshot_dir, get_img_dir())
    ensure_dir(out_dir)
    ensure_dir(screenshot_dir)

    outputs = {
        "raw_html": out_dir / "hanwha_raw.html",
        "clean_txt": out_dir / "hanwha_clean.txt",
        "json": out_dir / "hanwha_jobs.json",
        "screenshots": screenshot_dir,
    }

    all_job_urls = []
    jobs_list = []

    playwright_instance = None
    browser = None
    context = None
    page = None

    try:
        print("[1/10] ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
        playwright_instance = sync_playwright().start()
        browser = playwright_instance.chromium.launch(headless=True)
        context = browser.new_context()
        if fast:
            try:
                context.set_default_timeout(5000)
            except Exception:
                pass
        page = context.new_page()

        base_url = "https://www.hanwhain.com/web/apply/notification/list.do?schTp=&schNm=&schSdSeqsParam=215&schRjSeqsParam=29&schRtRegularYnParam="

        print(f"[2/10] í˜ì´ì§€ ì ‘ì†: {base_url}")
        page.goto(base_url, timeout=60000)
        page.wait_for_load_state("domcontentloaded")
        page.wait_for_timeout(3000)

        wait_for_results(page, timeout_ms=(7000 if fast else 12000))

        # ìŠ¤í¬ë¡¤
        print(f"[4/10] í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ì½˜í…ì¸  ë¡œë“œ ì¤‘...")
        for _ in range(3):
            try:
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(800)
            except Exception:
                pass

        # ì±„ìš© ê³µê³  ë§í¬ ì¶”ì¶œ
        print(f"[5/10] ì±„ìš© ê³µê³  ë§í¬ ì¶”ì¶œ ì¤‘...")
        html = page.content()
        pattern = r"goView\((\d+)\)"
        matches = re.findall(pattern, html)

        for seq in matches:
            job_url = f"https://www.hanwhain.com/web/apply/notification/view.do?rtSeq={seq}"
            if job_url not in all_job_urls:
                all_job_urls.append(job_url)

        print(f"[6/10] ì „ì²´ ìˆ˜ì§‘ëœ URL: {len(all_job_urls)}ê°œ")

        # HTML ì €ì¥
        outputs["raw_html"].write_text(html, encoding="utf-8")
        print(f"[8/10] ì›ë³¸ HTML ì €ì¥: {outputs['raw_html']}")

    finally:
        # ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
        if page:
            try:
                page.close()
            except Exception:
                pass
        if context:
            try:
                context.close()
            except Exception:
                pass
        if browser:
            try:
                browser.close()
            except Exception:
                pass
        if playwright_instance:
            try:
                playwright_instance.stop()
            except Exception:
                pass

    # ë³‘ë ¬ë¡œ ê° URLì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
    print(f"[7/10] ë³‘ë ¬ë¡œ {len(all_job_urls)}ê°œ ê³µê³  ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
    if all_job_urls:
        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
            futures = []
            for idx, job_url in enumerate(all_job_urls, 1):
                future = executor.submit(extract_job_detail_from_url, job_url, idx, screenshot_dir)
                futures.append(future)

            for future in concurrent.futures.as_completed(futures):
                try:
                    job_info = future.result()
                    if job_info:
                        jobs_list.append(job_info)
                except Exception as e:
                    print(f"  ì‘ì—… ì‹¤íŒ¨: {e}")

    # ì •ì œ
    if html:
        clean_txt = clean_text_from_html(html)
        print("[9/10] HTML ê¸°ë°˜ ì •ì œ ì™„ë£Œ")
        outputs["clean_txt"].write_text(clean_txt, encoding="utf-8")
        print(f"[9/10] ì •ì œ í…ìŠ¤íŠ¸ ì €ì¥: {outputs['clean_txt']}")

    return outputs, jobs_list


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    env_path = Path(__file__).parent.parent.parent.parent / ".env"
    load_dotenv(dotenv_path=env_path)

    parser = argparse.ArgumentParser(description="í•œí™”ì‹œìŠ¤í…œ ì±„ìš© ìŠ¤í¬ë˜í•‘ (Refactored)")
    parser.add_argument("--out-dir", default="../../output", help="ì¶œë ¥ í´ë”")
    parser.add_argument("--screenshot-dir", default="../../img", help="ìŠ¤í¬ë¦°ìƒ· í´ë”")
    parser.add_argument("--model", default=os.getenv("OPENAI_MODEL", "gpt-4o-mini"), help="OpenAI ëª¨ë¸")
    parser.add_argument("--fast", action="store_true", help="ë¹ ë¥¸ ëª¨ë“œ")
    args = parser.parse_args()

    out_dir = Path(args.out_dir)
    screenshot_dir = Path(args.screenshot_dir)
    
    print("="*80)
    print("ğŸš€ í•œí™”ì‹œìŠ¤í…œ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì‹œì‘ (Refactored)")
    print("="*80)
    print("[0/10] ì‘ì—… ì‹œì‘")

    paths, items = run_scrape(
        out_dir=out_dir,
        screenshot_dir=screenshot_dir,
        fast=args.fast
    )

    if items:
        print(f"[9/10] ì´ {len(items)}ê°œì˜ ê³µê³  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print("[9/10] ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")

    # ì €ì¥
    paths["json"].write_text(json.dumps(items, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"[9/10] JSON ì €ì¥ ì™„ë£Œ: {paths['json']}")

    print(str(paths["json"]))
    print("[10/10] ì‘ì—… ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    main()