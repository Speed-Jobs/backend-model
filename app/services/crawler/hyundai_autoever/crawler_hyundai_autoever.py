"""
í˜„ëŒ€ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬
1. HTML íŒŒì‹±ìœ¼ë¡œ ê³µê³  URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
2. Async Playwrightë¡œ ë³‘ë ¬ í¬ë¡¤ë§ (ë‹¨ì¼ ë¸Œë¼ìš°ì € + ì—¬ëŸ¬ í˜ì´ì§€)
"""

import json
import re
import os
import asyncio
from typing import List, Dict, Optional, Any
from pathlib import Path
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from playwright.async_api import async_playwright, BrowserContext, Page
from tenacity import retry, stop_after_attempt, wait_exponential

try:
    from app.services import resolve_dir, get_output_dir, get_img_dir
except ModuleNotFoundError:
    import sys
    _p = Path(__file__).resolve().parents[4]
    if str(_p) not in sys.path:
        sys.path.append(str(_p))
    from app.services import resolve_dir, get_output_dir, get_img_dir

try:
    from openai import AsyncOpenAI
except Exception:
    AsyncOpenAI = None


def load_env() -> None:
    """Load environment variables from .env with fallbacks.

    Order: nearest discoverable .env from CWD â†’ fproject/.env â†’ backend-model/.env
    """
    try:
        from dotenv import find_dotenv
        found = find_dotenv(usecwd=True)
        if found:
            load_dotenv(found, override=False)
    except Exception:
        pass

    try:
        proj_env = Path(__file__).resolve().parents[5] / ".env"
        if proj_env.exists():
            load_dotenv(dotenv_path=proj_env, override=False)
    except Exception:
        pass

    try:
        backend_env = Path(__file__).resolve().parents[4] / ".env"
        if backend_env.exists():
            load_dotenv(dotenv_path=backend_env, override=False)
    except Exception:
        pass


load_env()


def get_openai_client() -> Optional[Any]:
    """OpenAI ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return None
    if AsyncOpenAI is None:
        return None
    return AsyncOpenAI(api_key=api_key)


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
async def summarize_with_llm(raw_text: str, url: str, model: str = "gpt-4o-mini") -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì±„ìš©ê³µê³ ì—ì„œ ì •ë³´ ì¶”ì¶œ (description ì œì™¸) - ë¹„ë™ê¸°"""
    client = get_openai_client()
    if client is None:
        return {}

    system_prompt = """
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³  ì›¹í˜ì´ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ HTML ì½˜í…ì¸ ì—ì„œ ë‹¤ìŒ í•„ë“œë“¤ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

# ì¶”ì¶œí•  í•„ë“œ
- title: ê³µê³  ì œëª©
- location: ê·¼ë¬´ ìœ„ì¹˜
- employment_type: ê³ ìš© í˜•íƒœ (ì •ê·œì§, ê³„ì•½ì§, íŒŒíŠ¸íƒ€ì„ ë“±)
- experience: ê²½ë ¥ ìš”êµ¬ì‚¬í•­ (ì‹ ì…, ê²½ë ¥, ê²½ë ¥ë¬´ê´€, ì¸í„´ ë“±)
- crawl_date: í¬ë¡¤ë§ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
- posted_date: ê³µê³  ê²Œì‹œì¼ (YYYY-MM-DD í˜•ì‹, ìƒì‹œì±„ìš©ì¸ ê²½ìš° í¬ë¡¤ë§ ë‚ ì§œì™€ ë™ì¼)
- expired_date: ê³µê³  ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì—†ìœ¼ë©´ null)
- meta_data: ìœ„ í•„ë“œ ì™¸ ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ JSON ê°ì²´ (ì˜ˆ: ì§êµ°, ì—°ë´‰ì •ë³´, ë³µë¦¬í›„ìƒ, ìš°ëŒ€ì‚¬í•­, ê¸°ìˆ ìŠ¤íƒ ë“±)

â€» company, description, html, urlì€ ë³„ë„ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì¶”ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

# ì¤‘ìš” ì§€ì¹¨
1. ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í†µì¼
2. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ë°˜í™˜ (ë¹ˆ ë¬¸ìì—´ X)
3. meta_dataëŠ” ì˜ë¯¸ìˆëŠ” í‚¤ ì´ë¦„ìœ¼ë¡œ êµ¬ì¡°í™” (ì˜ë¬¸ snake_case ì‚¬ìš©)
4. ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ê³µë°± ì •ë¦¬ ë° ì •ê·œí™”

---
# Example
{
    "title": "ë°±ì—”ë“œ ê°œë°œì (Python/Django)",
    "location": "ì„œìš¸ ê°•ë‚¨êµ¬",
    "employment_type": "ì •ê·œì§",
    "experience": "ê²½ë ¥ 3~5ë…„",
    "crawl_date": "2025-11-05",
    "posted_date": "2025-10-28",
    "expired_date": "2025-11-30",
    "meta_data": {
        "job_category": "IT/ê°œë°œ",
        "tech_stack": ["Python", "Django", "PostgreSQL"],
        "benefits": "4ëŒ€ë³´í—˜, ì—°ì°¨, ì¬íƒê·¼ë¬´"
    }
}
---
"""
    
    user_prompt = (
        f"ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().strftime('%Y-%m-%d')}ì´ê³ , ì´ ë‚ ì§œë¥¼ crawl_dateë¡œ ì‚¬ìš©í•´. "
        f"ê³µê³  URL: {url}\n\n"
        f"ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜:\n\n{raw_text[:8000]}"
    )

    response = await client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
        max_tokens=3000,
    )
    
    content = response.choices[0].message.content if response and response.choices else "{}"
    
    # JSONë§Œ ë‚¨ë„ë¡ ì¶”ì¶œ
    json_match = re.search(r'\{.*\}', content, re.DOTALL)
    json_text = json_match.group(0) if json_match else content
    
    try:
        data = json.loads(json_text)
        return data
    except Exception:
        return {}


async def crawl_single_job(context: BrowserContext, job: Dict, index: int, total: int, semaphore: asyncio.Semaphore, screenshot_dir: Path = None) -> Optional[Dict[str, Any]]:
    """ê°œë³„ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ë¹„ë™ê¸°) - context ê³µìœ , ê°ì page ìƒì„±, semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ"""
    # Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ê°œìˆ˜ ì œí•œ
    async with semaphore:
        page = None
        try:
            job_url = job['url']

            # ìƒˆ í˜ì´ì§€ ìƒì„± (contextëŠ” ê³µìœ )
            page = await context.new_page()

            print(f"  [{index}/{total}] ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            await page.goto(job_url, timeout=30000)
            await page.wait_for_timeout(2000)

            today = datetime.now().strftime('%Y-%m-%d')

            # ê¸°ë³¸ ì •ë³´ ì„¤ì •
            job_info = {
                "title": None,
                "company": "í˜„ëŒ€ì˜¤í† ì—ë²„",
                "location": None,
                "employment_type": None,
                "experience": None,
                "crawl_date": today,
                "posted_date": None,
                "expired_date": None,
                "description": None,
                "url": job_url,
                "meta_data": {},
                "screenshots": {},  # ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ ì €ì¥ìš©
            }

            # description ì¶”ì¶œ - CSS Selector ì‚¬ìš©
            try:
                description_element = await page.query_selector("div.sc-2109ef67-5.cSmBgl")
                if description_element:
                    description_text = await description_element.inner_text()
                    if description_text:
                        job_info["description"] = description_text
                        print(f"  [{index}/{total}] description ì¶”ì¶œ ì™„ë£Œ")
                else:
                    # ë°±ì—…: body ì „ì²´ ì‚¬ìš©
                    print(f"  [{index}/{total}] description selectorë¥¼ ì°¾ì§€ ëª»í•¨, body ì „ì²´ ì‚¬ìš©")
                    job_info["description"] = await page.inner_text("body")
            except Exception as e:
                print(f"  [{index}/{total}] description ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ ì‚¬ìš©: {e}")
                job_info["description"] = await page.inner_text("body")

            # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            if screenshot_dir:
                try:
                    # URLì—ì„œ job ID ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª…ì— ì‚¬ìš©
                    job_id = job.get('id', f"job_{index}")

                    screenshot_filename = f"hyundai_autoever_job_{job_id}.png"
                    screenshot_path = screenshot_dir / screenshot_filename

                    # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·
                    await page.screenshot(path=str(screenshot_path), full_page=True)

                    job_info["screenshots"]["combined"] = str(screenshot_path)
                    print(f"  [{index}/{total}] ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_filename}")
                except Exception as e:
                    print(f"  [{index}/{total}] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")

            # LLMìœ¼ë¡œ ë‚˜ë¨¸ì§€ í•„ë“œ íŒŒì‹± (description ì œì™¸)
            try:
                full_text = await page.inner_text("body")
                parsed_data = await summarize_with_llm(full_text, job_url)
                
                if parsed_data:
                    # descriptionì„ ì œì™¸í•œ í•„ë“œë§Œ ì—…ë°ì´íŠ¸
                    for key in ["title", "location", "employment_type", "experience",
                               "posted_date", "expired_date", "meta_data"]:
                        if key in parsed_data and parsed_data[key] is not None:
                            job_info[key] = parsed_data[key]
                    print(f"  [{index}/{total}] LLM íŒŒì‹± ì™„ë£Œ")
            except Exception as e:
                print(f"  [{index}/{total}] LLM íŒŒì‹± ì‹¤íŒ¨ (descriptionì€ ì €ì¥ë¨): {e}")

            # í˜ì´ì§€ ë‹«ê¸°
            await page.close()
            
            print(f"  [{index}/{total}] âœ… ì™„ë£Œ: {job_info.get('title', 'N/A')}")
            return job_info

        except Exception as e:
            print(f"  [{index}/{total}] âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # í˜ì´ì§€ê°€ ì—´ë ¤ìˆìœ¼ë©´ ë‹«ê¸°
            if page:
                try:
                    await page.close()
                except:
                    pass
            return None


class HyundaiAutoeverCrawler:
    def __init__(self, max_concurrent: int = 30):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            max_concurrent: ë™ì‹œ í¬ë¡¤ë§ ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸: 30)
        """
        load_dotenv()
        
        # ê¸°ë³¸ URL ì„¤ì •
        self.base_url = "https://career.hyundai-autoever.com"
        self.list_url = f"{self.base_url}/ko/apply"
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        # ë™ì‹œ ì‹¤í–‰ ì œì–´
        self.max_concurrent = max_concurrent
    
    # ==================== 1ë‹¨ê³„: HTML íŒŒì‹±ìœ¼ë¡œ URL ìˆ˜ì§‘ ====================
    
    def get_job_list(self) -> List[Dict]:
        """HTML íŒŒì‹±ìœ¼ë¡œ ì±„ìš©ê³µê³  URL ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        print("ğŸ“¡ [1/3] ì±„ìš©ê³µê³  ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            response = requests.get(self.list_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # CSS ì„ íƒìë¡œ /ko/o/ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ë§í¬ ì„ íƒ
            job_links = soup.select('a[href^="/ko/o/"]')
            
            # URL ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set
            seen_urls = set()
            jobs = []
            
            for link in job_links:
                href = link.get('href')
                full_url = self.base_url + href
                
                # ì¤‘ë³µ ì œê±°
                if full_url not in seen_urls:
                    seen_urls.add(full_url)
                    
                    # URLì—ì„œ ID ì¶”ì¶œ
                    job_id = href.split('/')[-1]
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ (ìˆìœ¼ë©´)
                    title = link.get_text(strip=True) or f"ê³µê³ _{job_id}"
                    
                    job_info = {
                        'id': job_id,
                        'title': title,
                        'url': full_url
                    }
                    jobs.append(job_info)
            
            print(f"âœ… {len(jobs)}ê°œ ì±„ìš©ê³µê³  URL ìˆ˜ì§‘ ì™„ë£Œ")
            return jobs
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
        except Exception as e:
            print(f"âŒ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    # ==================== 2ë‹¨ê³„: ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§ ====================

    async def crawl_details_async(self, jobs: List[Dict], screenshot_dir: Path = None) -> List[Dict]:
        """ë¹„ë™ê¸°ë¡œ ëª¨ë“  ê³µê³ ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ë‹¨ì¼ ë¸Œë¼ìš°ì € + ì—¬ëŸ¬ í˜ì´ì§€ + Semaphore)"""
        if not jobs:
            return []

        print(f"\nğŸ” [2/3] ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ ({len(jobs)}ê°œ, ìµœëŒ€ ë™ì‹œ {self.max_concurrent}ê°œ)\n")

        async with async_playwright() as p:
            # ë‹¨ì¼ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤
            browser = await p.chromium.launch(headless=True)
            # ë‹¨ì¼ context
            context = await browser.new_context()

            # Semaphore ìƒì„± (ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ)
            semaphore = asyncio.Semaphore(self.max_concurrent)

            # ëª¨ë“  ì‘ì—…ì„ asyncio.gatherë¡œ ë™ì‹œ ì‹¤í–‰ (Semaphoreê°€ ì œì–´)
            tasks = [
                crawl_single_job(context, job, idx + 1, len(jobs), semaphore, screenshot_dir)
                for idx, job in enumerate(jobs)
            ]

            # ëª¨ë“  ì‘ì—… ì‹¤í–‰
            results = await asyncio.gather(*tasks)

            # Noneì´ ì•„ë‹Œ ê²°ê³¼ë§Œ í•„í„°ë§
            detailed_jobs = [job for job in results if job is not None]

            await context.close()
            await browser.close()

        print(f"\nâœ… [2/3] ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(detailed_jobs)}/{len(jobs)}ê°œ ì„±ê³µ")
        return detailed_jobs

    def crawl_details(self, jobs: List[Dict], screenshot_dir: Path = None) -> List[Dict]:
        """ë™ê¸° ë˜í¼: ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰"""
        return asyncio.run(self.crawl_details_async(jobs, screenshot_dir))
    
    # ==================== 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ====================
    
    def save_results(self, jobs: List[Dict], output_dir: str = "output"):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ [3/3] ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        output_path = resolve_dir(Path(output_dir), get_output_dir())
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_file = output_path / f"hyundai_autoever_jobs_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON ì €ì¥: {json_file}")
        
        # CSV ì €ì¥
        try:
            import csv
            csv_file = output_path / f"hyundai_autoever_jobs_{timestamp}.csv"

            if jobs:
                # CSVì— í•„ìš”í•œ í•„ë“œë§Œ í¬í•¨ (html ì œì™¸)
                simple_jobs = []
                for job in jobs:
                    simple_job = job.copy()
                    # html í•„ë“œ ì œê±°
                    simple_job.pop('html', None)
                    # meta_dataë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    if 'meta_data' in simple_job:
                        simple_job['meta_data'] = json.dumps(simple_job['meta_data'], ensure_ascii=False)
                    # screenshotsë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    if 'screenshots' in simple_job:
                        simple_job['screenshots'] = json.dumps(simple_job['screenshots'], ensure_ascii=False)
                    simple_jobs.append(simple_job)
                
                all_keys = set()
                for job in simple_jobs:
                    all_keys.update(job.keys())
                
                with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
                    writer.writeheader()
                    writer.writerows(simple_jobs)
                
                print(f"  âœ… CSV ì €ì¥: {csv_file}")
        except Exception as e:
            print(f"  âš ï¸ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print("\n" + "="*80)
        print(f"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! (ë¹„ë™ê¸° ë™ì‹œ ì²˜ë¦¬: ìµœëŒ€ {self.max_concurrent}ê°œ)")
        print("="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í˜„ëŒ€ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (Async)")
    parser.add_argument("--max-jobs", type=int, help="ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)")
    parser.add_argument("--max-concurrent", type=int, default=30, help="ìµœëŒ€ ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸: 30)")
    parser.add_argument("--output-dir", default="../../output", help="ì¶œë ¥ í´ë” (ê¸°ë³¸: ../../output)")
    parser.add_argument("--screenshot-dir", default="../../img", help="ìŠ¤í¬ë¦°ìƒ· í´ë” (ê¸°ë³¸: ../../img)")
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸš€ í˜„ëŒ€ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì‹œì‘ (Async Playwright)")
    print("="*80 + "\n")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = HyundaiAutoeverCrawler(max_concurrent=args.max_concurrent)
    
    # 1ë‹¨ê³„: URL ìˆ˜ì§‘
    basic_jobs = crawler.get_job_list()
    
    if not basic_jobs:
        print("âŒ ì±„ìš©ê³µê³  ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    if args.max_jobs:
        basic_jobs = basic_jobs[:args.max_jobs]
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸: ìµœëŒ€ {args.max_jobs}ê°œë¡œ ì œí•œ\n")

    # ìŠ¤í¬ë¦°ìƒ· ë””ë ‰í† ë¦¬ ì¤€ë¹„
    screenshot_dir = resolve_dir(Path(args.screenshot_dir), get_img_dir())
    screenshot_dir.mkdir(parents=True, exist_ok=True)

    # 2ë‹¨ê³„: ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    detailed_jobs = crawler.crawl_details(basic_jobs, screenshot_dir=screenshot_dir)
    
    if not detailed_jobs:
        print("âŒ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    crawler.save_results(detailed_jobs, output_dir=args.output_dir)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ (ì²˜ìŒ 3ê°œ)")
    print("="*80)
    
    for idx, job in enumerate(detailed_jobs[:3], 1):
        print(f"\n{idx}. {job.get('title', 'N/A')}")
        print(f"   íšŒì‚¬: {job.get('company', 'N/A')}")
        print(f"   ìœ„ì¹˜: {job.get('location', 'N/A')}")
        print(f"   ê²½ë ¥: {job.get('experience', 'N/A')}")
        print(f"   ë§ˆê°: {job.get('expired_date', 'N/A')}")
        print(f"   URL: {job.get('url', 'N/A')}")
    
    if len(detailed_jobs) > 3:
        print(f"\n... ì™¸ {len(detailed_jobs) - 3}ê°œ (ì „ì²´ ê²°ê³¼ëŠ” íŒŒì¼ ì°¸ì¡°)")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    main()
import sys as _sys
from pathlib import Path as _P0
_backend_root = _P0(__file__).resolve().parents[4]
if str(_backend_root) not in _sys.path:
    _sys.path.append(str(_backend_root))
import sys as _sys
from pathlib import Path as _PX
_backend_root = _PX(__file__).resolve().parents[4]
if str(_backend_root) not in _sys.path:
    _sys.path.insert(0, str(_backend_root))
