"""
í˜„ëŒ€ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (ë¦¬íŒ©í† ë§)
1. HTML íŒŒì‹±ìœ¼ë¡œ ê³µê³  URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
2. Async Playwrightë¡œ ë³‘ë ¬ í¬ë¡¤ë§ (ë‹¨ì¼ ë¸Œë¼ìš°ì € + ì—¬ëŸ¬ í˜ì´ì§€)

ì£¼ìš” ê°œì„ ì‚¬í•­:
- AsyncOpenAI context manager ì‚¬ìš©ìœ¼ë¡œ í™•ì‹¤í•œ cleanup
- Playwright ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ê°•í™”
- ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 
"""

import json
import re
import os
import asyncio
from typing import List, Dict, Optional, Any
from pathlib import Path
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from playwright.async_api import async_playwright, BrowserContext, Page
from tenacity import retry, stop_after_attempt, wait_exponential

try:
    from app.services import resolve_dir, get_output_dir, get_img_dir
except ModuleNotFoundError:
    import sys
    _p = Path(__file__).resolve().parents[4]
    if str(_p) not in sys.path:
        sys.path.append(str(_p))
    from app.services import resolve_dir, get_output_dir, get_img_dir

try:
    from openai import AsyncOpenAI
except Exception:
    AsyncOpenAI = None

try:
    from app.utils.s3_uploader import upload_screenshot_to_s3
except ModuleNotFoundError:
    import sys
    _p = Path(__file__).resolve().parents[4]
    if str(_p) not in sys.path:
        sys.path.append(str(_p))
    from app.utils.s3_uploader import upload_screenshot_to_s3


def load_env() -> None:
    """Load environment variables from .env with fallbacks."""
    try:
        from dotenv import find_dotenv
        found = find_dotenv(usecwd=True)
        if found:
            load_dotenv(found, override=False)
    except Exception:
        pass

    try:
        proj_env = Path(__file__).resolve().parents[5] / ".env"
        if proj_env.exists():
            load_dotenv(dotenv_path=proj_env, override=False)
    except Exception:
        pass

    try:
        backend_env = Path(__file__).resolve().parents[4] / ".env"
        if backend_env.exists():
            load_dotenv(dotenv_path=backend_env, override=False)
    except Exception:
        pass


load_env()


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
async def summarize_with_llm(raw_text: str, url: str, model: str = "gpt-4o-mini") -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì±„ìš©ê³µê³ ì—ì„œ ì •ë³´ ì¶”ì¶œ - AsyncOpenAI context manager ì‚¬ìš©"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key or AsyncOpenAI is None:
        return {}

    system_prompt = """
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³  ì›¹í˜ì´ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ HTML ì½˜í…ì¸ ì—ì„œ ë‹¤ìŒ í•„ë“œë“¤ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

# ì¶”ì¶œí•  í•„ë“œ
- title: ê³µê³  ì œëª©
- location: ê·¼ë¬´ ìœ„ì¹˜
- employment_type: ê³ ìš© í˜•íƒœ (ì •ê·œì§, ê³„ì•½ì§, íŒŒíŠ¸íƒ€ì„ ë“±)
- experience: ê²½ë ¥ ìš”êµ¬ì‚¬í•­ (ì‹ ì…, ê²½ë ¥, ê²½ë ¥ë¬´ê´€, ì¸í„´ ë“±)
- crawl_date: í¬ë¡¤ë§ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
- posted_date: ê³µê³  ê²Œì‹œì¼ (YYYY-MM-DD í˜•ì‹, ìƒì‹œì±„ìš©ì¸ ê²½ìš° í¬ë¡¤ë§ ë‚ ì§œì™€ ë™ì¼)
- expired_date: ê³µê³  ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì—†ìœ¼ë©´ null)
- meta_data: ìœ„ í•„ë“œ ì™¸ ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ JSON ê°ì²´

â€» company, description, html, urlì€ ë³„ë„ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì¶”ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

# ì¤‘ìš” ì§€ì¹¨
1. ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í†µì¼
2. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ë°˜í™˜ (ë¹ˆ ë¬¸ìì—´ X)
3. â­ **meta_dataëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ í‚¤ë¡œë§Œ êµ¬ì„±** (ì˜ˆ: "ì§ë¬´ë¶„ì•¼", "ìš°ëŒ€ì‚¬í•­", "ë³µë¦¬í›„ìƒ")
4. â­ **meta_dataì—ëŠ” ìœ„ì˜ ê¸°ë³¸ í•„ë“œ(title, company, location, employment_type, experience, crawl_date, posted_date, expired_date, description)ì™€ ì¤‘ë³µë˜ëŠ” ì •ë³´ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ**
5. â­ meta_dataì—ëŠ” ì˜¤ì§ ì¶”ê°€ì ì¸ ë³´ì¡° ì •ë³´ë§Œ í¬í•¨ (ì˜ˆ: ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, ë³µë¦¬í›„ìƒ, ë‹´ë‹¹ì—…ë¬´, í•™ë ¥ìš”ê±´, ì „í˜•ì ˆì°¨ ë“±)
6. â­ meta_dataì—ëŠ” ê¸°ìˆ ìŠ¤íƒ/ì†Œí”„íŠ¸ìŠ¤í‚¬ì„ ë„£ì§€ ì•ŠëŠ”ë‹¤ (ì˜ˆ: Python, Django, AWS, Docker ë“± ì œì™¸)
7. â­ meta_data í‚¤ëŠ” ì ˆëŒ€ ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ
8. ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ê³µë°± ì •ë¦¬ ë° ì •ê·œí™”

---
# Example (í•œêµ­ì–´ í‚¤ í•„ìˆ˜!)
{
    "title": "ë°±ì—”ë“œ ê°œë°œì (Python/Django)",
    "location": "ì„œìš¸ ê°•ë‚¨êµ¬",
    "employment_type": "ì •ê·œì§",
    "experience": "ê²½ë ¥ 3~5ë…„",
    "crawl_date": "2025-12-13",
    "posted_date": "2025-11-28",
    "expired_date": "2025-12-31",
    "meta_data": {
        "ì§ë¬´ë¶„ì•¼": "IT/ê°œë°œ",
        "ìš°ëŒ€ì‚¬í•­": ["AWS ê²½í—˜", "Docker/K8s ì‚¬ìš© ê²½í—˜", "MSA ì•„í‚¤í…ì²˜ ì´í•´"],
        "ë³µë¦¬í›„ìƒ": ["ê±´ê°•ê²€ì§„", "ìê¸°ê³„ë°œë¹„ ì§€ì›", "ìœ ì—°ê·¼ë¬´ì œ"],
        "í•™ë ¥ìš”ê±´": "í•™ì‚¬ ì´ìƒ",
        "ì „í˜•ì ˆì°¨": "ì„œë¥˜ì „í˜• > 1ì°¨ ë©´ì ‘ > 2ì°¨ ë©´ì ‘ > ìµœì¢…í•©ê²©",
        "ë‹´ë‹¹ì—…ë¬´": "ë°±ì—”ë“œ API ì„¤ê³„ ë° ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„",
        "ìê²©ìš”ê±´": "Python 3ë…„ ì´ìƒ ê²½í—˜"
    }
}
---

âš ï¸ ê²½ê³ : meta_dataì˜ í‚¤ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤!
âŒ ì˜ëª»ëœ ì˜ˆ: "job_category", "tech_stack", "benefits"
âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: "ì§ë¬´ë¶„ì•¼", "ìš°ëŒ€ì‚¬í•­", "ë³µë¦¬í›„ìƒ"
"""
    
    user_prompt = (
        f"ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().strftime('%Y-%m-%d')}ì´ê³ , ì´ ë‚ ì§œë¥¼ crawl_dateë¡œ ì‚¬ìš©í•´. "
        f"ê³µê³  URL: {url}\n\n"
        f"ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜:\n\n{raw_text[:8000]}"
    )

    # AsyncOpenAIë¥¼ async withë¡œ ì‚¬ìš©í•˜ì—¬ í™•ì‹¤í•œ cleanup
    async with AsyncOpenAI(api_key=api_key) as client:
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2,
            max_tokens=8000,
        )
        
        content = response.choices[0].message.content if response and response.choices else "{}"
        
        # JSONë§Œ ë‚¨ë„ë¡ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        json_text = json_match.group(0) if json_match else content
        
        try:
            data = json.loads(json_text)
            return data
        except Exception:
            return {}


async def crawl_single_job(
    context: BrowserContext, 
    job: Dict, 
    index: int, 
    total: int, 
    semaphore: asyncio.Semaphore, 
    screenshot_dir: Path = None
) -> Optional[Dict[str, Any]]:
    """ê°œë³„ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ë¹„ë™ê¸°) - context ê³µìœ , ê°ì page ìƒì„±, semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ"""
    page = None
    
    # Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ê°œìˆ˜ ì œí•œ
    async with semaphore:
        try:
            job_url = job['url']

            # ìƒˆ í˜ì´ì§€ ìƒì„± (contextëŠ” ê³µìœ )
            page = await context.new_page()

            print(f"  [{index}/{total}] ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            # íƒ€ì„ì•„ì›ƒ 60ì´ˆë¡œ ì¦ê°€, domcontentloadedë¡œ ë¹ ë¥´ê²Œ ë¡œë”©
            await page.goto(job_url, timeout=60000, wait_until='domcontentloaded')
            await page.wait_for_timeout(3000)  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ ì¦ê°€

            today = datetime.now().strftime('%Y-%m-%d')

            # ê¸°ë³¸ ì •ë³´ ì„¤ì •
            job_info = {
                "title": None,
                "company": "í˜„ëŒ€ì˜¤í† ì—ë²„",
                "location": None,
                "employment_type": None,
                "experience": None,
                "crawl_date": today,
                "posted_date": None,
                "expired_date": None,
                "description": None,
                "url": job_url,
                "meta_data": {},
                "screenshots": {},
            }

            # description ì¶”ì¶œ - CSS Selector ì‚¬ìš©
            try:
                description_element = await page.query_selector("div.sc-2109ef67-5.cSmBgl")
                if description_element:
                    description_text = await description_element.inner_text()
                    if description_text:
                        job_info["description"] = description_text
                        print(f"  [{index}/{total}] description ì¶”ì¶œ ì™„ë£Œ")
                else:
                    print(f"  [{index}/{total}] description selectorë¥¼ ì°¾ì§€ ëª»í•¨, body ì „ì²´ ì‚¬ìš©")
                    job_info["description"] = await page.inner_text("body")
            except Exception as e:
                print(f"  [{index}/{total}] description ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ ì‚¬ìš©: {e}")
                job_info["description"] = await page.inner_text("body")

            # ìŠ¤í¬ë¦°ìƒ· S3 ì—…ë¡œë“œ
            if screenshot_dir:
                try:
                    job_id = job.get('id', f"job_{index}")
                    screenshot_filename = f"hyundai_autoever_job_{job_id}.png"

                    # S3ì— ì§ì ‘ ì—…ë¡œë“œ
                    screenshot_bytes = await page.screenshot(full_page=True)
                    s3_url = upload_screenshot_to_s3(screenshot_bytes, screenshot_filename)

                    if s3_url:
                        job_info["screenshots"]["combined"] = s3_url
                        print(f"  [{index}/{total}] S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_url}")
                    else:
                        print(f"  [{index}/{total}] S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
                except Exception as e:
                    print(f"  [{index}/{total}] ìŠ¤í¬ë¦°ìƒ· ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            # LLMìœ¼ë¡œ ë‚˜ë¨¸ì§€ í•„ë“œ íŒŒì‹± (description ì œì™¸)
            try:
                full_text = await page.inner_text("body")
                parsed_data = await summarize_with_llm(full_text, job_url)
                
                if parsed_data:
                    for key in ["title", "location", "employment_type", "experience",
                               "posted_date", "expired_date", "meta_data"]:
                        if key in parsed_data and parsed_data[key] is not None:
                            job_info[key] = parsed_data[key]
                    print(f"  [{index}/{total}] LLM íŒŒì‹± ì™„ë£Œ")
            except Exception as e:
                print(f"  [{index}/{total}] LLM íŒŒì‹± ì‹¤íŒ¨ (descriptionì€ ì €ì¥ë¨): {e}")

            print(f"  [{index}/{total}] âœ… ì™„ë£Œ: {job_info.get('title', 'N/A')}")
            return job_info

        except Exception as e:
            print(f"  [{index}/{total}] âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
            
        finally:
            # í˜ì´ì§€ í™•ì‹¤í•˜ê²Œ ë‹«ê¸°
            if page:
                try:
                    await page.close()
                except Exception as e:
                    print(f"  [{index}/{total}] í˜ì´ì§€ ë‹«ê¸° ì‹¤íŒ¨: {e}")


class HyundaiAutoeverCrawler:
    def __init__(self, max_concurrent: int = 10):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            max_concurrent: ë™ì‹œ í¬ë¡¤ë§ ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸: 10, ì•ˆì •ì„±ì„ ìœ„í•´ 30ì—ì„œ ê°ì†Œ)
        """
        load_dotenv()
        
        self.base_url = "https://career.hyundai-autoever.com"
        self.list_url = f"{self.base_url}/ko/apply"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        self.max_concurrent = max_concurrent
    
    def get_job_list(self) -> List[Dict]:
        """HTML íŒŒì‹±ìœ¼ë¡œ ì±„ìš©ê³µê³  URL ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        print("ğŸ“¡ [1/3] ì±„ìš©ê³µê³  ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            response = requests.get(self.list_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            job_links = soup.select('a[href^="/ko/o/"]')
            
            seen_urls = set()
            jobs = []
            
            for link in job_links:
                href = link.get('href')
                full_url = self.base_url + href
                
                if full_url not in seen_urls:
                    seen_urls.add(full_url)
                    job_id = href.split('/')[-1]
                    title = link.get_text(strip=True) or f"ê³µê³ _{job_id}"
                    
                    jobs.append({
                        'id': job_id,
                        'title': title,
                        'url': full_url
                    })
            
            print(f"âœ… {len(jobs)}ê°œ ì±„ìš©ê³µê³  URL ìˆ˜ì§‘ ì™„ë£Œ")
            return jobs
            
        except Exception as e:
            print(f"âŒ ì±„ìš©ê³µê³  ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def crawl_details_async(self, jobs: List[Dict], screenshot_dir: Path = None) -> List[Dict]:
        """ë¹„ë™ê¸°ë¡œ ëª¨ë“  ê³µê³ ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        if not jobs:
            return []

        print(f"\nğŸ” [2/3] ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ ({len(jobs)}ê°œ, ìµœëŒ€ ë™ì‹œ {self.max_concurrent}ê°œ)\n")

        browser = None
        context = None
        
        try:
            async with async_playwright() as p:
                # ë‹¨ì¼ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()

                # Semaphore ìƒì„± (ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ)
                semaphore = asyncio.Semaphore(self.max_concurrent)

                # ëª¨ë“  ì‘ì—…ì„ asyncio.gatherë¡œ ë™ì‹œ ì‹¤í–‰
                tasks = [
                    crawl_single_job(context, job, idx + 1, len(jobs), semaphore, screenshot_dir)
                    for idx, job in enumerate(jobs)
                ]

                # ëª¨ë“  ì‘ì—… ì‹¤í–‰
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§ (ì˜ˆì™¸ ì œì™¸)
                detailed_jobs = [
                    job for job in results 
                    if job is not None and not isinstance(job, Exception)
                ]

                print(f"\nâœ… [2/3] ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(detailed_jobs)}/{len(jobs)}ê°œ ì„±ê³µ")
                return detailed_jobs
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return []
            
        finally:
            # ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
            if context:
                try:
                    await context.close()
                except Exception as e:
                    print(f"âš ï¸ Context ë‹«ê¸° ì‹¤íŒ¨: {e}")
            
            if browser:
                try:
                    await browser.close()
                except Exception as e:
                    print(f"âš ï¸ Browser ë‹«ê¸° ì‹¤íŒ¨: {e}")

    def crawl_details(self, jobs: List[Dict], screenshot_dir: Path = None) -> List[Dict]:
        """ë™ê¸° ë˜í¼: ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰"""
        return asyncio.run(self.crawl_details_async(jobs, screenshot_dir))
    
    def save_results(self, jobs: List[Dict], output_dir: str = "output"):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ [3/3] ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        output_path = resolve_dir(Path(output_dir), get_output_dir())
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_file = output_path / f"hyundai_autoever_jobs.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON ì €ì¥: {json_file}")
        
        # CSV ì €ì¥
        try:
            import csv
            csv_file = output_path / f"hyundai_autoever_jobs.csv"

            if jobs:
                simple_jobs = []
                for job in jobs:
                    simple_job = job.copy()
                    simple_job.pop('html', None)
                    
                    if 'meta_data' in simple_job:
                        simple_job['meta_data'] = json.dumps(simple_job['meta_data'], ensure_ascii=False)
                    if 'screenshots' in simple_job:
                        simple_job['screenshots'] = json.dumps(simple_job['screenshots'], ensure_ascii=False)
                    
                    simple_jobs.append(simple_job)
                
                all_keys = set()
                for job in simple_jobs:
                    all_keys.update(job.keys())
                
                with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
                    writer.writeheader()
                    writer.writerows(simple_jobs)
                
                print(f"  âœ… CSV ì €ì¥: {csv_file}")
        except Exception as e:
            print(f"  âš ï¸ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print("\n" + "="*80)
        print(f"ğŸ‰ í˜„ëŒ€ì˜¤í† ì—ë²„ í¬ë¡¤ë§ ì™„ë£Œ! (ë¹„ë™ê¸° ë™ì‹œ ì²˜ë¦¬: ìµœëŒ€ {self.max_concurrent}ê°œ)")
        print("="*80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í˜„ëŒ€ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (Refactored)")
    parser.add_argument("--max-jobs", type=int, help="ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)")
    parser.add_argument("--max-concurrent", type=int, default=10, help="ìµœëŒ€ ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸: 10)")
    parser.add_argument("--output-dir", default="../../output", help="ì¶œë ¥ í´ë” (ê¸°ë³¸: ../../output)")
    parser.add_argument("--screenshot-dir", default="../../img", help="ìŠ¤í¬ë¦°ìƒ· í´ë” (ê¸°ë³¸: ../../img)")
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸš€ í˜„ëŒ€ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì‹œì‘ (Refactored)")
    print("="*80 + "\n")
    
    crawler = HyundaiAutoeverCrawler(max_concurrent=args.max_concurrent)
    
    # 1ë‹¨ê³„: URL ìˆ˜ì§‘
    basic_jobs = crawler.get_job_list()
    
    if not basic_jobs:
        print("âŒ ì±„ìš©ê³µê³  ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    if args.max_jobs:
        basic_jobs = basic_jobs[:args.max_jobs]
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸: ìµœëŒ€ {args.max_jobs}ê°œë¡œ ì œí•œ\n")

    # ìŠ¤í¬ë¦°ìƒ· ë””ë ‰í† ë¦¬ ì¤€ë¹„
    screenshot_dir = resolve_dir(Path(args.screenshot_dir), get_img_dir())
    screenshot_dir.mkdir(parents=True, exist_ok=True)

    # 2ë‹¨ê³„: ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§
    detailed_jobs = crawler.crawl_details(basic_jobs, screenshot_dir=screenshot_dir)
    
    if not detailed_jobs:
        print("âŒ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    crawler.save_results(detailed_jobs, output_dir=args.output_dir)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ (ì²˜ìŒ 3ê°œ)")
    print("="*80)
    
    for idx, job in enumerate(detailed_jobs[:3], 1):
        print(f"\n{idx}. {job.get('title', 'N/A')}")
        print(f"   íšŒì‚¬: {job.get('company', 'N/A')}")
        print(f"   ìœ„ì¹˜: {job.get('location', 'N/A')}")
        print(f"   ê²½ë ¥: {job.get('experience', 'N/A')}")
        print(f"   ë§ˆê°: {job.get('expired_date', 'N/A')}")
        print(f"   URL: {job.get('url', 'N/A')}")
    
    if len(detailed_jobs) > 3:
        print(f"\n... ì™¸ {len(detailed_jobs) - 3}ê°œ (ì „ì²´ ê²°ê³¼ëŠ” íŒŒì¼ ì°¸ì¡°)")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    main()
