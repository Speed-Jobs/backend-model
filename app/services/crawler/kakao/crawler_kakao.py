"""
ì¹´ì¹´ì˜¤ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ (ë¦¬íŒ©í† ë§)
1. í˜ì´ì§€ë³„ë¡œ URL ìˆ˜ì§‘
2. ThreadPoolExecutorë¡œ ë³‘ë ¬ í¬ë¡¤ë§ (ê° ìŠ¤ë ˆë“œê°€ ë…ë¦½ ë¸Œë¼ìš°ì €)

ì£¼ìš” ê°œì„ ì‚¬í•­:
- OpenAI í´ë¼ì´ì–¸íŠ¸ ëª…ì‹œì  ê´€ë¦¬
- Playwright ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•œ cleanup
- ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 
"""

import argparse
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from dotenv import load_dotenv
try:
    from dotenv import find_dotenv  # type: ignore
except Exception:
    find_dotenv = None  # type: ignore
from tenacity import retry, stop_after_attempt, wait_exponential
from playwright.sync_api import sync_playwright, Browser, Page, BrowserContext
from app.services import resolve_dir, get_output_dir, get_img_dir
import concurrent.futures

try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore


def load_env() -> None:
    """Load environment variables from .env with fallbacks."""
    try:
        if find_dotenv is not None:
            found = find_dotenv(usecwd=True)
            if found:
                load_dotenv(found, override=False)
    except Exception:
        pass

    try:
        proj_env = Path(__file__).resolve().parents[5] / ".env"
        if proj_env.exists():
            load_dotenv(dotenv_path=proj_env, override=False)
    except Exception:
        pass

    try:
        backend_env = Path(__file__).resolve().parents[4] / ".env"
        if backend_env.exists():
            load_dotenv(dotenv_path=backend_env, override=False)
    except Exception:
        pass


load_env()


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(10))
def summarize_with_llm(raw_text: str, model: str = "gpt-4o-mini") -> List[Dict[str, Any]]:
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key or OpenAI is None:
        return []

    system_prompt = """
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³  ì›¹í˜ì´ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ HTML ì½˜í…ì¸ ì—ì„œ ë‹¤ìŒ í•„ë“œë“¤ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

# ì¶”ì¶œí•  í•„ë“œ
- title: ê³µê³  ì œëª©
- company: íšŒì‚¬ ì´ë¦„
- location: ê·¼ë¬´ ìœ„ì¹˜
- employment_type: ê³ ìš© í˜•íƒœ (ì •ê·œì§, ê³„ì•½ì§, íŒŒíŠ¸íƒ€ì„ ë“±)
- experience: ê²½ë ¥ ìš”êµ¬ì‚¬í•­ (ì‹ ì…, ê²½ë ¥, ê²½ë ¥ë¬´ê´€, ì¸í„´ ë“±)
- crawl_date: í¬ë¡¤ë§ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
- posted_date: ê³µê³  ê²Œì‹œì¼ (YYYY-MM-DD í˜•ì‹, ìƒì‹œì±„ìš©ì¸ ê²½ìš° í¬ë¡¤ë§ ë‚ ì§œì™€ ë™ì¼)
- expired_date: ê³µê³  ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹, ì—†ìœ¼ë©´ null)
- description: ì±„ìš©ê³µê³  ì „ë¬¸ í…ìŠ¤íŠ¸ (HTML íƒœê·¸ ì œê±°)
- meta_data: ìœ„ í•„ë“œ ì™¸ ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ JSON ê°ì²´

# ì¤‘ìš” ì§€ì¹¨
1. ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í†µì¼
2. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° null ë°˜í™˜ (ë¹ˆ ë¬¸ìì—´ X)
3. descriptionì€ HTML íƒœê·¸ë¥¼ ì œê±°í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸
4. meta_dataëŠ” ì˜ë¯¸ìˆëŠ” í‚¤ ì´ë¦„ìœ¼ë¡œ êµ¬ì¡°í™” (ì˜ë¬¸ snake_case ì‚¬ìš©)

---
# Example
{
    "title": "ë°±ì—”ë“œ ê°œë°œì",
    "company": "(ì£¼)í…Œí¬",
    "location": "ì„œìš¸",
    "employment_type": "ì •ê·œì§",
    "experience": "ê²½ë ¥ 3~5ë…„",
    "crawl_date": "2025-11-05",
    "posted_date": "2025-10-28",
    "expired_date": "2025-11-30",
    "description": "ì£¼ìš”ì—…ë¬´...",
    "meta_data": {"job_category": "IT/ê°œë°œ"}
}
---
"""
    
    user_prompt = (
        f"ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().strftime('%Y-%m-%d')}ì´ê³ , ì´ ë‚ ì§œë¥¼ crawl_dateë¡œ ì‚¬ìš©í•´. "
        f"ê³µê³ ë“¤ì„ ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ì¶° ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.\n\n" + raw_text
    )

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì‚¬ìš©
    client = OpenAI(api_key=api_key)
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2,
            max_tokens=4000,
        )
        content = response.choices[0].message.content if response and response.choices else "[]"

        json_text_match = re.search(r"(\[.*\])", content, re.DOTALL)
        json_text = json_text_match.group(1) if json_text_match else content
        
        try:
            data = json.loads(json_text)
            if isinstance(data, list):
                return data
        except Exception:
            pass
        return []
    finally:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ëª…ì‹œì  ì •ë¦¬
        client.close()


def extract_job_detail_from_url(job_url: str, job_index: int, screenshot_dir: Path = None) -> Dict[str, Any]:
    """URLë¡œ ì§ì ‘ ì ‘ì†í•˜ì—¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ìš© - ë…ë¦½ ë¸Œë¼ìš°ì €)"""
    playwright_instance = None
    browser = None
    context = None
    page = None
    
    try:
        playwright_instance = sync_playwright().start()
        browser = playwright_instance.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        print(f"  [{job_index}] ìƒì„¸ í˜ì´ì§€ ë¡œë”©...")
        page.goto(job_url, timeout=30000)
        page.wait_for_timeout(1500)

        today = datetime.now().strftime('%Y-%m-%d')

        job_info = {
            "title": None,
            "company": "Kakao",
            "location": None,
            "employment_type": None,
            "experience": None,
            "crawl_date": today,
            "posted_date": None,
            "expired_date": None,
            "description": None,
            "url": job_url,
            "meta_data": "{}",
            "screenshots": {},
        }

        # description ì¶”ì¶œ - div.area_contì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
        full_text = page.inner_text("body")
        try:
            area_cont = page.query_selector("div.area_cont")
            if area_cont:
                description_text = area_cont.inner_text()
                if description_text:
                    job_info["description"] = description_text
            else:
                job_info["description"] = full_text

            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            if screenshot_dir:
                try:
                    job_id_match = re.search(r'/jobs/(\d+)', job_url)
                    job_id = job_id_match.group(1) if job_id_match else f"job_{job_index}"

                    screenshot_filename = f"kakao_job_{job_id}.png"
                    screenshot_path = screenshot_dir / screenshot_filename

                    page.screenshot(path=str(screenshot_path), full_page=True)
                    job_info["screenshots"]["combined"] = screenshot_filename
                    print(f"  [{job_index}] ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_filename}")
                except Exception as e:
                    print(f"  [{job_index}] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")

        except Exception as e:
            print(f"  [{job_index}] area_cont ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ ì‚¬ìš©: {e}")
            job_info["description"] = full_text

        # LLMìœ¼ë¡œ ë‚˜ë¨¸ì§€ í•„ë“œ íŒŒì‹± ì‹œë„
        try:
            parsed = summarize_with_llm(full_text)
            if parsed and len(parsed) > 0:
                parsed_data = parsed[0]
                for key in ["title", "company", "location", "employment_type", "experience",
                           "posted_date", "expired_date", "meta_data"]:
                    if key in parsed_data and parsed_data[key]:
                        job_info[key] = parsed_data[key]
        except Exception as e:
            print(f"  [{job_index}] LLM íŒŒì‹± ì‹¤íŒ¨ (descriptionì€ ì €ì¥ë¨): {e}")

        print(f"  [{job_index}] ì™„ë£Œ: {job_info.get('title', 'N/A')}")
        return job_info

    except Exception as e:
        print(f"  [{job_index}] ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
        if page:
            try:
                page.close()
            except Exception:
                pass
        if context:
            try:
                context.close()
            except Exception:
                pass
        if browser:
            try:
                browser.close()
            except Exception:
                pass
        if playwright_instance:
            try:
                playwright_instance.stop()
            except Exception:
                pass


def run_scrape(
    part: str = "TECHNOLOGY",
    company: str = "KAKAO",
    keyword: str = "",
    out_dir: Path = None,
    screenshot_dir: Path = None,
    fast: bool = False
) -> Dict[str, Path]:
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    out_dir = resolve_dir(out_dir, get_output_dir())
    screenshot_dir = resolve_dir(screenshot_dir, get_img_dir())
    ensure_dir(out_dir)
    ensure_dir(screenshot_dir)

    outputs = {
        "raw_html": out_dir / "kakao_raw.html",
        "clean_txt": out_dir / "kakao_clean.txt",
        "json": out_dir / "kakao_jobs.json",
        "screenshots": screenshot_dir,
    }

    all_job_urls = []
    jobs_list = []

    playwright_instance = None
    browser = None
    context = None
    page = None

    try:
        print("[1/10] ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
        playwright_instance = sync_playwright().start()
        browser = playwright_instance.chromium.launch(headless=True)
        context = browser.new_context()
        if fast:
            try:
                context.set_default_timeout(5000)
            except Exception:
                pass
        page = context.new_page()

        base_url = "https://careers.kakao.com/jobs"
        page_num = 1
        all_html_content = []

        while True:
            url = f"{base_url}?part={part}&company={company}&keyword={keyword}&page={page_num}"

            print(f"[2/10] í˜ì´ì§€ {page_num} ì ‘ì†: {url}")
            page.goto(url, timeout=60000)
            page.wait_for_load_state("domcontentloaded")
            page.wait_for_timeout(2000)

            # ìŠ¤í¬ë¡¤
            print(f"[4/10] í˜ì´ì§€ {page_num} ìŠ¤í¬ë¡¤...")
            for _ in range(3):
                try:
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    page.wait_for_timeout(800)
                except Exception:
                    pass

            # ì±„ìš© ê³µê³  ì¹´ë“œ ì°¾ê¸°
            print(f"[5/10] í˜ì´ì§€ {page_num} ì±„ìš© ê³µê³  ì¹´ë“œ ì°¾ëŠ” ì¤‘...")
            try:
                card_selectors = [
                    "a[href*='/jobs/']",
                    "[class*='card']",
                    ".list_jobs > *",
                ]

                cards = None
                selector = None
                for sel in card_selectors:
                    cards = page.locator(sel)
                    if cards.count() > 0:
                        selector = sel
                        print(f"[5/10] í˜ì´ì§€ {page_num}: {cards.count()}ê°œ ì¹´ë“œ (ì…€ë ‰í„°: {selector})")
                        break

                if cards and cards.count() > 0:
                    total_cards = cards.count()
                    print(f"[6/10] í˜ì´ì§€ {page_num}: {total_cards}ê°œ URL ìˆ˜ì§‘ ì¤‘...")

                    page_job_urls = []
                    for i in range(total_cards):
                        try:
                            cards = page.locator(selector)
                            card = cards.nth(i)
                            href = card.get_attribute("href")
                            if href:
                                if href.startswith("/"):
                                    full_url = f"https://careers.kakao.com{href}"
                                else:
                                    full_url = href
                                if full_url not in all_job_urls:
                                    page_job_urls.append(full_url)
                                    all_job_urls.append(full_url)
                        except Exception as e:
                            print(f"  URL ìˆ˜ì§‘ ì‹¤íŒ¨ {i+1}: {e}")
                            continue

                    print(f"[6/10] í˜ì´ì§€ {page_num}ì—ì„œ ì‹ ê·œ URL: {len(page_job_urls)}ê°œ")

                    if len(page_job_urls) == 0:
                        print(f"[6/10] í˜ì´ì§€ {page_num}ì—ì„œ ì‹ ê·œ ê³µê³  ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ.")
                        break
                else:
                    print(f"[5/10] í˜ì´ì§€ {page_num}ì—ì„œ ì¹´ë“œ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ.")
                    break

            except Exception as e:
                print(f"[5/10] í˜ì´ì§€ {page_num} ì¹´ë“œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
                break

            html = page.content()
            all_html_content.append(html)

            page_num += 1
            page.wait_for_timeout(1000)

        print(f"[6/10] ì „ì²´ ìˆ˜ì§‘ëœ URL: {len(all_job_urls)}ê°œ")

        # HTML ì €ì¥
        if all_html_content:
            outputs["raw_html"].write_text(all_html_content[-1], encoding="utf-8")
            print(f"[8/10] ì›ë³¸ HTML ì €ì¥: {outputs['raw_html']}")

    finally:
        # ë¦¬ì†ŒìŠ¤ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
        if page:
            try:
                page.close()
            except Exception:
                pass
        if context:
            try:
                context.close()
            except Exception:
                pass
        if browser:
            try:
                browser.close()
            except Exception:
                pass
        if playwright_instance:
            try:
                playwright_instance.stop()
            except Exception:
                pass

    # ë³‘ë ¬ë¡œ ê° URLì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
    print(f"[7/10] ë³‘ë ¬ë¡œ {len(all_job_urls)}ê°œ ê³µê³  ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
    if all_job_urls:
        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
            futures = []
            for idx, job_url in enumerate(all_job_urls, 1):
                future = executor.submit(extract_job_detail_from_url, job_url, idx, screenshot_dir)
                futures.append(future)

            for future in concurrent.futures.as_completed(futures):
                try:
                    job_info = future.result()
                    if job_info:
                        jobs_list.append(job_info)
                except Exception as e:
                    print(f"  ì‘ì—… ì‹¤íŒ¨: {e}")

    print("[8/10] ë¸Œë¼ìš°ì € ì¢…ë£Œ")
    if all_html_content:
        print("[9/10] HTML ì €ì¥ ì™„ë£Œ")

    return outputs, jobs_list


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    env_path = Path(__file__).parent.parent.parent.parent / ".env"
    load_dotenv(dotenv_path=env_path)

    parser = argparse.ArgumentParser(description="Kakao Careers ìŠ¤í¬ë˜í•‘ (Refactored)")
    parser.add_argument("--part", default="TECHNOLOGY", help="ì§êµ°")
    parser.add_argument("--company", default="KAKAO", help="íšŒì‚¬")
    parser.add_argument("--keyword", default="", help="ê²€ìƒ‰ í‚¤ì›Œë“œ")
    parser.add_argument("--out-dir", default="../../output", help="ì¶œë ¥ í´ë”")
    parser.add_argument("--screenshot-dir", default="../../img", help="ìŠ¤í¬ë¦°ìƒ· í´ë”")
    parser.add_argument("--fast", action="store_true", help="ë¹ ë¥¸ ëª¨ë“œ")
    args = parser.parse_args()

    out_dir = Path(args.out_dir)
    screenshot_dir = Path(args.screenshot_dir)
    
    print("="*80)
    print("ğŸš€ Kakao ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì‹œì‘ (Refactored)")
    print("="*80)
    print("[0/10] ì‘ì—… ì‹œì‘")

    paths, items = run_scrape(
        part=args.part,
        company=args.company,
        keyword=args.keyword,
        out_dir=out_dir,
        screenshot_dir=screenshot_dir,
        fast=args.fast
    )

    if items:
        print(f"[9/10] ì´ {len(items)}ê°œì˜ ê³µê³  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print("[9/10] ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")

    paths["json"].write_text(json.dumps(items, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"[9/10] JSON ì €ì¥ ì™„ë£Œ: {paths['json']}")

    print(str(paths["json"]))
    print("[10/10] ì‘ì—… ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    main()