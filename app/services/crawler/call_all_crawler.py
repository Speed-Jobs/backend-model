"""
9ê°œ ë¦¬íŒ©í† ë§ëœ í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰ ìŠ¤ì¼€ì¤„ëŸ¬

ê¸°ì¡´ 4ê°œ í¬ë¡¤ëŸ¬:
- í˜„ëŒ€ì˜¤í† ì—ë²„ (ë¹„ë™ê¸°)
- LG CNS (ë¹„ë™ê¸°)
- í•œí™”ì‹œìŠ¤í…œ (ë™ê¸°)
- ì¹´ì¹´ì˜¤ (ë™ê¸°)

ì¶”ê°€ëœ 5ê°œ í¬ë¡¤ëŸ¬ (ëª¨ë‘ ë™ê¸°):
- Coupang
- Line
- Naver
- Toss
- Woowahan (ë°°ë‹¬ì˜ë¯¼ì¡±)
"""

# ê¸°ì¡´ ë¦¬íŒ©í† ë§ëœ í¬ë¡¤ëŸ¬ import
from app.services.crawler.hyundai_autoever.crawler_hyundai_autoever import main as hyundai_crawler
from app.services.crawler.lg_cns.crawler_lg_cns import main as lg_crawler
from app.services.crawler.hanwha.crawler_hanwha import main as hanwha_crawler
from app.services.crawler.kakao.crawler_kakao import main as kakao_crawler

# ìƒˆë¡œ ì¶”ê°€ëœ í¬ë¡¤ëŸ¬ import
from app.services.crawler.coupang.crawler_coupang import main as coupang_crawler
from app.services.crawler.line.crawler_line import main as line_crawler
from app.services.crawler.naver.crawler_naver import main as naver_crawler
from app.services.crawler.toss.crawler_toss import main as toss_crawler
from app.services.crawler.woowahan.crawler_woowahan import main as woowahan_crawler

from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.interval import IntervalTrigger
import asyncio
import inspect
import logging
from datetime import datetime
import time
import warnings


# AsyncOpenAI cleanup ì—ëŸ¬ ë¡œê¹… ì–µì œ
logging.getLogger("httpx").setLevel(logging.CRITICAL)
logging.getLogger("httpcore").setLevel(logging.CRITICAL)
warnings.filterwarnings("ignore", category=RuntimeWarning)


def run_crawler_safely(crawler_func, name):
    """ê°œë³„ í¬ë¡¤ëŸ¬ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰ (ë¦¬ì†ŒìŠ¤ ì™„ì „ ê²©ë¦¬)"""
    try:
        print(f"\n{'='*80}")
        print(f"[{datetime.now()}] ğŸš€ {name} ì‹œì‘")
        print(f"{'='*80}\n")
        
        start_time = time.time()
        
        if inspect.iscoroutinefunction(crawler_func):
            # ë¹„ë™ê¸° í•¨ìˆ˜: ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                loop.run_until_complete(crawler_func())
            finally:
                # ë³´ë¥˜ ì¤‘ì¸ íƒœìŠ¤í¬ ì •ë¦¬
                pending = asyncio.all_tasks(loop)
                for task in pending:
                    task.cancel()
                
                # ì·¨ì†Œëœ íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸° (ì—ëŸ¬ ë¬´ì‹œ)
                loop.run_until_complete(
                    asyncio.gather(*pending, return_exceptions=True)
                )
                
                # ë¹„ë™ê¸° ì œë„ˆë ˆì´í„° ì¢…ë£Œ (ì—ëŸ¬ ë¬´ì‹œ)
                try:
                    loop.run_until_complete(loop.shutdown_asyncgens())
                except Exception:
                    pass
                
                # ë£¨í”„ ì¢…ë£Œ
                loop.close()
        else:
            # ë™ê¸° í•¨ìˆ˜: ê·¸ëƒ¥ ì‹¤í–‰
            crawler_func()
        
        duration = time.time() - start_time
        
        print(f"\n{'='*80}")
        print(f"[{datetime.now()}] âœ… {name} ì™„ë£Œ ({duration/60:.1f}ë¶„)")
        print(f"{'='*80}\n")
        
        return True
        
    except Exception as e:
        print(f"\n{'='*80}")
        print(f"[{datetime.now()}] âŒ {name} ì‹¤íŒ¨: {e}")
        print(f"{'='*80}\n")
        
        import traceback
        traceback.print_exc()
        
        return False